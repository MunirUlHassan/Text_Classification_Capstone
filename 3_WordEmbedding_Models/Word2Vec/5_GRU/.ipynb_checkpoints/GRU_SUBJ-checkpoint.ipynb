{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with SUBJ Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smart and alert , thirteen conversations about...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>color , musical bounce and warm seas lapping o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is not a mass market entertainment but an u...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a light hearted french film about the spiritua...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my wife is an actress has its moments in looki...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>in the end , they discover that balance in lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>enter the beautiful and mysterious secret agen...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>after listening to a missionary from china spe...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>looking for a short cut to fame , glass concoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     smart and alert , thirteen conversations about...      0  train\n",
       "1     color , musical bounce and warm seas lapping o...      0  train\n",
       "2     it is not a mass market entertainment but an u...      0  train\n",
       "3     a light hearted french film about the spiritua...      0  train\n",
       "4     my wife is an actress has its moments in looki...      0  train\n",
       "...                                                 ...    ...    ...\n",
       "9995  in the end , they discover that balance in lif...      1  train\n",
       "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
       "9997  enter the beautiful and mysterious secret agen...      1  train\n",
       "9998  after listening to a missionary from china spe...      1  train\n",
       "9999  looking for a short cut to fame , glass concoc...      1  train\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/SUBJ/SUBJ.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10000 non-null  object\n",
      " 1   label     10000 non-null  int32 \n",
      " 2   split     10000 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          5000   5000\n",
       "1          5000   5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smart and alert , thirteen conversations about one thing is a small gem .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
      "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
      "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
      "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
      "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "a 3\n",
      "and 4\n",
      "of 5\n",
      "to 6\n",
      "in 7\n",
      "is 8\n",
      "'s 9\n",
      "it 10\n",
      "21324\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 440,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "282/282 [==============================] - 58s 149ms/step - loss: 0.4524 - accuracy: 0.7798 - val_loss: 0.2054 - val_accuracy: 0.9190\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 42s 150ms/step - loss: 0.0677 - accuracy: 0.9792 - val_loss: 0.2482 - val_accuracy: 0.9120\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 39s 139ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.3429 - val_accuracy: 0.9070\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.4184 - val_accuracy: 0.9110\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 36s 126ms/step - loss: 0.0018 - accuracy: 0.9990 - val_loss: 0.5232 - val_accuracy: 0.8990\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 36s 126ms/step - loss: 8.9157e-04 - accuracy: 0.9996 - val_loss: 0.3468 - val_accuracy: 0.9130\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.90000295639038\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 53s 143ms/step - loss: 0.4617 - accuracy: 0.7620 - val_loss: 0.2138 - val_accuracy: 0.9210\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.0530 - accuracy: 0.9824 - val_loss: 0.2431 - val_accuracy: 0.9110\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.3538 - val_accuracy: 0.9100\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.3466 - val_accuracy: 0.9100\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.4132 - val_accuracy: 0.8970\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.5354 - val_accuracy: 0.8950\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.10000038146973\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 53s 145ms/step - loss: 0.4555 - accuracy: 0.7773 - val_loss: 0.2025 - val_accuracy: 0.9210\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 137ms/step - loss: 0.0623 - accuracy: 0.9785 - val_loss: 0.2036 - val_accuracy: 0.9180\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.2442 - val_accuracy: 0.9250\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.3415 - val_accuracy: 0.9150\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 7.8487e-04 - accuracy: 0.9998 - val_loss: 0.3079 - val_accuracy: 0.9170\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3221 - val_accuracy: 0.9050\n",
      "Epoch 7/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.3831 - val_accuracy: 0.9130\n",
      "Epoch 8/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.4381 - val_accuracy: 0.9120\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 54s 149ms/step - loss: 0.4564 - accuracy: 0.7700 - val_loss: 0.1872 - val_accuracy: 0.9240\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0574 - accuracy: 0.9822 - val_loss: 0.2301 - val_accuracy: 0.9170\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0124 - accuracy: 0.9975 - val_loss: 0.3875 - val_accuracy: 0.9070\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.3464 - val_accuracy: 0.9060\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.3648 - val_accuracy: 0.9010\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.3779 - val_accuracy: 0.8970\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 54s 148ms/step - loss: 0.4488 - accuracy: 0.7706 - val_loss: 0.2149 - val_accuracy: 0.9160\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0573 - accuracy: 0.9810 - val_loss: 0.2371 - val_accuracy: 0.9100\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.3791 - val_accuracy: 0.9000\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3223 - val_accuracy: 0.9040\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 8.4652e-04 - accuracy: 1.0000 - val_loss: 0.4995 - val_accuracy: 0.9060\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 2.0294e-04 - accuracy: 1.0000 - val_loss: 0.5578 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.60000085830688\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 54s 149ms/step - loss: 0.4514 - accuracy: 0.7884 - val_loss: 0.1603 - val_accuracy: 0.9340\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0609 - accuracy: 0.9798 - val_loss: 0.1916 - val_accuracy: 0.9280\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 0.2295 - val_accuracy: 0.9260\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.3196 - val_accuracy: 0.9220\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 3.1037e-04 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9140\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 2.2742e-04 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.9140\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 93.4000015258789\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 54s 147ms/step - loss: 0.4532 - accuracy: 0.7802 - val_loss: 0.2059 - val_accuracy: 0.9210\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0689 - accuracy: 0.9757 - val_loss: 0.2211 - val_accuracy: 0.9190\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.2677 - val_accuracy: 0.9200\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.3278 - val_accuracy: 0.9170\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.3664 - val_accuracy: 0.9140\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 4.5215e-04 - accuracy: 1.0000 - val_loss: 0.4725 - val_accuracy: 0.9130\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.10000038146973\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 54s 150ms/step - loss: 0.4493 - accuracy: 0.7819 - val_loss: 0.1937 - val_accuracy: 0.9260\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0668 - accuracy: 0.9779 - val_loss: 0.1948 - val_accuracy: 0.9220\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0138 - accuracy: 0.9963 - val_loss: 0.3237 - val_accuracy: 0.9210\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.3260 - val_accuracy: 0.9130\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.4013 - val_accuracy: 0.9150\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 6.6059e-04 - accuracy: 0.9999 - val_loss: 0.3969 - val_accuracy: 0.9010\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.59999990463257\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 54s 149ms/step - loss: 0.4631 - accuracy: 0.7548 - val_loss: 0.2117 - val_accuracy: 0.9110\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 41s 146ms/step - loss: 0.0613 - accuracy: 0.9777 - val_loss: 0.2590 - val_accuracy: 0.8990\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.4775 - val_accuracy: 0.8960\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 40s 143ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.4438 - val_accuracy: 0.9020\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 48s 170ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.4691 - val_accuracy: 0.8970\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 44s 156ms/step - loss: 6.6907e-04 - accuracy: 1.0000 - val_loss: 0.4628 - val_accuracy: 0.8960\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.10000133514404\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 60s 166ms/step - loss: 0.4568 - accuracy: 0.7694 - val_loss: 0.1840 - val_accuracy: 0.9240\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 41s 144ms/step - loss: 0.0586 - accuracy: 0.9810 - val_loss: 0.2150 - val_accuracy: 0.9130\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 41s 144ms/step - loss: 0.0143 - accuracy: 0.9966 - val_loss: 0.2631 - val_accuracy: 0.9120\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.3009 - val_accuracy: 0.9120\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3698 - val_accuracy: 0.9150\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 4.5446e-04 - accuracy: 1.0000 - val_loss: 0.3246 - val_accuracy: 0.9130\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "\n",
      "        acc1  acc2       acc3       acc4       acc5       acc6  acc7  acc8  \\\n",
      "0  91.900003  92.1  92.500001  92.400002  91.600001  93.400002  92.1  92.6   \n",
      "\n",
      "        acc9      acc10        AVG  \n",
      "0  91.100001  92.400002  92.210001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.900003</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>91.600001</td>\n",
       "      <td>93.400002</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.6</td>\n",
       "      <td>91.100001</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>92.210001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1  acc2       acc3       acc4       acc5       acc6  acc7  acc8  \\\n",
       "0  91.900003  92.1  92.500001  92.400002  91.600001  93.400002  92.1  92.6   \n",
       "\n",
       "        acc9      acc10        AVG  \n",
       "0  91.100001  92.400002  92.210001  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_SUBJ.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-9.02488821e-01, -1.44414252e-03,  1.11408719e+00, ...,\n",
       "         2.96026624e-03, -1.23360697e+00,  1.51209989e+00],\n",
       "       [ 1.13769531e-01,  1.79687500e-01, -2.65625000e-01, ...,\n",
       "        -2.18750000e-01, -3.93066406e-02,  2.09960938e-01],\n",
       "       [ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02, ...,\n",
       "         1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "       [ 1.08886719e-01, -1.66992188e-01,  8.98437500e-02, ...,\n",
       "        -1.96289062e-01, -2.31445312e-01,  4.61425781e-02]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 140,673\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "282/282 [==============================] - 36s 75ms/step - loss: 0.4745 - accuracy: 0.7680 - val_loss: 0.2771 - val_accuracy: 0.8950\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.2295 - accuracy: 0.9044 - val_loss: 0.2480 - val_accuracy: 0.9010\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1900 - accuracy: 0.9262 - val_loss: 0.2364 - val_accuracy: 0.9050\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1687 - accuracy: 0.9339 - val_loss: 0.2632 - val_accuracy: 0.8950\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.1307 - accuracy: 0.9493 - val_loss: 0.2636 - val_accuracy: 0.9000\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 20s 70ms/step - loss: 0.1194 - accuracy: 0.9579 - val_loss: 0.2619 - val_accuracy: 0.9010\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0957 - accuracy: 0.9647 - val_loss: 0.2959 - val_accuracy: 0.9000\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0718 - accuracy: 0.9731 - val_loss: 0.3142 - val_accuracy: 0.8980\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0628 - accuracy: 0.9788 - val_loss: 0.3455 - val_accuracy: 0.8860\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0457 - accuracy: 0.9858 - val_loss: 0.3426 - val_accuracy: 0.9060\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0338 - accuracy: 0.9897 - val_loss: 0.4146 - val_accuracy: 0.8940\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.4478 - val_accuracy: 0.8840\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0223 - accuracy: 0.9938 - val_loss: 0.4933 - val_accuracy: 0.8870\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.4854 - val_accuracy: 0.8960\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0193 - accuracy: 0.9929 - val_loss: 0.5617 - val_accuracy: 0.9010\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 16s 57ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.5662 - val_accuracy: 0.9020\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.5913 - val_accuracy: 0.8940\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0170 - accuracy: 0.9937 - val_loss: 0.5584 - val_accuracy: 0.9000\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.6371 - val_accuracy: 0.9000\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.5486 - val_accuracy: 0.8950\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 90.6000018119812\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 32s 72ms/step - loss: 0.4730 - accuracy: 0.7627 - val_loss: 0.2741 - val_accuracy: 0.8960\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.2333 - accuracy: 0.9081 - val_loss: 0.2930 - val_accuracy: 0.8780\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1657 - accuracy: 0.9372 - val_loss: 0.3947 - val_accuracy: 0.8540\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.1542 - accuracy: 0.9422 - val_loss: 0.3986 - val_accuracy: 0.8510\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.1325 - accuracy: 0.9521 - val_loss: 0.3892 - val_accuracy: 0.8580\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1194 - accuracy: 0.9533 - val_loss: 0.4697 - val_accuracy: 0.8490\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.0870 - accuracy: 0.9669 - val_loss: 0.5036 - val_accuracy: 0.8620\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0753 - accuracy: 0.9732 - val_loss: 0.7300 - val_accuracy: 0.8360\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.6320 - val_accuracy: 0.8620\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 0.8065 - val_accuracy: 0.8470\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0416 - accuracy: 0.9841 - val_loss: 0.8985 - val_accuracy: 0.8240\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 89.60000276565552\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 34s 76ms/step - loss: 0.4979 - accuracy: 0.7387 - val_loss: 0.2589 - val_accuracy: 0.8830\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.2314 - accuracy: 0.9063 - val_loss: 0.2141 - val_accuracy: 0.9080\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.1804 - accuracy: 0.9306 - val_loss: 0.2065 - val_accuracy: 0.9120\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.1527 - accuracy: 0.9402 - val_loss: 0.2150 - val_accuracy: 0.9200\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.1407 - accuracy: 0.9489 - val_loss: 0.2331 - val_accuracy: 0.9110\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.1139 - accuracy: 0.9598 - val_loss: 0.2308 - val_accuracy: 0.9120\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0979 - accuracy: 0.9653 - val_loss: 0.3231 - val_accuracy: 0.8770\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.1016 - accuracy: 0.9632 - val_loss: 0.2481 - val_accuracy: 0.9140\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0674 - accuracy: 0.9768 - val_loss: 0.2709 - val_accuracy: 0.9040\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 16s 57ms/step - loss: 0.0485 - accuracy: 0.9817 - val_loss: 0.3057 - val_accuracy: 0.8980\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0351 - accuracy: 0.9877 - val_loss: 0.3528 - val_accuracy: 0.8980\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0372 - accuracy: 0.9890 - val_loss: 0.3709 - val_accuracy: 0.8840\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0202 - accuracy: 0.9949 - val_loss: 0.3811 - val_accuracy: 0.8920\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.4217 - val_accuracy: 0.9010\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 92.00000166893005\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 43s 108ms/step - loss: 0.5006 - accuracy: 0.7328 - val_loss: 0.2733 - val_accuracy: 0.8880\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.2254 - accuracy: 0.9105 - val_loss: 0.3620 - val_accuracy: 0.8430\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.1853 - accuracy: 0.9284 - val_loss: 0.3908 - val_accuracy: 0.8400\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.1537 - accuracy: 0.9450 - val_loss: 0.5932 - val_accuracy: 0.8150\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1432 - accuracy: 0.9486 - val_loss: 0.6703 - val_accuracy: 0.7890\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1167 - accuracy: 0.9587 - val_loss: 0.5203 - val_accuracy: 0.8450\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0900 - accuracy: 0.9687 - val_loss: 0.5610 - val_accuracy: 0.8350\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0739 - accuracy: 0.9715 - val_loss: 0.6403 - val_accuracy: 0.8310\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0600 - accuracy: 0.9786 - val_loss: 0.8085 - val_accuracy: 0.8170\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 0.9939 - val_accuracy: 0.7960\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.9962 - val_accuracy: 0.7920\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 88.80000114440918\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 33s 75ms/step - loss: 0.4818 - accuracy: 0.7594 - val_loss: 0.2907 - val_accuracy: 0.8750\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.2240 - accuracy: 0.9109 - val_loss: 0.2751 - val_accuracy: 0.8810\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1831 - accuracy: 0.9276 - val_loss: 0.2425 - val_accuracy: 0.9010\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.1476 - accuracy: 0.9448 - val_loss: 0.2394 - val_accuracy: 0.9050\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1249 - accuracy: 0.9529 - val_loss: 0.2383 - val_accuracy: 0.9050\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.1056 - accuracy: 0.9670 - val_loss: 0.2688 - val_accuracy: 0.8960\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0843 - accuracy: 0.9710 - val_loss: 0.2646 - val_accuracy: 0.9030\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0780 - accuracy: 0.9696 - val_loss: 0.3921 - val_accuracy: 0.8770\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0544 - accuracy: 0.9820 - val_loss: 0.3559 - val_accuracy: 0.8910\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0486 - accuracy: 0.9816 - val_loss: 0.4100 - val_accuracy: 0.8820\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0302 - accuracy: 0.9903 - val_loss: 0.4324 - val_accuracy: 0.8750\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.4562 - val_accuracy: 0.8830\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 0.4508 - val_accuracy: 0.8960\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.4755 - val_accuracy: 0.8810\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 90.49999713897705\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 33s 74ms/step - loss: 0.4812 - accuracy: 0.7592 - val_loss: 0.2978 - val_accuracy: 0.8740\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.2131 - accuracy: 0.9155 - val_loss: 0.2631 - val_accuracy: 0.8940\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.1752 - accuracy: 0.9330 - val_loss: 0.3020 - val_accuracy: 0.8750\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1560 - accuracy: 0.9397 - val_loss: 0.2853 - val_accuracy: 0.8850\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1332 - accuracy: 0.9482 - val_loss: 0.2657 - val_accuracy: 0.9020\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.1156 - accuracy: 0.9596 - val_loss: 0.3919 - val_accuracy: 0.8660\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1024 - accuracy: 0.9652 - val_loss: 0.3005 - val_accuracy: 0.9010\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.0795 - accuracy: 0.9727 - val_loss: 0.3523 - val_accuracy: 0.8900\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.3835 - val_accuracy: 0.8920\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0583 - accuracy: 0.9787 - val_loss: 0.4487 - val_accuracy: 0.8750\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0424 - accuracy: 0.9865 - val_loss: 0.3364 - val_accuracy: 0.9190\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.3500 - val_accuracy: 0.9010\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0248 - accuracy: 0.9941 - val_loss: 0.4502 - val_accuracy: 0.8960\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.4031 - val_accuracy: 0.9010\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0160 - accuracy: 0.9962 - val_loss: 0.5050 - val_accuracy: 0.8850\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0169 - accuracy: 0.9940 - val_loss: 0.5003 - val_accuracy: 0.9060\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0100 - accuracy: 0.9975 - val_loss: 0.4995 - val_accuracy: 0.9000\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.5722 - val_accuracy: 0.9030\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.5177 - val_accuracy: 0.8940\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.6274 - val_accuracy: 0.8900\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.6020 - val_accuracy: 0.8920\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 91.90000295639038\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 33s 75ms/step - loss: 0.4935 - accuracy: 0.7519 - val_loss: 0.3691 - val_accuracy: 0.8330\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.2403 - accuracy: 0.9041 - val_loss: 0.3670 - val_accuracy: 0.8440\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.1905 - accuracy: 0.9250 - val_loss: 0.3392 - val_accuracy: 0.8630\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.1544 - accuracy: 0.9439 - val_loss: 0.3730 - val_accuracy: 0.8520\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.1424 - accuracy: 0.9476 - val_loss: 0.3845 - val_accuracy: 0.8480\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.1157 - accuracy: 0.9602 - val_loss: 0.3690 - val_accuracy: 0.8510\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0991 - accuracy: 0.9644 - val_loss: 0.3808 - val_accuracy: 0.8650\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0790 - accuracy: 0.9731 - val_loss: 0.4784 - val_accuracy: 0.8520\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0595 - accuracy: 0.9799 - val_loss: 0.5263 - val_accuracy: 0.8600\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0442 - accuracy: 0.9839 - val_loss: 0.6715 - val_accuracy: 0.8300\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0346 - accuracy: 0.9890 - val_loss: 0.7922 - val_accuracy: 0.8240\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0283 - accuracy: 0.9921 - val_loss: 0.8532 - val_accuracy: 0.8360\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 1.2367 - val_accuracy: 0.8030\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 1.1921 - val_accuracy: 0.8170\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 1.0254 - val_accuracy: 0.8270\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.8460 - val_accuracy: 0.8480\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 1.0584 - val_accuracy: 0.8440\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.50000095367432\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 33s 76ms/step - loss: 0.4895 - accuracy: 0.7376 - val_loss: 0.4501 - val_accuracy: 0.8150\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.2097 - accuracy: 0.9169 - val_loss: 0.6113 - val_accuracy: 0.7790\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1884 - accuracy: 0.9277 - val_loss: 0.3958 - val_accuracy: 0.8370\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1581 - accuracy: 0.9429 - val_loss: 0.4993 - val_accuracy: 0.8080\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1410 - accuracy: 0.9472 - val_loss: 0.6257 - val_accuracy: 0.7930\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1189 - accuracy: 0.9564 - val_loss: 0.7218 - val_accuracy: 0.7820\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1015 - accuracy: 0.9658 - val_loss: 0.4972 - val_accuracy: 0.8270\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.0858 - accuracy: 0.9696 - val_loss: 0.8928 - val_accuracy: 0.7670\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.0710 - accuracy: 0.9753 - val_loss: 0.9071 - val_accuracy: 0.7790\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0497 - accuracy: 0.9831 - val_loss: 0.9082 - val_accuracy: 0.7890\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0467 - accuracy: 0.9853 - val_loss: 0.7105 - val_accuracy: 0.8210\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0306 - accuracy: 0.9911 - val_loss: 0.6962 - val_accuracy: 0.8330\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0310 - accuracy: 0.9901 - val_loss: 1.0298 - val_accuracy: 0.7880\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 83.70000123977661\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 39s 94ms/step - loss: 0.4719 - accuracy: 0.7694 - val_loss: 0.3620 - val_accuracy: 0.8490\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.2102 - accuracy: 0.9195 - val_loss: 0.3877 - val_accuracy: 0.8490\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.1838 - accuracy: 0.9258 - val_loss: 0.4438 - val_accuracy: 0.8460\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.1555 - accuracy: 0.9439 - val_loss: 0.4433 - val_accuracy: 0.8450\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 16s 59ms/step - loss: 0.1421 - accuracy: 0.9470 - val_loss: 0.3636 - val_accuracy: 0.8760\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.1151 - accuracy: 0.9560 - val_loss: 0.3681 - val_accuracy: 0.8640\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.0962 - accuracy: 0.9663 - val_loss: 0.3136 - val_accuracy: 0.8820\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0758 - accuracy: 0.9725 - val_loss: 0.3607 - val_accuracy: 0.8760\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 16s 59ms/step - loss: 0.0626 - accuracy: 0.9768 - val_loss: 0.3830 - val_accuracy: 0.8850\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.4167 - val_accuracy: 0.8790\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 19s 66ms/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 0.5834 - val_accuracy: 0.8610\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.0268 - accuracy: 0.9931 - val_loss: 0.5900 - val_accuracy: 0.8640\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0279 - accuracy: 0.9904 - val_loss: 0.6364 - val_accuracy: 0.8650\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.6470 - val_accuracy: 0.8660\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0094 - accuracy: 0.9981 - val_loss: 0.5489 - val_accuracy: 0.8800\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.6890 - val_accuracy: 0.8700\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 1.2134 - val_accuracy: 0.8400\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 0.6790 - val_accuracy: 0.8820\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.6066 - val_accuracy: 0.8840\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 88.49999904632568\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 35s 81ms/step - loss: 0.4942 - accuracy: 0.7481 - val_loss: 0.3497 - val_accuracy: 0.8500\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.2152 - accuracy: 0.9177 - val_loss: 0.2810 - val_accuracy: 0.8770\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.1822 - accuracy: 0.9265 - val_loss: 0.2762 - val_accuracy: 0.8860\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1473 - accuracy: 0.9401 - val_loss: 0.3231 - val_accuracy: 0.8650\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1247 - accuracy: 0.9530 - val_loss: 0.3490 - val_accuracy: 0.8650\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.1059 - accuracy: 0.9647 - val_loss: 0.4872 - val_accuracy: 0.8430\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0873 - accuracy: 0.9708 - val_loss: 0.4382 - val_accuracy: 0.8580\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.0746 - accuracy: 0.9728 - val_loss: 0.3980 - val_accuracy: 0.8720\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 0.0583 - accuracy: 0.9799 - val_loss: 0.5373 - val_accuracy: 0.8530\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0446 - accuracy: 0.9853 - val_loss: 0.5756 - val_accuracy: 0.8480\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.0321 - accuracy: 0.9899 - val_loss: 0.6936 - val_accuracy: 0.8480\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.0276 - accuracy: 0.9902 - val_loss: 0.6887 - val_accuracy: 0.8580\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 0.0162 - accuracy: 0.9959 - val_loss: 0.9601 - val_accuracy: 0.8360\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 88.59999775886536\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  90.600002  89.600003  92.000002  88.800001  90.499997  91.900003   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  86.500001  83.700001  88.499999  88.599998  89.070001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.600002</td>\n",
       "      <td>89.600003</td>\n",
       "      <td>92.000002</td>\n",
       "      <td>88.800001</td>\n",
       "      <td>90.499997</td>\n",
       "      <td>91.900003</td>\n",
       "      <td>86.500001</td>\n",
       "      <td>83.700001</td>\n",
       "      <td>88.499999</td>\n",
       "      <td>88.599998</td>\n",
       "      <td>89.070001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  90.600002  89.600003  92.000002  88.800001  90.499997  91.900003   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  86.500001  83.700001  88.499999  88.599998  89.070001  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_SUBJ_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 440,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "282/282 [==============================] - 56s 150ms/step - loss: 0.4327 - accuracy: 0.7908 - val_loss: 0.2142 - val_accuracy: 0.9160\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 0.0996 - accuracy: 0.9672 - val_loss: 0.2975 - val_accuracy: 0.8970\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 41s 146ms/step - loss: 0.0247 - accuracy: 0.9935 - val_loss: 0.3186 - val_accuracy: 0.9030\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.3898 - val_accuracy: 0.9100\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 43s 154ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.4400 - val_accuracy: 0.8980\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 52s 184ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.4816 - val_accuracy: 0.9030\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 42s 151ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.5274 - val_accuracy: 0.9000\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6183 - val_accuracy: 0.8990\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 49s 173ms/step - loss: 1.1908e-04 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.8970\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 52s 184ms/step - loss: 5.7165e-05 - accuracy: 1.0000 - val_loss: 0.6944 - val_accuracy: 0.9000\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 51s 182ms/step - loss: 5.5184e-05 - accuracy: 1.0000 - val_loss: 0.7211 - val_accuracy: 0.8940\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 91.60000085830688\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 67s 177ms/step - loss: 0.4556 - accuracy: 0.7770 - val_loss: 0.2020 - val_accuracy: 0.9200\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 44s 158ms/step - loss: 0.1007 - accuracy: 0.9658 - val_loss: 0.1805 - val_accuracy: 0.9260\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 42s 151ms/step - loss: 0.0248 - accuracy: 0.9927 - val_loss: 0.2366 - val_accuracy: 0.9150\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.3513 - val_accuracy: 0.9140\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 47s 166ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3420 - val_accuracy: 0.9230\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 52s 183ms/step - loss: 4.7034e-04 - accuracy: 1.0000 - val_loss: 0.4270 - val_accuracy: 0.9250\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 49s 173ms/step - loss: 1.9263e-04 - accuracy: 1.0000 - val_loss: 0.4678 - val_accuracy: 0.9140\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 39s 139ms/step - loss: 1.0055e-04 - accuracy: 1.0000 - val_loss: 0.4831 - val_accuracy: 0.9150\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 42s 149ms/step - loss: 6.8929e-05 - accuracy: 1.0000 - val_loss: 0.5285 - val_accuracy: 0.9090\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 4.5171e-05 - accuracy: 1.0000 - val_loss: 0.5269 - val_accuracy: 0.9140\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 39s 139ms/step - loss: 3.9968e-05 - accuracy: 1.0000 - val_loss: 0.5316 - val_accuracy: 0.9140\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 39s 139ms/step - loss: 3.0612e-05 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.9120\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 92.59999990463257\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 57s 155ms/step - loss: 0.4579 - accuracy: 0.7663 - val_loss: 0.2466 - val_accuracy: 0.8990\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 40s 143ms/step - loss: 0.0993 - accuracy: 0.9665 - val_loss: 0.2118 - val_accuracy: 0.9200\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.2892 - val_accuracy: 0.9030\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 51s 180ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.3412 - val_accuracy: 0.9160\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.3174 - val_accuracy: 0.9260\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.3874 - val_accuracy: 0.9150\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 1.8053e-04 - accuracy: 1.0000 - val_loss: 0.4169 - val_accuracy: 0.9160\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 8.7283e-05 - accuracy: 1.0000 - val_loss: 0.4341 - val_accuracy: 0.9180\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 6.6444e-05 - accuracy: 1.0000 - val_loss: 0.4422 - val_accuracy: 0.9200\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 40s 140ms/step - loss: 4.5854e-05 - accuracy: 1.0000 - val_loss: 0.4571 - val_accuracy: 0.9200\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 3.1125e-05 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.9220\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 38s 136ms/step - loss: 2.3769e-05 - accuracy: 1.0000 - val_loss: 0.4921 - val_accuracy: 0.9190\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 1.6622e-05 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.9200\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 39s 137ms/step - loss: 1.3853e-05 - accuracy: 1.0000 - val_loss: 0.5192 - val_accuracy: 0.9190\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 1.3417e-05 - accuracy: 1.0000 - val_loss: 0.5153 - val_accuracy: 0.9230\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 92.59999990463257\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 67s 153ms/step - loss: 0.4430 - accuracy: 0.7734 - val_loss: 0.1990 - val_accuracy: 0.9180\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 0.0922 - accuracy: 0.9714 - val_loss: 0.2049 - val_accuracy: 0.9100\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0281 - accuracy: 0.9925 - val_loss: 0.2709 - val_accuracy: 0.9080\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 39s 139ms/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 0.3598 - val_accuracy: 0.9010\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.3710 - val_accuracy: 0.9110\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 4.8529e-04 - accuracy: 1.0000 - val_loss: 0.4622 - val_accuracy: 0.9030\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 1.2052e-04 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.9060\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 40s 144ms/step - loss: 1.2797e-04 - accuracy: 1.0000 - val_loss: 0.4986 - val_accuracy: 0.9030\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 40s 143ms/step - loss: 4.9973e-05 - accuracy: 1.0000 - val_loss: 0.5187 - val_accuracy: 0.9060\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 50s 179ms/step - loss: 4.8202e-05 - accuracy: 1.0000 - val_loss: 0.5382 - val_accuracy: 0.9060\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 52s 184ms/step - loss: 2.6618e-05 - accuracy: 1.0000 - val_loss: 0.5476 - val_accuracy: 0.9070\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 91.79999828338623\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 67s 171ms/step - loss: 0.4527 - accuracy: 0.7663 - val_loss: 0.2092 - val_accuracy: 0.9240\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 43s 151ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 0.2017 - val_accuracy: 0.9250\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 43s 152ms/step - loss: 0.0242 - accuracy: 0.9947 - val_loss: 0.2546 - val_accuracy: 0.9180\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 54s 190ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.3488 - val_accuracy: 0.9170\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 57s 202ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.4042 - val_accuracy: 0.9120\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 8.5163e-04 - accuracy: 1.0000 - val_loss: 0.4383 - val_accuracy: 0.9120\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 44s 157ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.4709 - val_accuracy: 0.9030\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.4019 - val_accuracy: 0.9070\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 41s 144ms/step - loss: 7.1222e-04 - accuracy: 1.0000 - val_loss: 0.4996 - val_accuracy: 0.9120\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 9.0017e-05 - accuracy: 1.0000 - val_loss: 0.5345 - val_accuracy: 0.9100\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 4.2364e-05 - accuracy: 1.0000 - val_loss: 0.5625 - val_accuracy: 0.9110\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 2.2550e-05 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 0.9090\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 52s 142ms/step - loss: 0.4536 - accuracy: 0.7686 - val_loss: 0.3019 - val_accuracy: 0.8720\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 0.0932 - accuracy: 0.9693 - val_loss: 0.3061 - val_accuracy: 0.8770\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 0.0213 - accuracy: 0.9947 - val_loss: 0.3556 - val_accuracy: 0.8840\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.4106 - val_accuracy: 0.8660\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 0.0035 - accuracy: 0.9999 - val_loss: 0.5148 - val_accuracy: 0.8720\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 7.4584e-04 - accuracy: 1.0000 - val_loss: 0.5262 - val_accuracy: 0.8680\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 2.6431e-04 - accuracy: 1.0000 - val_loss: 0.6611 - val_accuracy: 0.8660\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 9.2293e-05 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.8680\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 6.7830e-05 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.8700\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 3.8150e-05 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.8710\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 3.0114e-05 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.8790\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 2.4277e-05 - accuracy: 1.0000 - val_loss: 0.7304 - val_accuracy: 0.8730\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 2.0072e-05 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.8740\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 88.40000033378601\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 52s 142ms/step - loss: 0.4348 - accuracy: 0.7906 - val_loss: 0.2153 - val_accuracy: 0.9170\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 37s 130ms/step - loss: 0.0948 - accuracy: 0.9707 - val_loss: 0.1893 - val_accuracy: 0.9250\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 0.0263 - accuracy: 0.9925 - val_loss: 0.2168 - val_accuracy: 0.9180\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 37s 130ms/step - loss: 0.0081 - accuracy: 0.9987 - val_loss: 0.4091 - val_accuracy: 0.8900\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5081 - val_accuracy: 0.8940\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 4.2412e-04 - accuracy: 1.0000 - val_loss: 0.5368 - val_accuracy: 0.8870\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 1.8679e-04 - accuracy: 1.0000 - val_loss: 0.5121 - val_accuracy: 0.9030\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 9.4468e-05 - accuracy: 1.0000 - val_loss: 0.5490 - val_accuracy: 0.9010\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 5.3926e-05 - accuracy: 1.0000 - val_loss: 0.5498 - val_accuracy: 0.9010\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 5.1552e-05 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.9010\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 3.1433e-05 - accuracy: 1.0000 - val_loss: 0.5745 - val_accuracy: 0.9010\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 36s 128ms/step - loss: 2.4450e-05 - accuracy: 1.0000 - val_loss: 0.5937 - val_accuracy: 0.9010\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 52s 142ms/step - loss: 0.4488 - accuracy: 0.7719 - val_loss: 0.2938 - val_accuracy: 0.8810\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 37s 130ms/step - loss: 0.0971 - accuracy: 0.9669 - val_loss: 0.2855 - val_accuracy: 0.8900\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 0.0270 - accuracy: 0.9926 - val_loss: 0.4103 - val_accuracy: 0.8780\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.4211 - val_accuracy: 0.8870\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 41s 147ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.6096 - val_accuracy: 0.8740\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 41s 147ms/step - loss: 5.4432e-04 - accuracy: 1.0000 - val_loss: 0.5777 - val_accuracy: 0.8840\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 38s 133ms/step - loss: 1.9534e-04 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 0.8860\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 1.0131e-04 - accuracy: 1.0000 - val_loss: 0.6758 - val_accuracy: 0.8850\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 5.3039e-05 - accuracy: 1.0000 - val_loss: 0.6956 - val_accuracy: 0.8820\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 37s 130ms/step - loss: 7.9842e-05 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.8830\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 3.0866e-05 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 0.8760\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 3.1717e-05 - accuracy: 1.0000 - val_loss: 0.7631 - val_accuracy: 0.8790\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 88.99999856948853\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 78s 152ms/step - loss: 0.4615 - accuracy: 0.7614 - val_loss: 0.1934 - val_accuracy: 0.9270\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 0.1028 - accuracy: 0.9664 - val_loss: 0.2140 - val_accuracy: 0.9160\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 38s 134ms/step - loss: 0.0311 - accuracy: 0.9923 - val_loss: 0.2319 - val_accuracy: 0.9230\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 39s 140ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.2798 - val_accuracy: 0.9220\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 39s 138ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.3393 - val_accuracy: 0.9110\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 40s 143ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.3982 - val_accuracy: 0.9130\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 41s 145ms/step - loss: 3.8442e-04 - accuracy: 1.0000 - val_loss: 0.4452 - val_accuracy: 0.9030\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 1.1858e-04 - accuracy: 1.0000 - val_loss: 0.4799 - val_accuracy: 0.9060\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 7.3364e-05 - accuracy: 1.0000 - val_loss: 0.5178 - val_accuracy: 0.9060\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 41s 147ms/step - loss: 7.6635e-05 - accuracy: 1.0000 - val_loss: 0.5109 - val_accuracy: 0.9060\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 39s 137ms/step - loss: 3.7062e-05 - accuracy: 1.0000 - val_loss: 0.5343 - val_accuracy: 0.9030\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 92.69999861717224\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 57s 154ms/step - loss: 0.4569 - accuracy: 0.7727 - val_loss: 0.2449 - val_accuracy: 0.9020\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.1005 - accuracy: 0.9647 - val_loss: 0.3021 - val_accuracy: 0.8800\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 40s 144ms/step - loss: 0.0275 - accuracy: 0.9919 - val_loss: 0.3582 - val_accuracy: 0.8910\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 40s 143ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.5426 - val_accuracy: 0.8810\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 40s 143ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.4984 - val_accuracy: 0.8960\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 41s 145ms/step - loss: 4.7961e-04 - accuracy: 0.9999 - val_loss: 0.6309 - val_accuracy: 0.8860\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 1.8728e-04 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 0.8870\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 42s 148ms/step - loss: 9.2935e-05 - accuracy: 1.0000 - val_loss: 0.6600 - val_accuracy: 0.8880\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 39s 137ms/step - loss: 6.0291e-05 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.8870\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 39s 140ms/step - loss: 3.6526e-05 - accuracy: 1.0000 - val_loss: 0.7097 - val_accuracy: 0.8860\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 39s 140ms/step - loss: 4.8678e-05 - accuracy: 1.0000 - val_loss: 0.7293 - val_accuracy: 0.8890\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 90.20000100135803\n",
      "\n",
      "        acc1  acc2       acc3       acc4       acc5       acc6  acc7  acc8  \\\n",
      "0  91.900003  92.1  92.500001  92.400002  91.600001  93.400002  92.1  92.6   \n",
      "\n",
      "        acc9      acc10        AVG  \n",
      "0  91.100001  92.400002  92.210001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.600001</td>\n",
       "      <td>92.6</td>\n",
       "      <td>92.6</td>\n",
       "      <td>91.799998</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>88.4</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>88.999999</td>\n",
       "      <td>92.699999</td>\n",
       "      <td>90.200001</td>\n",
       "      <td>91.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1  acc2  acc3       acc4       acc5  acc6       acc7       acc8  \\\n",
       "0  91.600001  92.6  92.6  91.799998  92.500001  88.4  92.500001  88.999999   \n",
       "\n",
       "        acc9      acc10    AVG  \n",
       "0  92.699999  90.200001  91.39  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_SUBJ_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
