{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Classification with MPQA Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using GRU model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10606, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaining</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failing to support</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desperately needs</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many years of decay</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no quick fix</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>urged</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>hope</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10606 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sentence  label  split\n",
       "0              complaining      0  train\n",
       "1       failing to support      0  train\n",
       "2        desperately needs      0  train\n",
       "3      many years of decay      0  train\n",
       "4             no quick fix      0  train\n",
       "...                    ...    ...    ...\n",
       "10601                urged      1  train\n",
       "10602       strictly abide      1  train\n",
       "10603                 hope      1  train\n",
       "10604       strictly abide      1  train\n",
       "10605                           1  train\n",
       "\n",
       "[10606 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MPQA/MPQA.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10606 non-null  object\n",
      " 1   label     10606 non-null  int32 \n",
      " 2   split     10606 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 207.3+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3312</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          7294   7294\n",
       "1          3312   3312"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complaining'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  no quick fix\n",
      "Into a sequence of int: [25, 945, 1476]\n",
      "Into a padded sequence: [  25  945 1476    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "of 3\n",
      "to 4\n",
      "a 5\n",
      "and 6\n",
      "not 7\n",
      "is 8\n",
      "in 9\n",
      "be 10\n",
      "6236\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 440,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 81s 224ms/step - loss: 0.5523 - accuracy: 0.7316 - val_loss: 0.3673 - val_accuracy: 0.8549\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 62s 208ms/step - loss: 0.2079 - accuracy: 0.9272 - val_loss: 0.3677 - val_accuracy: 0.8511\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 64s 214ms/step - loss: 0.1302 - accuracy: 0.9556 - val_loss: 0.4196 - val_accuracy: 0.8426\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 65s 219ms/step - loss: 0.0987 - accuracy: 0.9646 - val_loss: 0.4448 - val_accuracy: 0.8492\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 65s 219ms/step - loss: 0.0796 - accuracy: 0.9699 - val_loss: 0.5306 - val_accuracy: 0.8426\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 65s 219ms/step - loss: 0.0608 - accuracy: 0.9746 - val_loss: 0.5837 - val_accuracy: 0.8322\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 65s 217ms/step - loss: 0.0496 - accuracy: 0.9798 - val_loss: 0.5672 - val_accuracy: 0.8369\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 67s 223ms/step - loss: 0.0418 - accuracy: 0.9835 - val_loss: 0.6415 - val_accuracy: 0.8341\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 67s 224ms/step - loss: 0.0375 - accuracy: 0.9849 - val_loss: 0.6688 - val_accuracy: 0.8351\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 64s 215ms/step - loss: 0.0365 - accuracy: 0.9846 - val_loss: 0.6798 - val_accuracy: 0.8256\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 63s 210ms/step - loss: 0.0333 - accuracy: 0.9856 - val_loss: 0.7140 - val_accuracy: 0.8247\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 2: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 72s 202ms/step - loss: 0.5496 - accuracy: 0.7363 - val_loss: 0.3748 - val_accuracy: 0.8483\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 53s 177ms/step - loss: 0.2041 - accuracy: 0.9293 - val_loss: 0.3941 - val_accuracy: 0.8228\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 53s 177ms/step - loss: 0.1214 - accuracy: 0.9586 - val_loss: 0.4493 - val_accuracy: 0.8190\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0943 - accuracy: 0.9664 - val_loss: 0.5022 - val_accuracy: 0.8238\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0735 - accuracy: 0.9709 - val_loss: 0.5314 - val_accuracy: 0.8398\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0590 - accuracy: 0.9759 - val_loss: 0.5850 - val_accuracy: 0.8351\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0467 - accuracy: 0.9813 - val_loss: 0.6078 - val_accuracy: 0.8388\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0407 - accuracy: 0.9821 - val_loss: 0.7473 - val_accuracy: 0.8153\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0389 - accuracy: 0.9848 - val_loss: 0.6736 - val_accuracy: 0.8303\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0337 - accuracy: 0.9845 - val_loss: 0.7361 - val_accuracy: 0.8303\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0315 - accuracy: 0.9852 - val_loss: 0.7564 - val_accuracy: 0.8238\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 84.82563495635986\n",
      "Training 3: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 49s 130ms/step - loss: 0.5532 - accuracy: 0.7337 - val_loss: 0.3735 - val_accuracy: 0.8435\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 34s 113ms/step - loss: 0.2109 - accuracy: 0.9315 - val_loss: 0.3947 - val_accuracy: 0.8511\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 38s 126ms/step - loss: 0.1284 - accuracy: 0.9541 - val_loss: 0.4736 - val_accuracy: 0.8483\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.0924 - accuracy: 0.9661 - val_loss: 0.5122 - val_accuracy: 0.8445\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.0787 - accuracy: 0.9681 - val_loss: 0.5603 - val_accuracy: 0.8464\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.0629 - accuracy: 0.9736 - val_loss: 0.6193 - val_accuracy: 0.8398\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 39s 129ms/step - loss: 0.0536 - accuracy: 0.9804 - val_loss: 0.6276 - val_accuracy: 0.8511\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 39s 130ms/step - loss: 0.0409 - accuracy: 0.9853 - val_loss: 0.6749 - val_accuracy: 0.8435\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 39s 130ms/step - loss: 0.0379 - accuracy: 0.9838 - val_loss: 0.6877 - val_accuracy: 0.8379\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 39s 130ms/step - loss: 0.0316 - accuracy: 0.9873 - val_loss: 0.6939 - val_accuracy: 0.8407\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 40s 134ms/step - loss: 0.0336 - accuracy: 0.9843 - val_loss: 0.7612 - val_accuracy: 0.8360\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 40s 134ms/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 0.7751 - val_accuracy: 0.8332\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 85.10838747024536\n",
      "Training 4: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 49s 128ms/step - loss: 0.5477 - accuracy: 0.7328 - val_loss: 0.3630 - val_accuracy: 0.8549\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 35s 117ms/step - loss: 0.2056 - accuracy: 0.9314 - val_loss: 0.3935 - val_accuracy: 0.8530\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.1288 - accuracy: 0.9563 - val_loss: 0.4402 - val_accuracy: 0.8417\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0983 - accuracy: 0.9617 - val_loss: 0.4721 - val_accuracy: 0.8454\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0781 - accuracy: 0.9706 - val_loss: 0.5160 - val_accuracy: 0.8445\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 35s 119ms/step - loss: 0.0726 - accuracy: 0.9726 - val_loss: 0.5660 - val_accuracy: 0.8454\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0565 - accuracy: 0.9775 - val_loss: 0.6008 - val_accuracy: 0.8445\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0465 - accuracy: 0.9808 - val_loss: 0.6408 - val_accuracy: 0.8454\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0437 - accuracy: 0.9822 - val_loss: 0.6787 - val_accuracy: 0.8435\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 0.7094 - val_accuracy: 0.8360\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 35s 116ms/step - loss: 0.0366 - accuracy: 0.9848 - val_loss: 0.7220 - val_accuracy: 0.8303\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 5: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 87s 243ms/step - loss: 0.5526 - accuracy: 0.7258 - val_loss: 0.3431 - val_accuracy: 0.8558\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 69s 230ms/step - loss: 0.2064 - accuracy: 0.9290 - val_loss: 0.3616 - val_accuracy: 0.8643\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.1371 - accuracy: 0.9506 - val_loss: 0.3931 - val_accuracy: 0.8483\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0921 - accuracy: 0.9669 - val_loss: 0.4298 - val_accuracy: 0.8511\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0755 - accuracy: 0.9738 - val_loss: 0.4834 - val_accuracy: 0.8511\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0617 - accuracy: 0.9742 - val_loss: 0.5005 - val_accuracy: 0.8549\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0496 - accuracy: 0.9809 - val_loss: 0.5683 - val_accuracy: 0.8369\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0458 - accuracy: 0.9822 - val_loss: 0.6116 - val_accuracy: 0.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "299/299 [==============================] - 69s 232ms/step - loss: 0.0430 - accuracy: 0.9814 - val_loss: 0.6089 - val_accuracy: 0.8464\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0385 - accuracy: 0.9832 - val_loss: 0.6631 - val_accuracy: 0.8369\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 69s 232ms/step - loss: 0.0336 - accuracy: 0.9863 - val_loss: 0.6521 - val_accuracy: 0.8388\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0373 - accuracy: 0.9837 - val_loss: 0.6265 - val_accuracy: 0.8501\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 86.42789721488953\n",
      "Training 6: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 88s 255ms/step - loss: 0.5478 - accuracy: 0.7366 - val_loss: 0.3749 - val_accuracy: 0.8539\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 75s 251ms/step - loss: 0.2024 - accuracy: 0.9304 - val_loss: 0.3710 - val_accuracy: 0.8586\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.1351 - accuracy: 0.9531 - val_loss: 0.4320 - val_accuracy: 0.8464\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 73s 246ms/step - loss: 0.0964 - accuracy: 0.9656 - val_loss: 0.4282 - val_accuracy: 0.8360\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0765 - accuracy: 0.9716 - val_loss: 0.4769 - val_accuracy: 0.8520\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0622 - accuracy: 0.9764 - val_loss: 0.5322 - val_accuracy: 0.8530\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 73s 243ms/step - loss: 0.0529 - accuracy: 0.9778 - val_loss: 0.5640 - val_accuracy: 0.8426\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0424 - accuracy: 0.9837 - val_loss: 0.5934 - val_accuracy: 0.8407\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0404 - accuracy: 0.9830 - val_loss: 0.5829 - val_accuracy: 0.8426\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0323 - accuracy: 0.9879 - val_loss: 0.6287 - val_accuracy: 0.8417\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0328 - accuracy: 0.9864 - val_loss: 0.6450 - val_accuracy: 0.8369\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0316 - accuracy: 0.9851 - val_loss: 0.6586 - val_accuracy: 0.8351\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 85.86239218711853\n",
      "Training 7: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 84s 242ms/step - loss: 0.5507 - accuracy: 0.7386 - val_loss: 0.3036 - val_accuracy: 0.8755\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 59s 197ms/step - loss: 0.2124 - accuracy: 0.9262 - val_loss: 0.3063 - val_accuracy: 0.8840\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 58s 194ms/step - loss: 0.1341 - accuracy: 0.9538 - val_loss: 0.3496 - val_accuracy: 0.8774\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 57s 192ms/step - loss: 0.1036 - accuracy: 0.9618 - val_loss: 0.3500 - val_accuracy: 0.8764\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 57s 191ms/step - loss: 0.0798 - accuracy: 0.9697 - val_loss: 0.4107 - val_accuracy: 0.8755\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 56s 189ms/step - loss: 0.0580 - accuracy: 0.9775 - val_loss: 0.4131 - val_accuracy: 0.8698\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0514 - accuracy: 0.9780 - val_loss: 0.4912 - val_accuracy: 0.8708\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0464 - accuracy: 0.9798 - val_loss: 0.5044 - val_accuracy: 0.8632\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0370 - accuracy: 0.9852 - val_loss: 0.5452 - val_accuracy: 0.8613\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0352 - accuracy: 0.9849 - val_loss: 0.5574 - val_accuracy: 0.8491\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 56s 187ms/step - loss: 0.0355 - accuracy: 0.9852 - val_loss: 0.5965 - val_accuracy: 0.8623\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0387 - accuracy: 0.9834 - val_loss: 0.5835 - val_accuracy: 0.8632\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 88.39622735977173\n",
      "Training 8: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 65s 179ms/step - loss: 0.5472 - accuracy: 0.7345 - val_loss: 0.3331 - val_accuracy: 0.8745\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.2011 - accuracy: 0.9313 - val_loss: 0.3478 - val_accuracy: 0.8679\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.1254 - accuracy: 0.9530 - val_loss: 0.3917 - val_accuracy: 0.8566\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0978 - accuracy: 0.9631 - val_loss: 0.4550 - val_accuracy: 0.8585\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0773 - accuracy: 0.9714 - val_loss: 0.4608 - val_accuracy: 0.8632\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0616 - accuracy: 0.9767 - val_loss: 0.5280 - val_accuracy: 0.8557\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0498 - accuracy: 0.9787 - val_loss: 0.5505 - val_accuracy: 0.8547\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0476 - accuracy: 0.9799 - val_loss: 0.5617 - val_accuracy: 0.8528\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0421 - accuracy: 0.9838 - val_loss: 0.6227 - val_accuracy: 0.8528\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0362 - accuracy: 0.9815 - val_loss: 0.6173 - val_accuracy: 0.8585\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0353 - accuracy: 0.9830 - val_loss: 0.7099 - val_accuracy: 0.8462\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 87.45282888412476\n",
      "Training 9: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 72s 191ms/step - loss: 0.5465 - accuracy: 0.7330 - val_loss: 0.3913 - val_accuracy: 0.8406\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.2038 - accuracy: 0.9263 - val_loss: 0.3978 - val_accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.1238 - accuracy: 0.9568 - val_loss: 0.4655 - val_accuracy: 0.8481\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.0975 - accuracy: 0.9668 - val_loss: 0.4842 - val_accuracy: 0.8528\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.0766 - accuracy: 0.9725 - val_loss: 0.5355 - val_accuracy: 0.8415\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 53s 178ms/step - loss: 0.0597 - accuracy: 0.9787 - val_loss: 0.5772 - val_accuracy: 0.8349\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.0533 - accuracy: 0.9785 - val_loss: 0.6283 - val_accuracy: 0.8387\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 53s 178ms/step - loss: 0.0482 - accuracy: 0.9802 - val_loss: 0.6423 - val_accuracy: 0.8528\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0445 - accuracy: 0.9809 - val_loss: 0.6751 - val_accuracy: 0.8462\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0385 - accuracy: 0.9833 - val_loss: 0.6957 - val_accuracy: 0.8425\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0388 - accuracy: 0.9820 - val_loss: 0.6772 - val_accuracy: 0.8425\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 54s 180ms/step - loss: 0.0340 - accuracy: 0.9854 - val_loss: 0.7144 - val_accuracy: 0.8434\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0297 - accuracy: 0.9857 - val_loss: 0.7183 - val_accuracy: 0.8330\n",
      "Epoch 14/30\n",
      "299/299 [==============================] - 54s 180ms/step - loss: 0.0318 - accuracy: 0.9841 - val_loss: 0.7124 - val_accuracy: 0.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 85.2830171585083\n",
      "Training 10: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 72s 201ms/step - loss: 0.5508 - accuracy: 0.7293 - val_loss: 0.3428 - val_accuracy: 0.8670\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 50s 167ms/step - loss: 0.2130 - accuracy: 0.9264 - val_loss: 0.3522 - val_accuracy: 0.8717\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.1216 - accuracy: 0.9588 - val_loss: 0.3963 - val_accuracy: 0.8642\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.0999 - accuracy: 0.9628 - val_loss: 0.4265 - val_accuracy: 0.8613\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0748 - accuracy: 0.9732 - val_loss: 0.4726 - val_accuracy: 0.8509\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.0565 - accuracy: 0.9776 - val_loss: 0.4931 - val_accuracy: 0.8689\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0415 - accuracy: 0.9836 - val_loss: 0.5557 - val_accuracy: 0.8642\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0378 - accuracy: 0.9840 - val_loss: 0.5489 - val_accuracy: 0.8632\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 48s 161ms/step - loss: 0.0345 - accuracy: 0.9850 - val_loss: 0.6671 - val_accuracy: 0.8528\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 48s 160ms/step - loss: 0.0325 - accuracy: 0.9861 - val_loss: 0.5860 - val_accuracy: 0.8585\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0330 - accuracy: 0.9874 - val_loss: 0.7261 - val_accuracy: 0.8415\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.0321 - accuracy: 0.9869 - val_loss: 0.6860 - val_accuracy: 0.8425\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 87.16981410980225\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  85.485393  84.825635  85.108387  85.485393  86.427897  85.862392   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  88.396227  87.452829  85.283017  87.169814  86.149698  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.485393</td>\n",
       "      <td>84.825635</td>\n",
       "      <td>85.108387</td>\n",
       "      <td>85.485393</td>\n",
       "      <td>86.427897</td>\n",
       "      <td>85.862392</td>\n",
       "      <td>88.396227</td>\n",
       "      <td>87.452829</td>\n",
       "      <td>85.283017</td>\n",
       "      <td>87.169814</td>\n",
       "      <td>86.149698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  85.485393  84.825635  85.108387  85.485393  86.427897  85.862392   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  88.396227  87.452829  85.283017  87.169814  86.149698  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_MPQA.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6083 words present from 6236 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.6320836 , -0.39995871,  0.30284232, ..., -0.28593645,\n",
       "        -0.02975578, -0.58115922],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 140,673\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 46s 118ms/step - loss: 0.4690 - accuracy: 0.7794 - val_loss: 0.3482 - val_accuracy: 0.8662\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 31s 103ms/step - loss: 0.3094 - accuracy: 0.8782 - val_loss: 0.4063 - val_accuracy: 0.8699\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.2737 - accuracy: 0.8955 - val_loss: 0.4700 - val_accuracy: 0.8718\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2565 - accuracy: 0.9019 - val_loss: 0.4764 - val_accuracy: 0.8671\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2309 - accuracy: 0.9101 - val_loss: 0.5298 - val_accuracy: 0.8671\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.2185 - accuracy: 0.9163 - val_loss: 0.6000 - val_accuracy: 0.8728\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2093 - accuracy: 0.9168 - val_loss: 0.6157 - val_accuracy: 0.8746\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.1889 - accuracy: 0.9286 - val_loss: 0.6649 - val_accuracy: 0.8737\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 29s 99ms/step - loss: 0.1781 - accuracy: 0.9349 - val_loss: 0.7069 - val_accuracy: 0.8756\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1645 - accuracy: 0.9405 - val_loss: 0.7189 - val_accuracy: 0.8746\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1526 - accuracy: 0.9418 - val_loss: 0.8040 - val_accuracy: 0.8765\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.1409 - accuracy: 0.9490 - val_loss: 0.8285 - val_accuracy: 0.8756\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1286 - accuracy: 0.9491 - val_loss: 0.8648 - val_accuracy: 0.8680\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1273 - accuracy: 0.9499 - val_loss: 0.8876 - val_accuracy: 0.8728\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1022 - accuracy: 0.9638 - val_loss: 0.9108 - val_accuracy: 0.8765\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0987 - accuracy: 0.9618 - val_loss: 0.9060 - val_accuracy: 0.8709\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.1145 - accuracy: 0.9588 - val_loss: 0.9081 - val_accuracy: 0.8737\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0996 - accuracy: 0.9603 - val_loss: 0.9996 - val_accuracy: 0.8775\n",
      "Epoch 19/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0875 - accuracy: 0.9656 - val_loss: 0.9880 - val_accuracy: 0.8737\n",
      "Epoch 20/40\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0857 - accuracy: 0.9651 - val_loss: 0.9864 - val_accuracy: 0.8690\n",
      "Epoch 21/40\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0768 - accuracy: 0.9722 - val_loss: 1.0756 - val_accuracy: 0.8718\n",
      "Epoch 22/40\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0835 - accuracy: 0.9674 - val_loss: 1.0781 - val_accuracy: 0.8728\n",
      "Epoch 23/40\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0739 - accuracy: 0.9719 - val_loss: 1.0139 - val_accuracy: 0.8709\n",
      "Epoch 24/40\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0744 - accuracy: 0.9710 - val_loss: 1.1330 - val_accuracy: 0.8709\n",
      "Epoch 25/40\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0672 - accuracy: 0.9757 - val_loss: 1.1490 - val_accuracy: 0.8765\n",
      "Epoch 26/40\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0603 - accuracy: 0.9779 - val_loss: 1.1297 - val_accuracy: 0.8680\n",
      "Epoch 27/40\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0637 - accuracy: 0.9760 - val_loss: 1.1671 - val_accuracy: 0.8662\n",
      "Epoch 28/40\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0607 - accuracy: 0.9778 - val_loss: 1.1739 - val_accuracy: 0.8671\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 87.74740695953369\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 48s 122ms/step - loss: 0.4652 - accuracy: 0.7756 - val_loss: 0.5807 - val_accuracy: 0.7644\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 33s 110ms/step - loss: 0.3016 - accuracy: 0.8851 - val_loss: 0.6190 - val_accuracy: 0.7795\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2673 - accuracy: 0.9005 - val_loss: 0.6160 - val_accuracy: 0.7851\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2498 - accuracy: 0.9106 - val_loss: 0.5971 - val_accuracy: 0.7983\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2419 - accuracy: 0.9112 - val_loss: 0.6093 - val_accuracy: 0.8077\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2279 - accuracy: 0.9186 - val_loss: 0.6832 - val_accuracy: 0.8049\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2021 - accuracy: 0.9238 - val_loss: 0.6880 - val_accuracy: 0.7926\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2031 - accuracy: 0.9212 - val_loss: 0.6976 - val_accuracy: 0.8068\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.1820 - accuracy: 0.9298 - val_loss: 0.7374 - val_accuracy: 0.8049\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.1595 - accuracy: 0.9414 - val_loss: 0.7706 - val_accuracy: 0.8002\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1500 - accuracy: 0.9454 - val_loss: 0.7973 - val_accuracy: 0.8096\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1453 - accuracy: 0.9438 - val_loss: 0.8886 - val_accuracy: 0.8077\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1278 - accuracy: 0.9519 - val_loss: 0.8794 - val_accuracy: 0.8077\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1107 - accuracy: 0.9589 - val_loss: 0.9314 - val_accuracy: 0.8030\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1170 - accuracy: 0.9565 - val_loss: 0.9523 - val_accuracy: 0.8002\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1020 - accuracy: 0.9620 - val_loss: 0.9911 - val_accuracy: 0.8087\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0973 - accuracy: 0.9631 - val_loss: 1.0718 - val_accuracy: 0.7974\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0874 - accuracy: 0.9684 - val_loss: 1.1651 - val_accuracy: 0.7795\n",
      "Epoch 19/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0858 - accuracy: 0.9688 - val_loss: 1.2703 - val_accuracy: 0.7879\n",
      "Epoch 20/40\n",
      "299/299 [==============================] - 32s 105ms/step - loss: 0.0770 - accuracy: 0.9708 - val_loss: 1.2337 - val_accuracy: 0.7936\n",
      "Epoch 21/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.0734 - accuracy: 0.9721 - val_loss: 1.1344 - val_accuracy: 0.8096\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 80.9613585472107\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 46s 118ms/step - loss: 0.4565 - accuracy: 0.7879 - val_loss: 0.3902 - val_accuracy: 0.8643\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.2907 - accuracy: 0.8838 - val_loss: 0.3989 - val_accuracy: 0.8586\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2774 - accuracy: 0.8958 - val_loss: 0.3953 - val_accuracy: 0.8615\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 32s 109ms/step - loss: 0.2543 - accuracy: 0.9025 - val_loss: 0.4286 - val_accuracy: 0.8709\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2369 - accuracy: 0.9087 - val_loss: 0.4262 - val_accuracy: 0.8671\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 33s 109ms/step - loss: 0.2261 - accuracy: 0.9147 - val_loss: 0.4344 - val_accuracy: 0.8690\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 33s 109ms/step - loss: 0.2034 - accuracy: 0.9227 - val_loss: 0.4543 - val_accuracy: 0.8671\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 33s 109ms/step - loss: 0.1863 - accuracy: 0.9275 - val_loss: 0.4710 - val_accuracy: 0.8662\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 33s 109ms/step - loss: 0.1736 - accuracy: 0.9330 - val_loss: 0.4818 - val_accuracy: 0.8596\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 33s 109ms/step - loss: 0.1613 - accuracy: 0.9383 - val_loss: 0.5146 - val_accuracy: 0.8539\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 33s 110ms/step - loss: 0.1427 - accuracy: 0.9514 - val_loss: 0.5536 - val_accuracy: 0.8558\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 33s 110ms/step - loss: 0.1421 - accuracy: 0.9474 - val_loss: 0.5424 - val_accuracy: 0.8624\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 33s 111ms/step - loss: 0.1314 - accuracy: 0.9527 - val_loss: 0.6181 - val_accuracy: 0.8624\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 33s 111ms/step - loss: 0.1194 - accuracy: 0.9530 - val_loss: 0.6190 - val_accuracy: 0.8624\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 87.08765506744385\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 47s 122ms/step - loss: 0.4738 - accuracy: 0.7668 - val_loss: 0.5581 - val_accuracy: 0.7738\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.3075 - accuracy: 0.8800 - val_loss: 0.5119 - val_accuracy: 0.7992\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2657 - accuracy: 0.8971 - val_loss: 0.5204 - val_accuracy: 0.7992\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2546 - accuracy: 0.9037 - val_loss: 0.6570 - val_accuracy: 0.7898\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2427 - accuracy: 0.9067 - val_loss: 0.5893 - val_accuracy: 0.8030\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2239 - accuracy: 0.9129 - val_loss: 0.6566 - val_accuracy: 0.7974\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.2007 - accuracy: 0.9245 - val_loss: 0.6599 - val_accuracy: 0.7879\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.1922 - accuracy: 0.9261 - val_loss: 0.7087 - val_accuracy: 0.7974\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.1685 - accuracy: 0.9367 - val_loss: 0.6850 - val_accuracy: 0.7974\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.1592 - accuracy: 0.9419 - val_loss: 0.7196 - val_accuracy: 0.8002\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.1405 - accuracy: 0.9470 - val_loss: 0.7273 - val_accuracy: 0.8068\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1475 - accuracy: 0.9459 - val_loss: 0.8575 - val_accuracy: 0.7964\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1271 - accuracy: 0.9505 - val_loss: 0.9322 - val_accuracy: 0.7861\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1168 - accuracy: 0.9556 - val_loss: 0.8797 - val_accuracy: 0.8040\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1069 - accuracy: 0.9597 - val_loss: 0.9743 - val_accuracy: 0.7945\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1000 - accuracy: 0.9632 - val_loss: 0.9540 - val_accuracy: 0.7983\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.0975 - accuracy: 0.9638 - val_loss: 0.8743 - val_accuracy: 0.8002\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.0871 - accuracy: 0.9675 - val_loss: 0.8457 - val_accuracy: 0.7983\n",
      "Epoch 19/40\n",
      "299/299 [==============================] - 32s 107ms/step - loss: 0.0805 - accuracy: 0.9710 - val_loss: 0.9447 - val_accuracy: 0.7945\n",
      "Epoch 20/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.0767 - accuracy: 0.9709 - val_loss: 1.1329 - val_accuracy: 0.7889\n",
      "Epoch 21/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.0745 - accuracy: 0.9737 - val_loss: 1.0015 - val_accuracy: 0.7974\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 80.6786060333252\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 44s 113ms/step - loss: 0.4663 - accuracy: 0.7783 - val_loss: 0.4671 - val_accuracy: 0.8558\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 30s 100ms/step - loss: 0.2941 - accuracy: 0.8859 - val_loss: 0.5185 - val_accuracy: 0.8567\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 31s 103ms/step - loss: 0.2858 - accuracy: 0.8880 - val_loss: 0.5362 - val_accuracy: 0.8530\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 31s 103ms/step - loss: 0.2557 - accuracy: 0.9017 - val_loss: 0.5824 - val_accuracy: 0.8624\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 31s 103ms/step - loss: 0.2442 - accuracy: 0.9052 - val_loss: 0.6099 - val_accuracy: 0.8680\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.2275 - accuracy: 0.9168 - val_loss: 0.6254 - val_accuracy: 0.8690\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.2049 - accuracy: 0.9202 - val_loss: 0.6241 - val_accuracy: 0.8615\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1961 - accuracy: 0.9241 - val_loss: 0.6549 - val_accuracy: 0.8633\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1746 - accuracy: 0.9309 - val_loss: 0.6897 - val_accuracy: 0.8596\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1641 - accuracy: 0.9350 - val_loss: 0.6777 - val_accuracy: 0.8633\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1492 - accuracy: 0.9412 - val_loss: 0.7365 - val_accuracy: 0.8567\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1376 - accuracy: 0.9483 - val_loss: 0.7636 - val_accuracy: 0.8567\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1212 - accuracy: 0.9551 - val_loss: 0.7488 - val_accuracy: 0.8539\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1203 - accuracy: 0.9551 - val_loss: 0.7893 - val_accuracy: 0.8624\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1148 - accuracy: 0.9560 - val_loss: 0.8228 - val_accuracy: 0.8567\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1004 - accuracy: 0.9652 - val_loss: 0.8706 - val_accuracy: 0.8596\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 86.8991494178772\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 42s 105ms/step - loss: 0.4554 - accuracy: 0.7772 - val_loss: 0.4078 - val_accuracy: 0.8671\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 27s 89ms/step - loss: 0.3007 - accuracy: 0.8826 - val_loss: 0.4719 - val_accuracy: 0.8728\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 26s 89ms/step - loss: 0.2650 - accuracy: 0.8978 - val_loss: 0.5031 - val_accuracy: 0.8756\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.2596 - accuracy: 0.8983 - val_loss: 0.5646 - val_accuracy: 0.8794\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.2346 - accuracy: 0.9080 - val_loss: 0.5613 - val_accuracy: 0.8803\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.2292 - accuracy: 0.9111 - val_loss: 0.5557 - val_accuracy: 0.8765\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.2096 - accuracy: 0.9145 - val_loss: 0.5433 - val_accuracy: 0.8746\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.1860 - accuracy: 0.9311 - val_loss: 0.5721 - val_accuracy: 0.8746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.1756 - accuracy: 0.9346 - val_loss: 0.6173 - val_accuracy: 0.8765\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.1566 - accuracy: 0.9430 - val_loss: 0.6106 - val_accuracy: 0.8794\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.1493 - accuracy: 0.9434 - val_loss: 0.6469 - val_accuracy: 0.8737\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.1355 - accuracy: 0.9496 - val_loss: 0.7109 - val_accuracy: 0.8690\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 26s 87ms/step - loss: 0.1263 - accuracy: 0.9525 - val_loss: 0.7669 - val_accuracy: 0.8633\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 26s 89ms/step - loss: 0.1516 - accuracy: 0.9431 - val_loss: 0.7683 - val_accuracy: 0.8756\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.1054 - accuracy: 0.9603 - val_loss: 0.7538 - val_accuracy: 0.8718\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 88.03015947341919\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 77s 199ms/step - loss: 0.4658 - accuracy: 0.7790 - val_loss: 0.4760 - val_accuracy: 0.8575\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2935 - accuracy: 0.8848 - val_loss: 0.5626 - val_accuracy: 0.8585\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2708 - accuracy: 0.8992 - val_loss: 0.6039 - val_accuracy: 0.8613\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2466 - accuracy: 0.9067 - val_loss: 0.6130 - val_accuracy: 0.8670\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2351 - accuracy: 0.9098 - val_loss: 0.6378 - val_accuracy: 0.8632\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2182 - accuracy: 0.9215 - val_loss: 0.6445 - val_accuracy: 0.8642\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2161 - accuracy: 0.9158 - val_loss: 0.6389 - val_accuracy: 0.8651\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.2000 - accuracy: 0.9197 - val_loss: 0.6983 - val_accuracy: 0.8689\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 55s 186ms/step - loss: 0.1735 - accuracy: 0.9349 - val_loss: 0.7027 - val_accuracy: 0.8642\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 57s 190ms/step - loss: 0.1626 - accuracy: 0.9361 - val_loss: 0.7602 - val_accuracy: 0.8557\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 56s 187ms/step - loss: 0.1596 - accuracy: 0.9411 - val_loss: 0.8786 - val_accuracy: 0.8632\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 54s 182ms/step - loss: 0.1440 - accuracy: 0.9450 - val_loss: 0.9382 - val_accuracy: 0.8604\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.1361 - accuracy: 0.9488 - val_loss: 0.9319 - val_accuracy: 0.8566\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 58s 193ms/step - loss: 0.1225 - accuracy: 0.9513 - val_loss: 1.0148 - val_accuracy: 0.8528\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 57s 191ms/step - loss: 0.1142 - accuracy: 0.9544 - val_loss: 1.0681 - val_accuracy: 0.8519\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.0995 - accuracy: 0.9633 - val_loss: 1.1195 - val_accuracy: 0.8566\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1042 - accuracy: 0.9596 - val_loss: 1.1998 - val_accuracy: 0.8594\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 55s 185ms/step - loss: 0.1004 - accuracy: 0.9641 - val_loss: 1.2358 - val_accuracy: 0.8557\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 86.88679337501526\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 69s 195ms/step - loss: 0.4658 - accuracy: 0.7749 - val_loss: 0.3457 - val_accuracy: 0.8651\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.2962 - accuracy: 0.8869 - val_loss: 0.3484 - val_accuracy: 0.8708\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 48s 161ms/step - loss: 0.2648 - accuracy: 0.8997 - val_loss: 0.3588 - val_accuracy: 0.8717\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 48s 159ms/step - loss: 0.2490 - accuracy: 0.9064 - val_loss: 0.3709 - val_accuracy: 0.8792\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 48s 159ms/step - loss: 0.2342 - accuracy: 0.9113 - val_loss: 0.3657 - val_accuracy: 0.8755\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 48s 159ms/step - loss: 0.2213 - accuracy: 0.9138 - val_loss: 0.3797 - val_accuracy: 0.8755\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 48s 160ms/step - loss: 0.2093 - accuracy: 0.9208 - val_loss: 0.4085 - val_accuracy: 0.8708\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 48s 159ms/step - loss: 0.1961 - accuracy: 0.9245 - val_loss: 0.4066 - val_accuracy: 0.8764\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 48s 159ms/step - loss: 0.1798 - accuracy: 0.9316 - val_loss: 0.4147 - val_accuracy: 0.8755\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 48s 159ms/step - loss: 0.1736 - accuracy: 0.9348 - val_loss: 0.4710 - val_accuracy: 0.8764\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 47s 159ms/step - loss: 0.1559 - accuracy: 0.9431 - val_loss: 0.5244 - val_accuracy: 0.8736\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 47s 158ms/step - loss: 0.1468 - accuracy: 0.9489 - val_loss: 0.4740 - val_accuracy: 0.8745\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 47s 158ms/step - loss: 0.1316 - accuracy: 0.9497 - val_loss: 0.4761 - val_accuracy: 0.8792\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 47s 157ms/step - loss: 0.1211 - accuracy: 0.9556 - val_loss: 0.4874 - val_accuracy: 0.8717\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 87.92452812194824\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 67s 186ms/step - loss: 0.4681 - accuracy: 0.7850 - val_loss: 0.3725 - val_accuracy: 0.8472\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 51s 170ms/step - loss: 0.2972 - accuracy: 0.8839 - val_loss: 0.3886 - val_accuracy: 0.8481\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.2688 - accuracy: 0.8962 - val_loss: 0.3961 - val_accuracy: 0.8538\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.2530 - accuracy: 0.9022 - val_loss: 0.4097 - val_accuracy: 0.8538\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.2352 - accuracy: 0.9105 - val_loss: 0.3983 - val_accuracy: 0.8557\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.2228 - accuracy: 0.9163 - val_loss: 0.4002 - val_accuracy: 0.8547\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.2106 - accuracy: 0.9165 - val_loss: 0.4056 - val_accuracy: 0.8594\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.1871 - accuracy: 0.9285 - val_loss: 0.4145 - val_accuracy: 0.8594\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.1809 - accuracy: 0.9288 - val_loss: 0.3877 - val_accuracy: 0.8557\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.1580 - accuracy: 0.9408 - val_loss: 0.4142 - val_accuracy: 0.8557\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.1497 - accuracy: 0.9415 - val_loss: 0.4263 - val_accuracy: 0.8538\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.1372 - accuracy: 0.9474 - val_loss: 0.4723 - val_accuracy: 0.8566\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.1306 - accuracy: 0.9501 - val_loss: 0.5184 - val_accuracy: 0.8236\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.1104 - accuracy: 0.9620 - val_loss: 0.5194 - val_accuracy: 0.8519\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.1091 - accuracy: 0.9595 - val_loss: 0.5939 - val_accuracy: 0.8575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.1135 - accuracy: 0.9601 - val_loss: 0.5846 - val_accuracy: 0.8170\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0935 - accuracy: 0.9680 - val_loss: 0.5995 - val_accuracy: 0.8198\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 85.94339489936829\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 63s 175ms/step - loss: 0.4792 - accuracy: 0.7663 - val_loss: 0.3379 - val_accuracy: 0.8566\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 47s 158ms/step - loss: 0.2952 - accuracy: 0.8845 - val_loss: 0.3136 - val_accuracy: 0.8689\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 47s 156ms/step - loss: 0.2719 - accuracy: 0.8956 - val_loss: 0.3180 - val_accuracy: 0.8708\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 46s 155ms/step - loss: 0.2521 - accuracy: 0.9028 - val_loss: 0.3692 - val_accuracy: 0.8708\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.2352 - accuracy: 0.9105 - val_loss: 0.3487 - val_accuracy: 0.8717\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.2175 - accuracy: 0.9154 - val_loss: 0.3597 - val_accuracy: 0.8745\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.2081 - accuracy: 0.9188 - val_loss: 0.3556 - val_accuracy: 0.8755\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1886 - accuracy: 0.9303 - val_loss: 0.3499 - val_accuracy: 0.8764\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1819 - accuracy: 0.9313 - val_loss: 0.3610 - val_accuracy: 0.8745\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.1627 - accuracy: 0.9398 - val_loss: 0.3918 - val_accuracy: 0.8755\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1527 - accuracy: 0.9430 - val_loss: 0.3835 - val_accuracy: 0.8764\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1369 - accuracy: 0.9482 - val_loss: 0.4044 - val_accuracy: 0.8670\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1245 - accuracy: 0.9548 - val_loss: 0.4420 - val_accuracy: 0.8745\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1103 - accuracy: 0.9593 - val_loss: 0.4403 - val_accuracy: 0.8708\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1115 - accuracy: 0.9558 - val_loss: 0.4857 - val_accuracy: 0.8642\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1052 - accuracy: 0.9591 - val_loss: 0.4621 - val_accuracy: 0.8651\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.0946 - accuracy: 0.9638 - val_loss: 0.4973 - val_accuracy: 0.8594\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 45s 149ms/step - loss: 0.0830 - accuracy: 0.9688 - val_loss: 0.5089 - val_accuracy: 0.8547\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 87.64150738716125\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  87.747407  80.961359  87.087655  80.678606  86.899149  88.030159   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  86.886793  87.924528  85.943395  87.641507  85.980056  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.747407</td>\n",
       "      <td>80.961359</td>\n",
       "      <td>87.087655</td>\n",
       "      <td>80.678606</td>\n",
       "      <td>86.899149</td>\n",
       "      <td>88.030159</td>\n",
       "      <td>86.886793</td>\n",
       "      <td>87.924528</td>\n",
       "      <td>85.943395</td>\n",
       "      <td>87.641507</td>\n",
       "      <td>85.980056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  87.747407  80.961359  87.087655  80.678606  86.899149  88.030159   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  86.886793  87.924528  85.943395  87.641507  85.980056  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_MPQA_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 440,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 86s 240ms/step - loss: 0.4582 - accuracy: 0.7847 - val_loss: 0.3267 - val_accuracy: 0.8464\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 68s 228ms/step - loss: 0.1929 - accuracy: 0.9336 - val_loss: 0.3510 - val_accuracy: 0.8407\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.1359 - accuracy: 0.9528 - val_loss: 0.3999 - val_accuracy: 0.8369\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0970 - accuracy: 0.9665 - val_loss: 0.4513 - val_accuracy: 0.8247\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0841 - accuracy: 0.9693 - val_loss: 0.4234 - val_accuracy: 0.8549\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0727 - accuracy: 0.9728 - val_loss: 0.5291 - val_accuracy: 0.8256\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0538 - accuracy: 0.9789 - val_loss: 0.5052 - val_accuracy: 0.8322\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0490 - accuracy: 0.9828 - val_loss: 0.5719 - val_accuracy: 0.8398\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0430 - accuracy: 0.9825 - val_loss: 0.6731 - val_accuracy: 0.8238\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 68s 226ms/step - loss: 0.0397 - accuracy: 0.9817 - val_loss: 0.6342 - val_accuracy: 0.8313\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 68s 226ms/step - loss: 0.0380 - accuracy: 0.9853 - val_loss: 0.7534 - val_accuracy: 0.8190\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0388 - accuracy: 0.9822 - val_loss: 0.6858 - val_accuracy: 0.8256\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0331 - accuracy: 0.9854 - val_loss: 0.6766 - val_accuracy: 0.8341\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 68s 226ms/step - loss: 0.0300 - accuracy: 0.9876 - val_loss: 0.7195 - val_accuracy: 0.8303\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 68s 227ms/step - loss: 0.0323 - accuracy: 0.9841 - val_loss: 0.7526 - val_accuracy: 0.8303\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 70s 198ms/step - loss: 0.4467 - accuracy: 0.7948 - val_loss: 0.3259 - val_accuracy: 0.8369\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.1991 - accuracy: 0.9304 - val_loss: 0.3213 - val_accuracy: 0.8624\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 53s 177ms/step - loss: 0.1329 - accuracy: 0.9550 - val_loss: 0.3649 - val_accuracy: 0.8624\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 53s 176ms/step - loss: 0.1108 - accuracy: 0.9602 - val_loss: 0.3969 - val_accuracy: 0.8567\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0859 - accuracy: 0.9650 - val_loss: 0.4584 - val_accuracy: 0.8483\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0731 - accuracy: 0.9714 - val_loss: 0.4672 - val_accuracy: 0.8662\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0576 - accuracy: 0.9754 - val_loss: 0.4817 - val_accuracy: 0.8633\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0484 - accuracy: 0.9797 - val_loss: 0.5150 - val_accuracy: 0.8633\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0420 - accuracy: 0.9843 - val_loss: 0.5488 - val_accuracy: 0.8586\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0407 - accuracy: 0.9829 - val_loss: 0.5930 - val_accuracy: 0.8549\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0391 - accuracy: 0.9833 - val_loss: 0.6004 - val_accuracy: 0.8511\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0354 - accuracy: 0.9845 - val_loss: 0.5783 - val_accuracy: 0.8596\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0347 - accuracy: 0.9840 - val_loss: 0.6167 - val_accuracy: 0.8520\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0352 - accuracy: 0.9840 - val_loss: 0.6396 - val_accuracy: 0.8483\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.0325 - accuracy: 0.9831 - val_loss: 0.6414 - val_accuracy: 0.8445\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.0277 - accuracy: 0.9876 - val_loss: 0.6381 - val_accuracy: 0.8426\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 86.6163969039917\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 74s 210ms/step - loss: 0.4571 - accuracy: 0.7829 - val_loss: 0.4302 - val_accuracy: 0.8115\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 59s 197ms/step - loss: 0.2093 - accuracy: 0.9246 - val_loss: 0.4586 - val_accuracy: 0.8209\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 58s 194ms/step - loss: 0.1336 - accuracy: 0.9545 - val_loss: 0.5449 - val_accuracy: 0.8134\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 57s 190ms/step - loss: 0.1123 - accuracy: 0.9580 - val_loss: 0.6057 - val_accuracy: 0.8106\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 57s 191ms/step - loss: 0.0868 - accuracy: 0.9651 - val_loss: 0.5857 - val_accuracy: 0.8134\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 56s 189ms/step - loss: 0.0714 - accuracy: 0.9736 - val_loss: 0.6055 - val_accuracy: 0.8096\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 56s 189ms/step - loss: 0.0588 - accuracy: 0.9761 - val_loss: 0.5858 - val_accuracy: 0.8181\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0488 - accuracy: 0.9807 - val_loss: 0.6129 - val_accuracy: 0.8115\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0464 - accuracy: 0.9804 - val_loss: 0.6993 - val_accuracy: 0.8030\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0418 - accuracy: 0.9840 - val_loss: 0.7171 - val_accuracy: 0.8143\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 60s 201ms/step - loss: 0.0366 - accuracy: 0.9844 - val_loss: 0.7627 - val_accuracy: 0.8077\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 61s 204ms/step - loss: 0.0369 - accuracy: 0.9833 - val_loss: 0.7315 - val_accuracy: 0.7992\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 82.09236860275269\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 78s 219ms/step - loss: 0.4459 - accuracy: 0.7985 - val_loss: 0.4368 - val_accuracy: 0.8181\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 61s 204ms/step - loss: 0.1984 - accuracy: 0.9313 - val_loss: 0.5016 - val_accuracy: 0.8087\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 60s 199ms/step - loss: 0.1291 - accuracy: 0.9532 - val_loss: 0.5065 - val_accuracy: 0.8172\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 60s 202ms/step - loss: 0.0973 - accuracy: 0.9673 - val_loss: 0.6180 - val_accuracy: 0.8153\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 61s 203ms/step - loss: 0.0872 - accuracy: 0.9673 - val_loss: 0.7570 - val_accuracy: 0.7992\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 61s 203ms/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.7011 - val_accuracy: 0.8096\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 61s 203ms/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 0.8026 - val_accuracy: 0.8068\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 61s 204ms/step - loss: 0.0514 - accuracy: 0.9811 - val_loss: 0.8466 - val_accuracy: 0.8011\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 60s 202ms/step - loss: 0.0406 - accuracy: 0.9834 - val_loss: 0.8639 - val_accuracy: 0.8058\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 61s 202ms/step - loss: 0.0431 - accuracy: 0.9817 - val_loss: 1.0481 - val_accuracy: 0.7945\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 62s 207ms/step - loss: 0.0343 - accuracy: 0.9856 - val_loss: 0.9518 - val_accuracy: 0.7945\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 81.80961608886719\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 92s 251ms/step - loss: 0.4463 - accuracy: 0.7957 - val_loss: 0.4504 - val_accuracy: 0.8040\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 71s 237ms/step - loss: 0.1943 - accuracy: 0.9309 - val_loss: 0.4599 - val_accuracy: 0.8124\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 71s 238ms/step - loss: 0.1217 - accuracy: 0.9594 - val_loss: 0.5178 - val_accuracy: 0.7964\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 71s 239ms/step - loss: 0.1052 - accuracy: 0.9611 - val_loss: 0.5927 - val_accuracy: 0.8040\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 71s 238ms/step - loss: 0.0805 - accuracy: 0.9727 - val_loss: 0.6078 - val_accuracy: 0.8011\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 70s 232ms/step - loss: 0.0710 - accuracy: 0.9719 - val_loss: 0.6815 - val_accuracy: 0.7917\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 71s 237ms/step - loss: 0.0556 - accuracy: 0.9784 - val_loss: 0.7849 - val_accuracy: 0.7889\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 71s 237ms/step - loss: 0.0475 - accuracy: 0.9807 - val_loss: 0.7483 - val_accuracy: 0.7889\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 71s 236ms/step - loss: 0.0418 - accuracy: 0.9829 - val_loss: 0.8425 - val_accuracy: 0.7879\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 71s 239ms/step - loss: 0.0393 - accuracy: 0.9846 - val_loss: 0.8189 - val_accuracy: 0.7955\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 72s 240ms/step - loss: 0.0364 - accuracy: 0.9842 - val_loss: 0.7644 - val_accuracy: 0.8077\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 72s 239ms/step - loss: 0.0341 - accuracy: 0.9854 - val_loss: 0.8003 - val_accuracy: 0.8209\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 72s 239ms/step - loss: 0.0315 - accuracy: 0.9878 - val_loss: 0.8539 - val_accuracy: 0.8002\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 72s 240ms/step - loss: 0.0286 - accuracy: 0.9884 - val_loss: 0.8496 - val_accuracy: 0.8162\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 72s 240ms/step - loss: 0.0306 - accuracy: 0.9866 - val_loss: 0.8977 - val_accuracy: 0.7889\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 72s 240ms/step - loss: 0.0282 - accuracy: 0.9865 - val_loss: 0.8997 - val_accuracy: 0.7974\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0297 - accuracy: 0.9874 - val_loss: 0.9316 - val_accuracy: 0.7945\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 72s 241ms/step - loss: 0.0294 - accuracy: 0.9848 - val_loss: 0.9420 - val_accuracy: 0.7945\n",
      "Epoch 19/40\n",
      "299/299 [==============================] - 72s 241ms/step - loss: 0.0254 - accuracy: 0.9880 - val_loss: 0.8877 - val_accuracy: 0.7964\n",
      "Epoch 20/40\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0235 - accuracy: 0.9892 - val_loss: 0.9545 - val_accuracy: 0.7917\n",
      "Epoch 21/40\n",
      "299/299 [==============================] - 72s 241ms/step - loss: 0.0259 - accuracy: 0.9875 - val_loss: 0.9789 - val_accuracy: 0.7842\n",
      "Epoch 22/40\n",
      "299/299 [==============================] - 69s 232ms/step - loss: 0.0298 - accuracy: 0.9851 - val_loss: 0.9400 - val_accuracy: 0.7917\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 82.09236860275269\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 97s 280ms/step - loss: 0.4428 - accuracy: 0.7889 - val_loss: 0.6147 - val_accuracy: 0.7597\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 85s 285ms/step - loss: 0.1859 - accuracy: 0.9338 - val_loss: 0.6227 - val_accuracy: 0.7719\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 85s 283ms/step - loss: 0.1267 - accuracy: 0.9551 - val_loss: 0.7657 - val_accuracy: 0.7653\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 85s 284ms/step - loss: 0.0957 - accuracy: 0.9661 - val_loss: 0.8669 - val_accuracy: 0.7521\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 84s 283ms/step - loss: 0.0800 - accuracy: 0.9725 - val_loss: 0.9949 - val_accuracy: 0.7606\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 84s 282ms/step - loss: 0.0669 - accuracy: 0.9749 - val_loss: 0.9503 - val_accuracy: 0.7653\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 85s 283ms/step - loss: 0.0547 - accuracy: 0.9777 - val_loss: 1.1060 - val_accuracy: 0.7512\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 84s 280ms/step - loss: 0.0510 - accuracy: 0.9795 - val_loss: 0.9807 - val_accuracy: 0.7644\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 83s 278ms/step - loss: 0.0416 - accuracy: 0.9835 - val_loss: 0.9925 - val_accuracy: 0.7681\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 83s 278ms/step - loss: 0.0439 - accuracy: 0.9799 - val_loss: 1.1431 - val_accuracy: 0.7559\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 82s 274ms/step - loss: 0.0367 - accuracy: 0.9835 - val_loss: 1.0601 - val_accuracy: 0.7568\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 83s 277ms/step - loss: 0.0377 - accuracy: 0.9823 - val_loss: 1.1878 - val_accuracy: 0.7559\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.19132900238037\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 105s 308ms/step - loss: 0.4500 - accuracy: 0.7864 - val_loss: 0.2945 - val_accuracy: 0.8613\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 85s 285ms/step - loss: 0.1972 - accuracy: 0.9319 - val_loss: 0.3025 - val_accuracy: 0.8453\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 84s 280ms/step - loss: 0.1375 - accuracy: 0.9526 - val_loss: 0.3753 - val_accuracy: 0.8283\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 84s 280ms/step - loss: 0.1044 - accuracy: 0.9635 - val_loss: 0.3750 - val_accuracy: 0.8358\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 83s 277ms/step - loss: 0.0811 - accuracy: 0.9697 - val_loss: 0.3797 - val_accuracy: 0.8340\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 83s 277ms/step - loss: 0.0718 - accuracy: 0.9721 - val_loss: 0.4793 - val_accuracy: 0.8377\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 83s 277ms/step - loss: 0.0596 - accuracy: 0.9765 - val_loss: 0.5022 - val_accuracy: 0.8245\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 83s 276ms/step - loss: 0.0499 - accuracy: 0.9792 - val_loss: 0.5230 - val_accuracy: 0.8236\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 83s 276ms/step - loss: 0.0408 - accuracy: 0.9835 - val_loss: 0.5284 - val_accuracy: 0.8330\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 83s 278ms/step - loss: 0.0394 - accuracy: 0.9822 - val_loss: 0.5707 - val_accuracy: 0.8321\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 83s 278ms/step - loss: 0.0350 - accuracy: 0.9863 - val_loss: 0.5589 - val_accuracy: 0.8283\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 86.13207340240479\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 93s 269ms/step - loss: 0.4568 - accuracy: 0.7812 - val_loss: 0.6250 - val_accuracy: 0.7887\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 0.2071 - accuracy: 0.9270 - val_loss: 0.6948 - val_accuracy: 0.7981\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.1355 - accuracy: 0.9551 - val_loss: 0.7985 - val_accuracy: 0.8094\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 73s 245ms/step - loss: 0.1048 - accuracy: 0.9630 - val_loss: 0.8616 - val_accuracy: 0.7943\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0814 - accuracy: 0.9696 - val_loss: 0.8153 - val_accuracy: 0.7991\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 0.0689 - accuracy: 0.9755 - val_loss: 0.9815 - val_accuracy: 0.8028\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 74s 246ms/step - loss: 0.0555 - accuracy: 0.9793 - val_loss: 0.9193 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 0.0490 - accuracy: 0.9813 - val_loss: 1.0755 - val_accuracy: 0.7896\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 0.0438 - accuracy: 0.9821 - val_loss: 1.1356 - val_accuracy: 0.7981\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 73s 246ms/step - loss: 0.0380 - accuracy: 0.9843 - val_loss: 1.1545 - val_accuracy: 0.7953\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 73s 245ms/step - loss: 0.0394 - accuracy: 0.9856 - val_loss: 1.1035 - val_accuracy: 0.8075\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 73s 245ms/step - loss: 0.0374 - accuracy: 0.9823 - val_loss: 1.0357 - val_accuracy: 0.8085\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0363 - accuracy: 0.9831 - val_loss: 1.0863 - val_accuracy: 0.8009\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 80.94339370727539\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 94s 259ms/step - loss: 0.4426 - accuracy: 0.7957 - val_loss: 0.3252 - val_accuracy: 0.8726\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 73s 243ms/step - loss: 0.1948 - accuracy: 0.9297 - val_loss: 0.3957 - val_accuracy: 0.8642\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.1330 - accuracy: 0.9573 - val_loss: 0.4450 - val_accuracy: 0.8670\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 72s 241ms/step - loss: 0.1038 - accuracy: 0.9648 - val_loss: 0.4699 - val_accuracy: 0.8604\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 73s 245ms/step - loss: 0.0796 - accuracy: 0.9696 - val_loss: 0.5186 - val_accuracy: 0.8623\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 0.0632 - accuracy: 0.9769 - val_loss: 0.5212 - val_accuracy: 0.8623\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 0.0589 - accuracy: 0.9773 - val_loss: 0.5799 - val_accuracy: 0.8604\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 0.0465 - accuracy: 0.9802 - val_loss: 0.6009 - val_accuracy: 0.8623\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 0.0405 - accuracy: 0.9827 - val_loss: 0.6099 - val_accuracy: 0.8623\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 71s 238ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 0.6350 - val_accuracy: 0.8557\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 71s 238ms/step - loss: 0.0349 - accuracy: 0.9843 - val_loss: 0.6805 - val_accuracy: 0.8566\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 87.26415038108826\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 102s 300ms/step - loss: 0.4512 - accuracy: 0.7891 - val_loss: 0.3597 - val_accuracy: 0.8349\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 73s 246ms/step - loss: 0.1989 - accuracy: 0.9271 - val_loss: 0.3664 - val_accuracy: 0.8330\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 73s 243ms/step - loss: 0.1251 - accuracy: 0.9574 - val_loss: 0.4076 - val_accuracy: 0.8274\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 71s 239ms/step - loss: 0.0995 - accuracy: 0.9632 - val_loss: 0.4523 - val_accuracy: 0.8217\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 71s 237ms/step - loss: 0.0921 - accuracy: 0.9643 - val_loss: 0.5036 - val_accuracy: 0.8170\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 71s 236ms/step - loss: 0.0682 - accuracy: 0.9735 - val_loss: 0.5931 - val_accuracy: 0.8189\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 71s 236ms/step - loss: 0.0580 - accuracy: 0.9767 - val_loss: 0.6145 - val_accuracy: 0.8113\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 71s 237ms/step - loss: 0.0547 - accuracy: 0.9770 - val_loss: 0.6276 - val_accuracy: 0.8170\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 72s 241ms/step - loss: 0.0409 - accuracy: 0.9814 - val_loss: 0.6857 - val_accuracy: 0.8123\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 72s 241ms/step - loss: 0.0377 - accuracy: 0.9842 - val_loss: 0.7508 - val_accuracy: 0.8075\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0370 - accuracy: 0.9830 - val_loss: 0.7594 - val_accuracy: 0.8066\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 83.49056839942932\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  85.485393  84.825635  85.108387  85.485393  86.427897  85.862392   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  88.396227  87.452829  85.283017  87.169814  86.149698  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.485393</td>\n",
       "      <td>86.616397</td>\n",
       "      <td>82.092369</td>\n",
       "      <td>81.809616</td>\n",
       "      <td>82.092369</td>\n",
       "      <td>77.191329</td>\n",
       "      <td>86.132073</td>\n",
       "      <td>80.943394</td>\n",
       "      <td>87.26415</td>\n",
       "      <td>83.490568</td>\n",
       "      <td>83.311766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  85.485393  86.616397  82.092369  81.809616  82.092369  77.191329   \n",
       "\n",
       "        acc7       acc8      acc9      acc10        AVG  \n",
       "0  86.132073  80.943394  87.26415  83.490568  83.311766  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_MPQA_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
