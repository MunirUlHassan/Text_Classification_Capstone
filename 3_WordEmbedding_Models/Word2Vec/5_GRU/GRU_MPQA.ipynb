{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Classification with MPQA Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using GRU model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10606, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaining</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failing to support</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desperately needs</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many years of decay</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no quick fix</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>urged</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>hope</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10606 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sentence  label  split\n",
       "0              complaining      0  train\n",
       "1       failing to support      0  train\n",
       "2        desperately needs      0  train\n",
       "3      many years of decay      0  train\n",
       "4             no quick fix      0  train\n",
       "...                    ...    ...    ...\n",
       "10601                urged      1  train\n",
       "10602       strictly abide      1  train\n",
       "10603                 hope      1  train\n",
       "10604       strictly abide      1  train\n",
       "10605                           1  train\n",
       "\n",
       "[10606 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MPQA/MPQA.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10606 non-null  object\n",
      " 1   label     10606 non-null  int32 \n",
      " 2   split     10606 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 207.3+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3312</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          7294   7294\n",
       "1          3312   3312"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complaining'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  no quick fix\n",
      "Into a sequence of int: [25, 945, 1476]\n",
      "Into a padded sequence: [  25  945 1476    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "of 3\n",
      "to 4\n",
      "a 5\n",
      "and 6\n",
      "not 7\n",
      "is 8\n",
      "in 9\n",
      "be 10\n",
      "6236\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 440,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 81s 224ms/step - loss: 0.5523 - accuracy: 0.7316 - val_loss: 0.3673 - val_accuracy: 0.8549\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 62s 208ms/step - loss: 0.2079 - accuracy: 0.9272 - val_loss: 0.3677 - val_accuracy: 0.8511\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 64s 214ms/step - loss: 0.1302 - accuracy: 0.9556 - val_loss: 0.4196 - val_accuracy: 0.8426\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 65s 219ms/step - loss: 0.0987 - accuracy: 0.9646 - val_loss: 0.4448 - val_accuracy: 0.8492\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 65s 219ms/step - loss: 0.0796 - accuracy: 0.9699 - val_loss: 0.5306 - val_accuracy: 0.8426\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 65s 219ms/step - loss: 0.0608 - accuracy: 0.9746 - val_loss: 0.5837 - val_accuracy: 0.8322\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 65s 217ms/step - loss: 0.0496 - accuracy: 0.9798 - val_loss: 0.5672 - val_accuracy: 0.8369\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 67s 223ms/step - loss: 0.0418 - accuracy: 0.9835 - val_loss: 0.6415 - val_accuracy: 0.8341\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 67s 224ms/step - loss: 0.0375 - accuracy: 0.9849 - val_loss: 0.6688 - val_accuracy: 0.8351\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 64s 215ms/step - loss: 0.0365 - accuracy: 0.9846 - val_loss: 0.6798 - val_accuracy: 0.8256\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 63s 210ms/step - loss: 0.0333 - accuracy: 0.9856 - val_loss: 0.7140 - val_accuracy: 0.8247\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 2: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 72s 202ms/step - loss: 0.5496 - accuracy: 0.7363 - val_loss: 0.3748 - val_accuracy: 0.8483\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 53s 177ms/step - loss: 0.2041 - accuracy: 0.9293 - val_loss: 0.3941 - val_accuracy: 0.8228\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 53s 177ms/step - loss: 0.1214 - accuracy: 0.9586 - val_loss: 0.4493 - val_accuracy: 0.8190\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0943 - accuracy: 0.9664 - val_loss: 0.5022 - val_accuracy: 0.8238\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0735 - accuracy: 0.9709 - val_loss: 0.5314 - val_accuracy: 0.8398\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0590 - accuracy: 0.9759 - val_loss: 0.5850 - val_accuracy: 0.8351\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0467 - accuracy: 0.9813 - val_loss: 0.6078 - val_accuracy: 0.8388\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0407 - accuracy: 0.9821 - val_loss: 0.7473 - val_accuracy: 0.8153\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0389 - accuracy: 0.9848 - val_loss: 0.6736 - val_accuracy: 0.8303\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0337 - accuracy: 0.9845 - val_loss: 0.7361 - val_accuracy: 0.8303\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0315 - accuracy: 0.9852 - val_loss: 0.7564 - val_accuracy: 0.8238\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 84.82563495635986\n",
      "Training 3: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 49s 130ms/step - loss: 0.5532 - accuracy: 0.7337 - val_loss: 0.3735 - val_accuracy: 0.8435\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 34s 113ms/step - loss: 0.2109 - accuracy: 0.9315 - val_loss: 0.3947 - val_accuracy: 0.8511\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 38s 126ms/step - loss: 0.1284 - accuracy: 0.9541 - val_loss: 0.4736 - val_accuracy: 0.8483\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.0924 - accuracy: 0.9661 - val_loss: 0.5122 - val_accuracy: 0.8445\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.0787 - accuracy: 0.9681 - val_loss: 0.5603 - val_accuracy: 0.8464\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.0629 - accuracy: 0.9736 - val_loss: 0.6193 - val_accuracy: 0.8398\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 39s 129ms/step - loss: 0.0536 - accuracy: 0.9804 - val_loss: 0.6276 - val_accuracy: 0.8511\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 39s 130ms/step - loss: 0.0409 - accuracy: 0.9853 - val_loss: 0.6749 - val_accuracy: 0.8435\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 39s 130ms/step - loss: 0.0379 - accuracy: 0.9838 - val_loss: 0.6877 - val_accuracy: 0.8379\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 39s 130ms/step - loss: 0.0316 - accuracy: 0.9873 - val_loss: 0.6939 - val_accuracy: 0.8407\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 40s 134ms/step - loss: 0.0336 - accuracy: 0.9843 - val_loss: 0.7612 - val_accuracy: 0.8360\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 40s 134ms/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 0.7751 - val_accuracy: 0.8332\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 85.10838747024536\n",
      "Training 4: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 49s 128ms/step - loss: 0.5477 - accuracy: 0.7328 - val_loss: 0.3630 - val_accuracy: 0.8549\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 35s 117ms/step - loss: 0.2056 - accuracy: 0.9314 - val_loss: 0.3935 - val_accuracy: 0.8530\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.1288 - accuracy: 0.9563 - val_loss: 0.4402 - val_accuracy: 0.8417\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0983 - accuracy: 0.9617 - val_loss: 0.4721 - val_accuracy: 0.8454\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0781 - accuracy: 0.9706 - val_loss: 0.5160 - val_accuracy: 0.8445\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 35s 119ms/step - loss: 0.0726 - accuracy: 0.9726 - val_loss: 0.5660 - val_accuracy: 0.8454\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0565 - accuracy: 0.9775 - val_loss: 0.6008 - val_accuracy: 0.8445\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0465 - accuracy: 0.9808 - val_loss: 0.6408 - val_accuracy: 0.8454\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0437 - accuracy: 0.9822 - val_loss: 0.6787 - val_accuracy: 0.8435\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 0.7094 - val_accuracy: 0.8360\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 35s 116ms/step - loss: 0.0366 - accuracy: 0.9848 - val_loss: 0.7220 - val_accuracy: 0.8303\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 5: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 87s 243ms/step - loss: 0.5526 - accuracy: 0.7258 - val_loss: 0.3431 - val_accuracy: 0.8558\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 69s 230ms/step - loss: 0.2064 - accuracy: 0.9290 - val_loss: 0.3616 - val_accuracy: 0.8643\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.1371 - accuracy: 0.9506 - val_loss: 0.3931 - val_accuracy: 0.8483\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0921 - accuracy: 0.9669 - val_loss: 0.4298 - val_accuracy: 0.8511\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0755 - accuracy: 0.9738 - val_loss: 0.4834 - val_accuracy: 0.8511\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0617 - accuracy: 0.9742 - val_loss: 0.5005 - val_accuracy: 0.8549\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0496 - accuracy: 0.9809 - val_loss: 0.5683 - val_accuracy: 0.8369\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0458 - accuracy: 0.9822 - val_loss: 0.6116 - val_accuracy: 0.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "299/299 [==============================] - 69s 232ms/step - loss: 0.0430 - accuracy: 0.9814 - val_loss: 0.6089 - val_accuracy: 0.8464\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0385 - accuracy: 0.9832 - val_loss: 0.6631 - val_accuracy: 0.8369\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 69s 232ms/step - loss: 0.0336 - accuracy: 0.9863 - val_loss: 0.6521 - val_accuracy: 0.8388\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.0373 - accuracy: 0.9837 - val_loss: 0.6265 - val_accuracy: 0.8501\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 86.42789721488953\n",
      "Training 6: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 88s 255ms/step - loss: 0.5478 - accuracy: 0.7366 - val_loss: 0.3749 - val_accuracy: 0.8539\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 75s 251ms/step - loss: 0.2024 - accuracy: 0.9304 - val_loss: 0.3710 - val_accuracy: 0.8586\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.1351 - accuracy: 0.9531 - val_loss: 0.4320 - val_accuracy: 0.8464\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 73s 246ms/step - loss: 0.0964 - accuracy: 0.9656 - val_loss: 0.4282 - val_accuracy: 0.8360\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0765 - accuracy: 0.9716 - val_loss: 0.4769 - val_accuracy: 0.8520\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0622 - accuracy: 0.9764 - val_loss: 0.5322 - val_accuracy: 0.8530\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 73s 243ms/step - loss: 0.0529 - accuracy: 0.9778 - val_loss: 0.5640 - val_accuracy: 0.8426\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0424 - accuracy: 0.9837 - val_loss: 0.5934 - val_accuracy: 0.8407\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0404 - accuracy: 0.9830 - val_loss: 0.5829 - val_accuracy: 0.8426\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0323 - accuracy: 0.9879 - val_loss: 0.6287 - val_accuracy: 0.8417\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0328 - accuracy: 0.9864 - val_loss: 0.6450 - val_accuracy: 0.8369\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.0316 - accuracy: 0.9851 - val_loss: 0.6586 - val_accuracy: 0.8351\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 85.86239218711853\n",
      "Training 7: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 84s 242ms/step - loss: 0.5507 - accuracy: 0.7386 - val_loss: 0.3036 - val_accuracy: 0.8755\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 59s 197ms/step - loss: 0.2124 - accuracy: 0.9262 - val_loss: 0.3063 - val_accuracy: 0.8840\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 58s 194ms/step - loss: 0.1341 - accuracy: 0.9538 - val_loss: 0.3496 - val_accuracy: 0.8774\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 57s 192ms/step - loss: 0.1036 - accuracy: 0.9618 - val_loss: 0.3500 - val_accuracy: 0.8764\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 57s 191ms/step - loss: 0.0798 - accuracy: 0.9697 - val_loss: 0.4107 - val_accuracy: 0.8755\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 56s 189ms/step - loss: 0.0580 - accuracy: 0.9775 - val_loss: 0.4131 - val_accuracy: 0.8698\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0514 - accuracy: 0.9780 - val_loss: 0.4912 - val_accuracy: 0.8708\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0464 - accuracy: 0.9798 - val_loss: 0.5044 - val_accuracy: 0.8632\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0370 - accuracy: 0.9852 - val_loss: 0.5452 - val_accuracy: 0.8613\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0352 - accuracy: 0.9849 - val_loss: 0.5574 - val_accuracy: 0.8491\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 56s 187ms/step - loss: 0.0355 - accuracy: 0.9852 - val_loss: 0.5965 - val_accuracy: 0.8623\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0387 - accuracy: 0.9834 - val_loss: 0.5835 - val_accuracy: 0.8632\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 88.39622735977173\n",
      "Training 8: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 65s 179ms/step - loss: 0.5472 - accuracy: 0.7345 - val_loss: 0.3331 - val_accuracy: 0.8745\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.2011 - accuracy: 0.9313 - val_loss: 0.3478 - val_accuracy: 0.8679\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.1254 - accuracy: 0.9530 - val_loss: 0.3917 - val_accuracy: 0.8566\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0978 - accuracy: 0.9631 - val_loss: 0.4550 - val_accuracy: 0.8585\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0773 - accuracy: 0.9714 - val_loss: 0.4608 - val_accuracy: 0.8632\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0616 - accuracy: 0.9767 - val_loss: 0.5280 - val_accuracy: 0.8557\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0498 - accuracy: 0.9787 - val_loss: 0.5505 - val_accuracy: 0.8547\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0476 - accuracy: 0.9799 - val_loss: 0.5617 - val_accuracy: 0.8528\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0421 - accuracy: 0.9838 - val_loss: 0.6227 - val_accuracy: 0.8528\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0362 - accuracy: 0.9815 - val_loss: 0.6173 - val_accuracy: 0.8585\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0353 - accuracy: 0.9830 - val_loss: 0.7099 - val_accuracy: 0.8462\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 87.45282888412476\n",
      "Training 9: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 72s 191ms/step - loss: 0.5465 - accuracy: 0.7330 - val_loss: 0.3913 - val_accuracy: 0.8406\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.2038 - accuracy: 0.9263 - val_loss: 0.3978 - val_accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.1238 - accuracy: 0.9568 - val_loss: 0.4655 - val_accuracy: 0.8481\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.0975 - accuracy: 0.9668 - val_loss: 0.4842 - val_accuracy: 0.8528\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.0766 - accuracy: 0.9725 - val_loss: 0.5355 - val_accuracy: 0.8415\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 53s 178ms/step - loss: 0.0597 - accuracy: 0.9787 - val_loss: 0.5772 - val_accuracy: 0.8349\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 53s 179ms/step - loss: 0.0533 - accuracy: 0.9785 - val_loss: 0.6283 - val_accuracy: 0.8387\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 53s 178ms/step - loss: 0.0482 - accuracy: 0.9802 - val_loss: 0.6423 - val_accuracy: 0.8528\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0445 - accuracy: 0.9809 - val_loss: 0.6751 - val_accuracy: 0.8462\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0385 - accuracy: 0.9833 - val_loss: 0.6957 - val_accuracy: 0.8425\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0388 - accuracy: 0.9820 - val_loss: 0.6772 - val_accuracy: 0.8425\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 54s 180ms/step - loss: 0.0340 - accuracy: 0.9854 - val_loss: 0.7144 - val_accuracy: 0.8434\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 54s 179ms/step - loss: 0.0297 - accuracy: 0.9857 - val_loss: 0.7183 - val_accuracy: 0.8330\n",
      "Epoch 14/30\n",
      "299/299 [==============================] - 54s 180ms/step - loss: 0.0318 - accuracy: 0.9841 - val_loss: 0.7124 - val_accuracy: 0.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 85.2830171585083\n",
      "Training 10: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 72s 201ms/step - loss: 0.5508 - accuracy: 0.7293 - val_loss: 0.3428 - val_accuracy: 0.8670\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 50s 167ms/step - loss: 0.2130 - accuracy: 0.9264 - val_loss: 0.3522 - val_accuracy: 0.8717\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.1216 - accuracy: 0.9588 - val_loss: 0.3963 - val_accuracy: 0.8642\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.0999 - accuracy: 0.9628 - val_loss: 0.4265 - val_accuracy: 0.8613\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0748 - accuracy: 0.9732 - val_loss: 0.4726 - val_accuracy: 0.8509\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.0565 - accuracy: 0.9776 - val_loss: 0.4931 - val_accuracy: 0.8689\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0415 - accuracy: 0.9836 - val_loss: 0.5557 - val_accuracy: 0.8642\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0378 - accuracy: 0.9840 - val_loss: 0.5489 - val_accuracy: 0.8632\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 48s 161ms/step - loss: 0.0345 - accuracy: 0.9850 - val_loss: 0.6671 - val_accuracy: 0.8528\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 48s 160ms/step - loss: 0.0325 - accuracy: 0.9861 - val_loss: 0.5860 - val_accuracy: 0.8585\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0330 - accuracy: 0.9874 - val_loss: 0.7261 - val_accuracy: 0.8415\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.0321 - accuracy: 0.9869 - val_loss: 0.6860 - val_accuracy: 0.8425\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 87.16981410980225\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  85.485393  84.825635  85.108387  85.485393  86.427897  85.862392   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  88.396227  87.452829  85.283017  87.169814  86.149698  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.485393</td>\n",
       "      <td>84.825635</td>\n",
       "      <td>85.108387</td>\n",
       "      <td>85.485393</td>\n",
       "      <td>86.427897</td>\n",
       "      <td>85.862392</td>\n",
       "      <td>88.396227</td>\n",
       "      <td>87.452829</td>\n",
       "      <td>85.283017</td>\n",
       "      <td>87.169814</td>\n",
       "      <td>86.149698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  85.485393  84.825635  85.108387  85.485393  86.427897  85.862392   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  88.396227  87.452829  85.283017  87.169814  86.149698  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_MPQA.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6083 words present from 6236 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 140,673\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 23s 47ms/step - loss: 0.4328 - accuracy: 0.7985 - val_loss: 0.3153 - val_accuracy: 0.8746\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.2882 - accuracy: 0.8885 - val_loss: 0.3012 - val_accuracy: 0.8765\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.2582 - accuracy: 0.9020 - val_loss: 0.2971 - val_accuracy: 0.8822\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2462 - accuracy: 0.9074 - val_loss: 0.2996 - val_accuracy: 0.8831\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.2186 - accuracy: 0.9133 - val_loss: 0.2992 - val_accuracy: 0.8812\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.2076 - accuracy: 0.9220 - val_loss: 0.3079 - val_accuracy: 0.8803\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1981 - accuracy: 0.9252 - val_loss: 0.3322 - val_accuracy: 0.8756\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1810 - accuracy: 0.9306 - val_loss: 0.3215 - val_accuracy: 0.8765\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1662 - accuracy: 0.9356 - val_loss: 0.3533 - val_accuracy: 0.8737\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1527 - accuracy: 0.9419 - val_loss: 0.3648 - val_accuracy: 0.8746\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1305 - accuracy: 0.9524 - val_loss: 0.3674 - val_accuracy: 0.8775\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1293 - accuracy: 0.9526 - val_loss: 0.3651 - val_accuracy: 0.8728\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 88.31291198730469\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 25s 46ms/step - loss: 0.4403 - accuracy: 0.7995 - val_loss: 0.3075 - val_accuracy: 0.8775\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.2903 - accuracy: 0.8910 - val_loss: 0.3045 - val_accuracy: 0.8822\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.2620 - accuracy: 0.8983 - val_loss: 0.3033 - val_accuracy: 0.8822\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.2494 - accuracy: 0.9090 - val_loss: 0.2995 - val_accuracy: 0.8850\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.2271 - accuracy: 0.9141 - val_loss: 0.2992 - val_accuracy: 0.8888\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.2121 - accuracy: 0.9146 - val_loss: 0.3253 - val_accuracy: 0.8831\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.1828 - accuracy: 0.9282 - val_loss: 0.3125 - val_accuracy: 0.8831\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1802 - accuracy: 0.9354 - val_loss: 0.3273 - val_accuracy: 0.8822\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1565 - accuracy: 0.9389 - val_loss: 0.3416 - val_accuracy: 0.8709\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1490 - accuracy: 0.9457 - val_loss: 0.3596 - val_accuracy: 0.8765\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.1239 - accuracy: 0.9536 - val_loss: 0.3764 - val_accuracy: 0.8737\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1212 - accuracy: 0.9564 - val_loss: 0.4086 - val_accuracy: 0.8784\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.1068 - accuracy: 0.9595 - val_loss: 0.4171 - val_accuracy: 0.8699\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 88.87841701507568\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 33s 72ms/step - loss: 0.4410 - accuracy: 0.7867 - val_loss: 0.2916 - val_accuracy: 0.8841\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.3004 - accuracy: 0.8881 - val_loss: 0.2808 - val_accuracy: 0.8878\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 18s 61ms/step - loss: 0.2599 - accuracy: 0.9052 - val_loss: 0.2750 - val_accuracy: 0.8935\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 18s 62ms/step - loss: 0.2477 - accuracy: 0.9045 - val_loss: 0.2691 - val_accuracy: 0.8944\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.2280 - accuracy: 0.9116 - val_loss: 0.2653 - val_accuracy: 0.8944\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.2012 - accuracy: 0.9262 - val_loss: 0.2725 - val_accuracy: 0.8982\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1922 - accuracy: 0.9256 - val_loss: 0.3011 - val_accuracy: 0.8878\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 19s 65ms/step - loss: 0.1706 - accuracy: 0.9360 - val_loss: 0.2837 - val_accuracy: 0.8944\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1463 - accuracy: 0.9452 - val_loss: 0.2979 - val_accuracy: 0.8926\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.1427 - accuracy: 0.9459 - val_loss: 0.3337 - val_accuracy: 0.8737\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.1190 - accuracy: 0.9568 - val_loss: 0.3574 - val_accuracy: 0.8850\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.1158 - accuracy: 0.9555 - val_loss: 0.3548 - val_accuracy: 0.8850\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.1046 - accuracy: 0.9619 - val_loss: 0.3639 - val_accuracy: 0.8869\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.0981 - accuracy: 0.9653 - val_loss: 0.3746 - val_accuracy: 0.8841\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 89.82092142105103\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 30s 70ms/step - loss: 0.4333 - accuracy: 0.7996 - val_loss: 0.3168 - val_accuracy: 0.8624\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 13s 42ms/step - loss: 0.2813 - accuracy: 0.8901 - val_loss: 0.3049 - val_accuracy: 0.8746\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 12s 41ms/step - loss: 0.2543 - accuracy: 0.9047 - val_loss: 0.3033 - val_accuracy: 0.8662\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 12s 41ms/step - loss: 0.2405 - accuracy: 0.9107 - val_loss: 0.3028 - val_accuracy: 0.8690\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 12s 41ms/step - loss: 0.2296 - accuracy: 0.9132 - val_loss: 0.3039 - val_accuracy: 0.8709\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 12s 41ms/step - loss: 0.2147 - accuracy: 0.9147 - val_loss: 0.3208 - val_accuracy: 0.8690\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 12s 40ms/step - loss: 0.1892 - accuracy: 0.9289 - val_loss: 0.3224 - val_accuracy: 0.8737\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 12s 40ms/step - loss: 0.1729 - accuracy: 0.9362 - val_loss: 0.3454 - val_accuracy: 0.8671\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 12s 41ms/step - loss: 0.1558 - accuracy: 0.9414 - val_loss: 0.3465 - val_accuracy: 0.8643\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 12s 40ms/step - loss: 0.1433 - accuracy: 0.9453 - val_loss: 0.3642 - val_accuracy: 0.8709\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 87.4646544456482\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 26s 54ms/step - loss: 0.4369 - accuracy: 0.7933 - val_loss: 0.3490 - val_accuracy: 0.8577\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2865 - accuracy: 0.8894 - val_loss: 0.3334 - val_accuracy: 0.8596\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.2630 - accuracy: 0.9031 - val_loss: 0.3206 - val_accuracy: 0.8624\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2403 - accuracy: 0.9102 - val_loss: 0.3186 - val_accuracy: 0.8652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.2225 - accuracy: 0.9132 - val_loss: 0.3189 - val_accuracy: 0.8662\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2077 - accuracy: 0.9236 - val_loss: 0.3342 - val_accuracy: 0.8756\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1920 - accuracy: 0.9259 - val_loss: 0.3448 - val_accuracy: 0.8709\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1695 - accuracy: 0.9377 - val_loss: 0.3561 - val_accuracy: 0.8690\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1488 - accuracy: 0.9441 - val_loss: 0.3570 - val_accuracy: 0.8765\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1372 - accuracy: 0.9507 - val_loss: 0.4072 - val_accuracy: 0.8680\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1238 - accuracy: 0.9557 - val_loss: 0.4097 - val_accuracy: 0.8633\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 12s 39ms/step - loss: 0.1145 - accuracy: 0.9584 - val_loss: 0.4077 - val_accuracy: 0.8680\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1064 - accuracy: 0.9626 - val_loss: 0.4245 - val_accuracy: 0.8643\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0937 - accuracy: 0.9653 - val_loss: 0.4407 - val_accuracy: 0.8699\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0823 - accuracy: 0.9703 - val_loss: 0.4745 - val_accuracy: 0.8699\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0826 - accuracy: 0.9685 - val_loss: 0.4862 - val_accuracy: 0.8690\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0836 - accuracy: 0.9698 - val_loss: 0.5153 - val_accuracy: 0.8652\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 87.65316009521484\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 24s 50ms/step - loss: 0.4434 - accuracy: 0.7845 - val_loss: 0.3165 - val_accuracy: 0.8690\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2890 - accuracy: 0.8864 - val_loss: 0.2924 - val_accuracy: 0.8765\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2601 - accuracy: 0.9029 - val_loss: 0.2790 - val_accuracy: 0.8907\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2479 - accuracy: 0.9055 - val_loss: 0.2769 - val_accuracy: 0.8869\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2272 - accuracy: 0.9149 - val_loss: 0.2832 - val_accuracy: 0.8841\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2135 - accuracy: 0.9221 - val_loss: 0.2881 - val_accuracy: 0.8831\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1912 - accuracy: 0.9294 - val_loss: 0.2881 - val_accuracy: 0.8860\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1744 - accuracy: 0.9352 - val_loss: 0.3051 - val_accuracy: 0.8812\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1538 - accuracy: 0.9459 - val_loss: 0.3169 - val_accuracy: 0.8794\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1451 - accuracy: 0.9480 - val_loss: 0.3352 - val_accuracy: 0.8869\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1403 - accuracy: 0.9495 - val_loss: 0.3336 - val_accuracy: 0.8812\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 89.06691670417786\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 42s 98ms/step - loss: 0.4391 - accuracy: 0.7880 - val_loss: 0.3145 - val_accuracy: 0.8849\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.2879 - accuracy: 0.8868 - val_loss: 0.3014 - val_accuracy: 0.8915\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.2696 - accuracy: 0.8924 - val_loss: 0.2919 - val_accuracy: 0.8934\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.2413 - accuracy: 0.9067 - val_loss: 0.2926 - val_accuracy: 0.8962\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.2362 - accuracy: 0.9051 - val_loss: 0.2970 - val_accuracy: 0.8830\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.2068 - accuracy: 0.9184 - val_loss: 0.2940 - val_accuracy: 0.8915\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.1982 - accuracy: 0.9232 - val_loss: 0.2969 - val_accuracy: 0.8981\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 18s 60ms/step - loss: 0.1793 - accuracy: 0.9294 - val_loss: 0.3245 - val_accuracy: 0.8840\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 18s 60ms/step - loss: 0.1549 - accuracy: 0.9418 - val_loss: 0.3350 - val_accuracy: 0.8840\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.1433 - accuracy: 0.9494 - val_loss: 0.3488 - val_accuracy: 0.8821\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1321 - accuracy: 0.9506 - val_loss: 0.3645 - val_accuracy: 0.8840\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 17s 59ms/step - loss: 0.1134 - accuracy: 0.9559 - val_loss: 0.3678 - val_accuracy: 0.8849\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 18s 60ms/step - loss: 0.1080 - accuracy: 0.9585 - val_loss: 0.3802 - val_accuracy: 0.8830\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 18s 61ms/step - loss: 0.1017 - accuracy: 0.9626 - val_loss: 0.4355 - val_accuracy: 0.8736\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 18s 61ms/step - loss: 0.1093 - accuracy: 0.9599 - val_loss: 0.4176 - val_accuracy: 0.8830\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 89.81131911277771\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 38s 96ms/step - loss: 0.4419 - accuracy: 0.7804 - val_loss: 0.3361 - val_accuracy: 0.8708\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.2817 - accuracy: 0.8938 - val_loss: 0.3187 - val_accuracy: 0.8726\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 26s 88ms/step - loss: 0.2554 - accuracy: 0.9010 - val_loss: 0.3163 - val_accuracy: 0.8736\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 25s 85ms/step - loss: 0.2342 - accuracy: 0.9127 - val_loss: 0.3143 - val_accuracy: 0.8736\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.2213 - accuracy: 0.9125 - val_loss: 0.3207 - val_accuracy: 0.8774\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 25s 84ms/step - loss: 0.2087 - accuracy: 0.9230 - val_loss: 0.3259 - val_accuracy: 0.8689\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.1868 - accuracy: 0.9298 - val_loss: 0.3401 - val_accuracy: 0.8660\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 25s 82ms/step - loss: 0.1660 - accuracy: 0.9370 - val_loss: 0.3727 - val_accuracy: 0.8651\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.1490 - accuracy: 0.9456 - val_loss: 0.3690 - val_accuracy: 0.8623\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1433 - accuracy: 0.9474 - val_loss: 0.4071 - val_accuracy: 0.8557\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.1301 - accuracy: 0.9532 - val_loss: 0.4029 - val_accuracy: 0.8472\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 23s 78ms/step - loss: 0.1207 - accuracy: 0.9524 - val_loss: 0.4356 - val_accuracy: 0.8462\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 23s 77ms/step - loss: 0.1172 - accuracy: 0.9589 - val_loss: 0.4588 - val_accuracy: 0.8557\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 87.73584961891174\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 27s 59ms/step - loss: 0.4362 - accuracy: 0.7963 - val_loss: 0.3530 - val_accuracy: 0.8585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.2784 - accuracy: 0.8920 - val_loss: 0.3419 - val_accuracy: 0.8660\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 15s 52ms/step - loss: 0.2591 - accuracy: 0.8973 - val_loss: 0.3493 - val_accuracy: 0.8509\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 0.2399 - accuracy: 0.9055 - val_loss: 0.3394 - val_accuracy: 0.8679\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.2100 - accuracy: 0.9193 - val_loss: 0.3428 - val_accuracy: 0.8670\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.2039 - accuracy: 0.9210 - val_loss: 0.3636 - val_accuracy: 0.8670\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.1876 - accuracy: 0.9261 - val_loss: 0.3665 - val_accuracy: 0.8642\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.1651 - accuracy: 0.9378 - val_loss: 0.3842 - val_accuracy: 0.8585\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.1574 - accuracy: 0.9405 - val_loss: 0.4072 - val_accuracy: 0.8604\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 15s 49ms/step - loss: 0.1431 - accuracy: 0.9458 - val_loss: 0.4318 - val_accuracy: 0.8604\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.1258 - accuracy: 0.9539 - val_loss: 0.4269 - val_accuracy: 0.8594\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.1163 - accuracy: 0.9559 - val_loss: 0.4766 - val_accuracy: 0.8547\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 86.79245114326477\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 26s 52ms/step - loss: 0.4380 - accuracy: 0.7976 - val_loss: 0.4084 - val_accuracy: 0.8462\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 13s 42ms/step - loss: 0.2785 - accuracy: 0.8970 - val_loss: 0.3822 - val_accuracy: 0.8538\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 13s 43ms/step - loss: 0.2571 - accuracy: 0.9051 - val_loss: 0.3751 - val_accuracy: 0.8547\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 13s 45ms/step - loss: 0.2269 - accuracy: 0.9130 - val_loss: 0.3708 - val_accuracy: 0.8519\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.2129 - accuracy: 0.9156 - val_loss: 0.3652 - val_accuracy: 0.8519\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.2059 - accuracy: 0.9246 - val_loss: 0.3736 - val_accuracy: 0.8462\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1853 - accuracy: 0.9265 - val_loss: 0.3882 - val_accuracy: 0.8481\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1729 - accuracy: 0.9315 - val_loss: 0.3864 - val_accuracy: 0.8481\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 13s 43ms/step - loss: 0.1551 - accuracy: 0.9425 - val_loss: 0.3981 - val_accuracy: 0.8519\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1334 - accuracy: 0.9507 - val_loss: 0.4274 - val_accuracy: 0.8472\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 13s 43ms/step - loss: 0.1318 - accuracy: 0.9510 - val_loss: 0.4459 - val_accuracy: 0.8538\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 85.4716956615448\n",
      "\n",
      "        acc1       acc2       acc3       acc4      acc5       acc6       acc7  \\\n",
      "0  88.312912  88.878417  89.820921  87.464654  87.65316  89.066917  89.811319   \n",
      "\n",
      "       acc8       acc9      acc10       AVG  \n",
      "0  87.73585  86.792451  85.471696  88.10083  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.312912</td>\n",
       "      <td>88.878417</td>\n",
       "      <td>89.820921</td>\n",
       "      <td>87.464654</td>\n",
       "      <td>87.65316</td>\n",
       "      <td>89.066917</td>\n",
       "      <td>89.811319</td>\n",
       "      <td>87.73585</td>\n",
       "      <td>86.792451</td>\n",
       "      <td>85.471696</td>\n",
       "      <td>88.10083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4      acc5       acc6       acc7  \\\n",
       "0  88.312912  88.878417  89.820921  87.464654  87.65316  89.066917  89.811319   \n",
       "\n",
       "       acc8       acc9      acc10       AVG  \n",
       "0  87.73585  86.792451  85.471696  88.10083  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_MPQA_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               140544    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 440,673\n",
      "Trainable params: 440,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 64s 171ms/step - loss: 0.4326 - accuracy: 0.7989 - val_loss: 0.2841 - val_accuracy: 0.8841\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 38s 128ms/step - loss: 0.1962 - accuracy: 0.9284 - val_loss: 0.2999 - val_accuracy: 0.8897\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 37s 123ms/step - loss: 0.1315 - accuracy: 0.9539 - val_loss: 0.3356 - val_accuracy: 0.8756\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 36s 120ms/step - loss: 0.0932 - accuracy: 0.9685 - val_loss: 0.3610 - val_accuracy: 0.8709\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 35s 119ms/step - loss: 0.0702 - accuracy: 0.9723 - val_loss: 0.4204 - val_accuracy: 0.8709\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 35s 119ms/step - loss: 0.0594 - accuracy: 0.9768 - val_loss: 0.4480 - val_accuracy: 0.8652\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 34s 114ms/step - loss: 0.0472 - accuracy: 0.9824 - val_loss: 0.4808 - val_accuracy: 0.8662\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 34s 114ms/step - loss: 0.0424 - accuracy: 0.9823 - val_loss: 0.5067 - val_accuracy: 0.8671\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 34s 115ms/step - loss: 0.0409 - accuracy: 0.9828 - val_loss: 0.5051 - val_accuracy: 0.8662\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 34s 113ms/step - loss: 0.0313 - accuracy: 0.9873 - val_loss: 0.5327 - val_accuracy: 0.8520\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 88.97266983985901\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 46s 116ms/step - loss: 0.4285 - accuracy: 0.7980 - val_loss: 0.3062 - val_accuracy: 0.8765\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1875 - accuracy: 0.9344 - val_loss: 0.3368 - val_accuracy: 0.8680\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 32s 105ms/step - loss: 0.1277 - accuracy: 0.9551 - val_loss: 0.3834 - val_accuracy: 0.8605\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0975 - accuracy: 0.9642 - val_loss: 0.4649 - val_accuracy: 0.8483\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0764 - accuracy: 0.9690 - val_loss: 0.4942 - val_accuracy: 0.8577\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.0528 - accuracy: 0.9795 - val_loss: 0.5495 - val_accuracy: 0.8407\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.0423 - accuracy: 0.9845 - val_loss: 0.5670 - val_accuracy: 0.8483\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.0398 - accuracy: 0.9843 - val_loss: 0.5989 - val_accuracy: 0.8473\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0354 - accuracy: 0.9849 - val_loss: 0.6397 - val_accuracy: 0.8426\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 87.65316009521484\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 49s 128ms/step - loss: 0.4319 - accuracy: 0.7974 - val_loss: 0.3092 - val_accuracy: 0.8605\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.2038 - accuracy: 0.9262 - val_loss: 0.3288 - val_accuracy: 0.8652\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.1234 - accuracy: 0.9577 - val_loss: 0.3574 - val_accuracy: 0.8652\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0980 - accuracy: 0.9626 - val_loss: 0.4083 - val_accuracy: 0.8652\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0727 - accuracy: 0.9730 - val_loss: 0.4949 - val_accuracy: 0.8605\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0555 - accuracy: 0.9772 - val_loss: 0.5129 - val_accuracy: 0.8483\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 28s 94ms/step - loss: 0.0503 - accuracy: 0.9781 - val_loss: 0.5626 - val_accuracy: 0.8492\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0377 - accuracy: 0.9852 - val_loss: 0.5932 - val_accuracy: 0.8586\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.0429 - accuracy: 0.9792 - val_loss: 0.6559 - val_accuracy: 0.8549\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.0340 - accuracy: 0.9870 - val_loss: 0.6605 - val_accuracy: 0.8388\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 86.52215003967285\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 35s 78ms/step - loss: 0.4270 - accuracy: 0.8057 - val_loss: 0.3141 - val_accuracy: 0.8841\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.1838 - accuracy: 0.9353 - val_loss: 0.3100 - val_accuracy: 0.8812\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.1229 - accuracy: 0.9560 - val_loss: 0.3361 - val_accuracy: 0.8812\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.0940 - accuracy: 0.9632 - val_loss: 0.3803 - val_accuracy: 0.8784\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.0726 - accuracy: 0.9756 - val_loss: 0.4043 - val_accuracy: 0.8746\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0557 - accuracy: 0.9780 - val_loss: 0.4659 - val_accuracy: 0.8737\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.0460 - accuracy: 0.9827 - val_loss: 0.4864 - val_accuracy: 0.8756\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.0416 - accuracy: 0.9835 - val_loss: 0.4929 - val_accuracy: 0.8746\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0357 - accuracy: 0.9837 - val_loss: 0.5420 - val_accuracy: 0.8709\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 88.40716481208801\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 67s 179ms/step - loss: 0.4292 - accuracy: 0.7992 - val_loss: 0.3531 - val_accuracy: 0.8680\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.1869 - accuracy: 0.9297 - val_loss: 0.3657 - val_accuracy: 0.8662\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.1285 - accuracy: 0.9557 - val_loss: 0.4204 - val_accuracy: 0.8530\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0992 - accuracy: 0.9626 - val_loss: 0.4899 - val_accuracy: 0.8445\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0641 - accuracy: 0.9757 - val_loss: 0.5272 - val_accuracy: 0.8511\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0580 - accuracy: 0.9762 - val_loss: 0.5888 - val_accuracy: 0.8445\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0440 - accuracy: 0.9829 - val_loss: 0.6197 - val_accuracy: 0.8473\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0422 - accuracy: 0.9824 - val_loss: 0.6518 - val_accuracy: 0.8445\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 49s 162ms/step - loss: 0.0374 - accuracy: 0.9850 - val_loss: 0.6902 - val_accuracy: 0.8501\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 86.80490255355835\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 68s 188ms/step - loss: 0.4262 - accuracy: 0.8008 - val_loss: 0.2704 - val_accuracy: 0.8944\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.1909 - accuracy: 0.9313 - val_loss: 0.2807 - val_accuracy: 0.8916\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.1272 - accuracy: 0.9548 - val_loss: 0.3105 - val_accuracy: 0.8897\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.0929 - accuracy: 0.9657 - val_loss: 0.3623 - val_accuracy: 0.8794\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.0738 - accuracy: 0.9703 - val_loss: 0.4014 - val_accuracy: 0.8746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0608 - accuracy: 0.9737 - val_loss: 0.4279 - val_accuracy: 0.8765\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 50s 168ms/step - loss: 0.0490 - accuracy: 0.9795 - val_loss: 0.4692 - val_accuracy: 0.8746\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0423 - accuracy: 0.9835 - val_loss: 0.5043 - val_accuracy: 0.8756\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0410 - accuracy: 0.9843 - val_loss: 0.5268 - val_accuracy: 0.8756\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 89.44392204284668\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 45s 114ms/step - loss: 0.4262 - accuracy: 0.8052 - val_loss: 0.3168 - val_accuracy: 0.8868\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 27s 89ms/step - loss: 0.1833 - accuracy: 0.9361 - val_loss: 0.3421 - val_accuracy: 0.8689\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 0.1294 - accuracy: 0.9547 - val_loss: 0.3652 - val_accuracy: 0.8726\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 0.0934 - accuracy: 0.9663 - val_loss: 0.4114 - val_accuracy: 0.8698\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 0.0730 - accuracy: 0.9737 - val_loss: 0.4715 - val_accuracy: 0.8538\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0558 - accuracy: 0.9795 - val_loss: 0.5151 - val_accuracy: 0.8557\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0503 - accuracy: 0.9790 - val_loss: 0.5393 - val_accuracy: 0.8623\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0369 - accuracy: 0.9850 - val_loss: 0.5334 - val_accuracy: 0.8594\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.0367 - accuracy: 0.9840 - val_loss: 0.5764 - val_accuracy: 0.8575\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 88.67924809455872\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 41s 99ms/step - loss: 0.4269 - accuracy: 0.7986 - val_loss: 0.3023 - val_accuracy: 0.8708\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 27s 89ms/step - loss: 0.1847 - accuracy: 0.9367 - val_loss: 0.3125 - val_accuracy: 0.8745\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 27s 92ms/step - loss: 0.1290 - accuracy: 0.9545 - val_loss: 0.3284 - val_accuracy: 0.8736\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0953 - accuracy: 0.9642 - val_loss: 0.3703 - val_accuracy: 0.8783\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 0.0700 - accuracy: 0.9722 - val_loss: 0.4107 - val_accuracy: 0.8708\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 0.0552 - accuracy: 0.9772 - val_loss: 0.4561 - val_accuracy: 0.8689\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0502 - accuracy: 0.9806 - val_loss: 0.5037 - val_accuracy: 0.8708\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 0.0381 - accuracy: 0.9836 - val_loss: 0.5258 - val_accuracy: 0.8670\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 27s 89ms/step - loss: 0.0373 - accuracy: 0.9855 - val_loss: 0.5619 - val_accuracy: 0.8632\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0335 - accuracy: 0.9861 - val_loss: 0.5782 - val_accuracy: 0.8660\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0339 - accuracy: 0.9866 - val_loss: 0.5854 - val_accuracy: 0.8660\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0279 - accuracy: 0.9867 - val_loss: 0.5797 - val_accuracy: 0.8651\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 87.83018589019775\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 42s 105ms/step - loss: 0.4297 - accuracy: 0.7991 - val_loss: 0.3073 - val_accuracy: 0.8858\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 56s 186ms/step - loss: 0.1924 - accuracy: 0.9303 - val_loss: 0.3255 - val_accuracy: 0.8906\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 56s 187ms/step - loss: 0.1228 - accuracy: 0.9569 - val_loss: 0.3714 - val_accuracy: 0.8840\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0925 - accuracy: 0.9656 - val_loss: 0.4015 - val_accuracy: 0.8802\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 56s 186ms/step - loss: 0.0793 - accuracy: 0.9687 - val_loss: 0.4623 - val_accuracy: 0.8774\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0583 - accuracy: 0.9781 - val_loss: 0.4908 - val_accuracy: 0.8613\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 56s 189ms/step - loss: 0.0533 - accuracy: 0.9781 - val_loss: 0.5482 - val_accuracy: 0.8698\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 56s 186ms/step - loss: 0.0425 - accuracy: 0.9823 - val_loss: 0.5740 - val_accuracy: 0.8660\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 56s 188ms/step - loss: 0.0336 - accuracy: 0.9861 - val_loss: 0.5782 - val_accuracy: 0.8679\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 56s 187ms/step - loss: 0.0346 - accuracy: 0.9851 - val_loss: 0.6279 - val_accuracy: 0.8679\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 89.05660510063171\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 71s 197ms/step - loss: 0.4222 - accuracy: 0.8046 - val_loss: 0.3196 - val_accuracy: 0.8642\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 43s 145ms/step - loss: 0.1851 - accuracy: 0.9332 - val_loss: 0.3374 - val_accuracy: 0.8726\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 42s 140ms/step - loss: 0.1258 - accuracy: 0.9547 - val_loss: 0.3836 - val_accuracy: 0.8689\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 41s 136ms/step - loss: 0.0948 - accuracy: 0.9637 - val_loss: 0.4083 - val_accuracy: 0.8670\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 41s 136ms/step - loss: 0.0774 - accuracy: 0.9703 - val_loss: 0.4730 - val_accuracy: 0.8660\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 41s 135ms/step - loss: 0.0682 - accuracy: 0.9713 - val_loss: 0.5140 - val_accuracy: 0.8613\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 40s 135ms/step - loss: 0.0484 - accuracy: 0.9805 - val_loss: 0.5706 - val_accuracy: 0.8594\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 41s 135ms/step - loss: 0.0433 - accuracy: 0.9825 - val_loss: 0.5640 - val_accuracy: 0.8613\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 40s 135ms/step - loss: 0.0429 - accuracy: 0.9817 - val_loss: 0.6348 - val_accuracy: 0.8491\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 41s 136ms/step - loss: 0.0345 - accuracy: 0.9846 - val_loss: 0.6023 - val_accuracy: 0.8528\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 87.26415038108826\n",
      "\n",
      "       acc1      acc2      acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  88.97267  87.65316  86.52215  88.407165  86.804903  89.443922  88.679248   \n",
      "\n",
      "        acc8       acc9     acc10        AVG  \n",
      "0  87.830186  89.056605  87.26415  88.063416  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.97267</td>\n",
       "      <td>87.65316</td>\n",
       "      <td>86.52215</td>\n",
       "      <td>88.407165</td>\n",
       "      <td>86.804903</td>\n",
       "      <td>89.443922</td>\n",
       "      <td>88.679248</td>\n",
       "      <td>87.830186</td>\n",
       "      <td>89.056605</td>\n",
       "      <td>87.26415</td>\n",
       "      <td>88.063416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc1      acc2      acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  88.97267  87.65316  86.52215  88.407165  86.804903  89.443922  88.679248   \n",
       "\n",
       "        acc8       acc9     acc10        AVG  \n",
       "0  87.830186  89.056605  87.26415  88.063416  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_MPQA_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
