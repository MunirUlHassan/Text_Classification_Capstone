{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "GRU_MR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PkZHB5EELwjc",
        "3onTP0irLwjg",
        "98-h4rh0Lwjz"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i_OByCzLwiU"
      },
      "source": [
        "# GRU Classification with MR Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using GRU model on the MR Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgGKVv68MkcB",
        "outputId": "52cd1019-c3dc-4cc9-e906-79b660401181"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEq_Y64oLwjE",
        "outputId": "b076ecc2-3ef0-4a38-c99b-9b1871e8d9dc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0lGU42mLwjH",
        "outputId": "e1e5d772-73e5-48f0-bf8c-399e199f1c1e"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DtnTwohLwjL"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "OjigUOkWLwjN",
        "outputId": "b9023b70-9a86-495a-e45a-9a936508a532"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Disertasi/0_data/MR/MR.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10662, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simplistic , silly and tedious .</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it 's so laddish and juvenile , only teenage b...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>exploitative and largely devoid of the depth o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>garbus discards the potential for pathological...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a visually flashy but narratively opaque and e...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10657</th>\n",
              "      <td>both exuberantly romantic and serenely melanch...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10658</th>\n",
              "      <td>mazel tov to a film about a family 's joyous l...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10659</th>\n",
              "      <td>standing in the shadows of motown is the best ...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10660</th>\n",
              "      <td>it 's nice to see piscopo again after all thes...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10661</th>\n",
              "      <td>provides a porthole into that noble , tremblin...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10662 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label  split\n",
              "0                       simplistic , silly and tedious .      0  train\n",
              "1      it 's so laddish and juvenile , only teenage b...      0  train\n",
              "2      exploitative and largely devoid of the depth o...      0  train\n",
              "3      garbus discards the potential for pathological...      0  train\n",
              "4      a visually flashy but narratively opaque and e...      0  train\n",
              "...                                                  ...    ...    ...\n",
              "10657  both exuberantly romantic and serenely melanch...      1  train\n",
              "10658  mazel tov to a film about a family 's joyous l...      1  train\n",
              "10659  standing in the shadows of motown is the best ...      1  train\n",
              "10660  it 's nice to see piscopo again after all thes...      1  train\n",
              "10661  provides a porthole into that noble , tremblin...      1  train\n",
              "\n",
              "[10662 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwPMwQcGLwjP",
        "outputId": "8c03db96-b8cb-4616-dfa1-49b1c3610bcd"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10662 entries, 0 to 10661\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10662 non-null  object\n",
            " 1   label     10662 non-null  int64 \n",
            " 2   split     10662 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 250.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "D8NRy3b1LwjR",
        "outputId": "8727a924-1d2e-4d64-83ad-82eaa3ed62fe"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5331</td>\n",
              "      <td>5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5331</td>\n",
              "      <td>5331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          5331   5331\n",
              "1          5331   5331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tr_pdouLwjU"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "etdtqlfrLwjV",
        "outputId": "10b436c7-286d-4767-bbfe-1130d04696ed"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'simplistic , silly and tedious .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL8N5kWILwjW"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6g1BaJvLwjX"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Vx_D1ELwjY"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkO4iLpJLwjZ",
        "outputId": "4e9ed8cc-404e-4493-89d3-83730eee8e1a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
            "Into a sequence of int: [3, 544, 1838, 13, 3909, 3366, 4, 658, 2629, 416, 10, 236, 4, 10112]\n",
            "Into a padded sequence: [    3   544  1838    13  3909  3366     4   658  2629   416    10   236\n",
            "     4 10112     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RiDxbxzLwja",
        "outputId": "736e4b0c-6dd3-4d0e-c6cd-089eb789484f"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "a 3\n",
            "and 4\n",
            "of 5\n",
            "to 6\n",
            "is 7\n",
            "'s 8\n",
            "it 9\n",
            "in 10\n",
            "18760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mztOl28bLwjb"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkZHB5EELwjc"
      },
      "source": [
        "## GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csROg7gmLwjd"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plqXN92ZLwje",
        "outputId": "14c2dd2e-2d23-418a-b324-7b1d375c4b7a"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_41 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_41 (Bidirectio (None, 128)               140544    \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 440,673\n",
            "Trainable params: 440,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1btUa6QiLwjf"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3onTP0irLwjg"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ThSLsg5ILwjh",
        "outputId": "272c3983-02b4-4bb3-a3c2-f02d2f9f67bc"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 122s 372ms/step - loss: 0.6322 - accuracy: 0.6182 - val_loss: 0.4879 - val_accuracy: 0.7648\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 119s 397ms/step - loss: 0.2375 - accuracy: 0.9085 - val_loss: 0.5737 - val_accuracy: 0.7807\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 120s 401ms/step - loss: 0.0724 - accuracy: 0.9762 - val_loss: 0.7641 - val_accuracy: 0.7582\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 117s 390ms/step - loss: 0.0298 - accuracy: 0.9917 - val_loss: 1.0340 - val_accuracy: 0.7685\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 112s 374ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 1.1488 - val_accuracy: 0.7657\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 113s 377ms/step - loss: 0.0066 - accuracy: 0.9987 - val_loss: 1.3445 - val_accuracy: 0.7723\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 113s 377ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 1.6030 - val_accuracy: 0.7676\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 78.0693531036377\n",
            "Training 2: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 99s 291ms/step - loss: 0.6296 - accuracy: 0.6260 - val_loss: 0.4774 - val_accuracy: 0.7779\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 83s 276ms/step - loss: 0.2350 - accuracy: 0.9084 - val_loss: 0.6062 - val_accuracy: 0.7545\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0794 - accuracy: 0.9722 - val_loss: 0.7047 - val_accuracy: 0.7629\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0229 - accuracy: 0.9935 - val_loss: 0.9506 - val_accuracy: 0.7432\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 1.2312 - val_accuracy: 0.7498\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 80s 268ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 1.2504 - val_accuracy: 0.7554\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 77.7881920337677\n",
            "Training 3: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 100s 298ms/step - loss: 0.6345 - accuracy: 0.6209 - val_loss: 0.4656 - val_accuracy: 0.7786\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 94s 314ms/step - loss: 0.2454 - accuracy: 0.9045 - val_loss: 0.5395 - val_accuracy: 0.7720\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 94s 314ms/step - loss: 0.0753 - accuracy: 0.9747 - val_loss: 0.7527 - val_accuracy: 0.7692\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 94s 313ms/step - loss: 0.0288 - accuracy: 0.9908 - val_loss: 0.9641 - val_accuracy: 0.7636\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 93s 311ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 1.0793 - val_accuracy: 0.7533\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 93s 311ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 1.2943 - val_accuracy: 0.7627\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 77.86116600036621\n",
            "Training 4: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 154s 456ms/step - loss: 0.6301 - accuracy: 0.6206 - val_loss: 0.4553 - val_accuracy: 0.7899\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 139s 464ms/step - loss: 0.2396 - accuracy: 0.9103 - val_loss: 0.4734 - val_accuracy: 0.7880\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 134s 448ms/step - loss: 0.0747 - accuracy: 0.9765 - val_loss: 0.6600 - val_accuracy: 0.7786\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 145s 482ms/step - loss: 0.0250 - accuracy: 0.9931 - val_loss: 0.8715 - val_accuracy: 0.7655\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 147s 488ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 1.0338 - val_accuracy: 0.7767\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 146s 487ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 1.3821 - val_accuracy: 0.7674\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 78.98686528205872\n",
            "Training 5: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 148s 454ms/step - loss: 0.6267 - accuracy: 0.6194 - val_loss: 0.4989 - val_accuracy: 0.7552\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 115s 382ms/step - loss: 0.2389 - accuracy: 0.9078 - val_loss: 0.6679 - val_accuracy: 0.7270\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 116s 385ms/step - loss: 0.0738 - accuracy: 0.9769 - val_loss: 0.9243 - val_accuracy: 0.7139\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 116s 386ms/step - loss: 0.0283 - accuracy: 0.9903 - val_loss: 1.1498 - val_accuracy: 0.7214\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 116s 385ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 1.3343 - val_accuracy: 0.7186\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 115s 384ms/step - loss: 0.0097 - accuracy: 0.9966 - val_loss: 1.4812 - val_accuracy: 0.7129\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 75.51594972610474\n",
            "Training 6: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 105s 310ms/step - loss: 0.6385 - accuracy: 0.6098 - val_loss: 0.4849 - val_accuracy: 0.7617\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 91s 302ms/step - loss: 0.2475 - accuracy: 0.9010 - val_loss: 0.5684 - val_accuracy: 0.7645\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 93s 310ms/step - loss: 0.0755 - accuracy: 0.9738 - val_loss: 0.7840 - val_accuracy: 0.7402\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 89s 297ms/step - loss: 0.0220 - accuracy: 0.9936 - val_loss: 0.9636 - val_accuracy: 0.7298\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 89s 298ms/step - loss: 0.0101 - accuracy: 0.9976 - val_loss: 1.2677 - val_accuracy: 0.7542\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 91s 302ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 1.4468 - val_accuracy: 0.7411\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 91s 304ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 1.3459 - val_accuracy: 0.7364\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 76.45403146743774\n",
            "Training 7: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 106s 317ms/step - loss: 0.6310 - accuracy: 0.6222 - val_loss: 0.4619 - val_accuracy: 0.7711\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 90s 301ms/step - loss: 0.2374 - accuracy: 0.9028 - val_loss: 0.4696 - val_accuracy: 0.7674\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 89s 296ms/step - loss: 0.0843 - accuracy: 0.9720 - val_loss: 0.7952 - val_accuracy: 0.7767\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 90s 300ms/step - loss: 0.0250 - accuracy: 0.9922 - val_loss: 0.8146 - val_accuracy: 0.7786\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 89s 298ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 1.1876 - val_accuracy: 0.7711\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 90s 298ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 1.0877 - val_accuracy: 0.7636\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 89s 298ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 1.3148 - val_accuracy: 0.7645\n",
            "Epoch 8/15\n",
            "300/300 [==============================] - 86s 288ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 1.5954 - val_accuracy: 0.7523\n",
            "Epoch 9/15\n",
            "300/300 [==============================] - 88s 292ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 1.2888 - val_accuracy: 0.7749\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 77.86116600036621\n",
            "Training 8: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 139s 415ms/step - loss: 0.6339 - accuracy: 0.6086 - val_loss: 0.4879 - val_accuracy: 0.7683\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 121s 403ms/step - loss: 0.2563 - accuracy: 0.8985 - val_loss: 0.5613 - val_accuracy: 0.7608\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 121s 404ms/step - loss: 0.0679 - accuracy: 0.9779 - val_loss: 0.7430 - val_accuracy: 0.7570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/15\n",
            "300/300 [==============================] - 121s 404ms/step - loss: 0.0220 - accuracy: 0.9952 - val_loss: 0.9008 - val_accuracy: 0.7570\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 121s 403ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 1.0815 - val_accuracy: 0.7561\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 121s 405ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 1.1452 - val_accuracy: 0.7608\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 76.82926654815674\n",
            "Training 9: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 123s 374ms/step - loss: 0.6291 - accuracy: 0.6133 - val_loss: 0.4601 - val_accuracy: 0.7824\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 107s 355ms/step - loss: 0.2401 - accuracy: 0.9024 - val_loss: 0.5134 - val_accuracy: 0.7777\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 105s 349ms/step - loss: 0.0696 - accuracy: 0.9775 - val_loss: 0.6872 - val_accuracy: 0.7580\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 105s 350ms/step - loss: 0.0253 - accuracy: 0.9935 - val_loss: 0.7997 - val_accuracy: 0.7674\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 105s 349ms/step - loss: 0.0142 - accuracy: 0.9948 - val_loss: 1.0282 - val_accuracy: 0.7795\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 104s 348ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 1.2236 - val_accuracy: 0.7711\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 78.23639512062073\n",
            "Training 10: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 95s 282ms/step - loss: 0.6281 - accuracy: 0.6296 - val_loss: 0.5018 - val_accuracy: 0.7411\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 83s 276ms/step - loss: 0.2488 - accuracy: 0.9027 - val_loss: 0.5807 - val_accuracy: 0.7430\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 83s 275ms/step - loss: 0.0803 - accuracy: 0.9737 - val_loss: 0.7884 - val_accuracy: 0.7420\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 82s 273ms/step - loss: 0.0250 - accuracy: 0.9920 - val_loss: 1.0968 - val_accuracy: 0.7355\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 83s 275ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 1.4069 - val_accuracy: 0.7298\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 83s 278ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 1.4996 - val_accuracy: 0.7270\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 83s 278ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 1.2885 - val_accuracy: 0.7336\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 74.29643273353577\n",
            "\n",
            "        acc1       acc2       acc3       acc4      acc5       acc6       acc7  \\\n",
            "0  78.069353  77.788192  77.861166  78.986865  75.51595  76.454031  77.861166   \n",
            "\n",
            "        acc8       acc9      acc10        AVG  \n",
            "0  76.829267  78.236395  74.296433  77.189882  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98-h4rh0Lwjz"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcNO99sdLwj7",
        "outputId": "e39f5e54-a474-45d1-bb25-25bf6916015a"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78.069353</td>\n",
              "      <td>77.788192</td>\n",
              "      <td>77.861166</td>\n",
              "      <td>78.986865</td>\n",
              "      <td>75.51595</td>\n",
              "      <td>76.454031</td>\n",
              "      <td>77.861166</td>\n",
              "      <td>76.829267</td>\n",
              "      <td>78.236395</td>\n",
              "      <td>74.296433</td>\n",
              "      <td>77.189882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3       acc4      acc5       acc6       acc7  \\\n",
              "0  78.069353  77.788192  77.861166  78.986865  75.51595  76.454031  77.861166   \n",
              "\n",
              "        acc8       acc9      acc10        AVG  \n",
              "0  76.829267  78.236395  74.296433  77.189882  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EihdLBoJLwj9"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('GRU_MR.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5RmL5rrLwj_"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbjRdQYxLwkA"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0gGYlMhLwkB"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AeGo9w_LwkE"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Disertasi/WordEmbedding_Models/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7EEQfwrLwkF",
        "outputId": "00a8d7e9-a3b8-42c3-9b7e-d5d2e60167ea"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMmSmSMhLwkH"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFU8onJwLwkI"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_2-5CJ3LwkJ",
        "outputId": "4eb56283-efa5-4a89-c839-e9cdd9e7950c"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16448 words present from 18760 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg9RnP9xLwkK"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV20O7mOLwkL",
        "outputId": "73867f86-d5e4-4068-ff70-7c40810219f7"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jICvQzvLwkM"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICl89axYLwkN",
        "outputId": "960f5813-7d57-48bd-fa72-2016c96bacdf"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3UqmYqLwkO"
      },
      "source": [
        "## GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVn3lnxvLwkO"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4roHSr2ILwkP",
        "outputId": "676a06bc-60aa-408f-e330-50e1f890a07b"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               140544    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 440,673\n",
            "Trainable params: 140,673\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbYiRnbLwki"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_6xjV2LLwkk"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM9ERc-ILwkm",
        "outputId": "936e13c7-f68e-4be0-b40c-10269b7e0c0f"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 29s 75ms/step - loss: 0.5913 - accuracy: 0.6685 - val_loss: 0.4449 - val_accuracy: 0.7919\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.4497 - accuracy: 0.7784 - val_loss: 0.4210 - val_accuracy: 0.7929\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.4162 - accuracy: 0.8099 - val_loss: 0.4194 - val_accuracy: 0.7985\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.3906 - accuracy: 0.8172 - val_loss: 0.4308 - val_accuracy: 0.7882\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.3647 - accuracy: 0.8336 - val_loss: 0.4107 - val_accuracy: 0.7948\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3378 - accuracy: 0.8529 - val_loss: 0.4407 - val_accuracy: 0.7985\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.3107 - accuracy: 0.8633 - val_loss: 0.4327 - val_accuracy: 0.8022\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.2663 - accuracy: 0.8891 - val_loss: 0.4375 - val_accuracy: 0.7976\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.2286 - accuracy: 0.9068 - val_loss: 0.4843 - val_accuracy: 0.7919\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.1785 - accuracy: 0.9295 - val_loss: 0.5378 - val_accuracy: 0.7844\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.1501 - accuracy: 0.9417 - val_loss: 0.5897 - val_accuracy: 0.7751\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1123 - accuracy: 0.9612 - val_loss: 0.6386 - val_accuracy: 0.7844\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0829 - accuracy: 0.9719 - val_loss: 0.7606 - val_accuracy: 0.7760\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0609 - accuracy: 0.9807 - val_loss: 0.8237 - val_accuracy: 0.7732\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.8910 - val_accuracy: 0.7882\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 80.22493124008179\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 30s 78ms/step - loss: 0.5942 - accuracy: 0.6602 - val_loss: 0.4844 - val_accuracy: 0.7648\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.4498 - accuracy: 0.7854 - val_loss: 0.4814 - val_accuracy: 0.7582\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.4150 - accuracy: 0.7997 - val_loss: 0.4615 - val_accuracy: 0.7732\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3838 - accuracy: 0.8238 - val_loss: 0.4605 - val_accuracy: 0.7732\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3562 - accuracy: 0.8368 - val_loss: 0.4802 - val_accuracy: 0.7873\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.3265 - accuracy: 0.8555 - val_loss: 0.5019 - val_accuracy: 0.7723\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.3076 - accuracy: 0.8666 - val_loss: 0.5001 - val_accuracy: 0.7844\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.2703 - accuracy: 0.8834 - val_loss: 0.5252 - val_accuracy: 0.7854\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.2206 - accuracy: 0.9147 - val_loss: 0.5782 - val_accuracy: 0.7732\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1823 - accuracy: 0.9268 - val_loss: 0.6471 - val_accuracy: 0.7601\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1511 - accuracy: 0.9421 - val_loss: 0.6851 - val_accuracy: 0.7648\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1137 - accuracy: 0.9565 - val_loss: 0.8201 - val_accuracy: 0.7694\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0769 - accuracy: 0.9741 - val_loss: 0.7988 - val_accuracy: 0.7601\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 78.72539758682251\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 30s 80ms/step - loss: 0.5979 - accuracy: 0.6558 - val_loss: 0.4709 - val_accuracy: 0.7645\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.4316 - accuracy: 0.7944 - val_loss: 0.4585 - val_accuracy: 0.7720\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.4116 - accuracy: 0.8058 - val_loss: 0.4489 - val_accuracy: 0.7739\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.3902 - accuracy: 0.8181 - val_loss: 0.4575 - val_accuracy: 0.7777\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3629 - accuracy: 0.8355 - val_loss: 0.4640 - val_accuracy: 0.7889\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3246 - accuracy: 0.8551 - val_loss: 0.4729 - val_accuracy: 0.7824\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.2988 - accuracy: 0.8719 - val_loss: 0.5248 - val_accuracy: 0.7749\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.2607 - accuracy: 0.8887 - val_loss: 0.5396 - val_accuracy: 0.7758\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.2302 - accuracy: 0.9060 - val_loss: 0.5624 - val_accuracy: 0.7917\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.2024 - accuracy: 0.9155 - val_loss: 0.5908 - val_accuracy: 0.7861\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.1536 - accuracy: 0.9392 - val_loss: 0.6637 - val_accuracy: 0.7767\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.1182 - accuracy: 0.9546 - val_loss: 0.7683 - val_accuracy: 0.7692\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0916 - accuracy: 0.9651 - val_loss: 0.8611 - val_accuracy: 0.7739\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0783 - accuracy: 0.9714 - val_loss: 0.9284 - val_accuracy: 0.7683\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0613 - accuracy: 0.9798 - val_loss: 0.9573 - val_accuracy: 0.7552\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0407 - accuracy: 0.9880 - val_loss: 1.0977 - val_accuracy: 0.7533\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0354 - accuracy: 0.9879 - val_loss: 1.1971 - val_accuracy: 0.7589\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00017: early stopping\n",
            "Test Accuracy: 79.17448282241821\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 29s 75ms/step - loss: 0.5953 - accuracy: 0.6670 - val_loss: 0.4616 - val_accuracy: 0.7711\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 21s 68ms/step - loss: 0.4459 - accuracy: 0.7934 - val_loss: 0.4651 - val_accuracy: 0.7636\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.4186 - accuracy: 0.8018 - val_loss: 0.4488 - val_accuracy: 0.7852\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3856 - accuracy: 0.8199 - val_loss: 0.4346 - val_accuracy: 0.7871\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.3532 - accuracy: 0.8345 - val_loss: 0.4378 - val_accuracy: 0.7833\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.3273 - accuracy: 0.8602 - val_loss: 0.4388 - val_accuracy: 0.7955\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.2942 - accuracy: 0.8740 - val_loss: 0.4530 - val_accuracy: 0.7946\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.2630 - accuracy: 0.8913 - val_loss: 0.4841 - val_accuracy: 0.7889\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.2295 - accuracy: 0.9099 - val_loss: 0.4977 - val_accuracy: 0.7908\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.1960 - accuracy: 0.9196 - val_loss: 0.5607 - val_accuracy: 0.7880\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1554 - accuracy: 0.9434 - val_loss: 0.6249 - val_accuracy: 0.7749\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 68ms/step - loss: 0.1188 - accuracy: 0.9567 - val_loss: 0.6680 - val_accuracy: 0.7795\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0936 - accuracy: 0.9676 - val_loss: 0.7372 - val_accuracy: 0.7861\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0754 - accuracy: 0.9736 - val_loss: 0.8761 - val_accuracy: 0.8030\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0515 - accuracy: 0.9826 - val_loss: 0.8745 - val_accuracy: 0.7730\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0621 - accuracy: 0.9759 - val_loss: 0.9686 - val_accuracy: 0.7861\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0311 - accuracy: 0.9894 - val_loss: 1.2218 - val_accuracy: 0.7627\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0428 - accuracy: 0.9848 - val_loss: 0.9812 - val_accuracy: 0.7824\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0289 - accuracy: 0.9898 - val_loss: 1.0625 - val_accuracy: 0.7795\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 21s 68ms/step - loss: 0.0147 - accuracy: 0.9961 - val_loss: 1.1222 - val_accuracy: 0.7871\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0291 - accuracy: 0.9888 - val_loss: 1.1235 - val_accuracy: 0.7824\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0165 - accuracy: 0.9966 - val_loss: 1.2005 - val_accuracy: 0.7814\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00022: early stopping\n",
            "Test Accuracy: 80.3001880645752\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 29s 76ms/step - loss: 0.5906 - accuracy: 0.6604 - val_loss: 0.4871 - val_accuracy: 0.7570\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.4478 - accuracy: 0.7860 - val_loss: 0.4447 - val_accuracy: 0.7871\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4064 - accuracy: 0.8049 - val_loss: 0.4332 - val_accuracy: 0.7880\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3855 - accuracy: 0.8232 - val_loss: 0.4330 - val_accuracy: 0.7983\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.3579 - accuracy: 0.8400 - val_loss: 0.4371 - val_accuracy: 0.7983\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.3192 - accuracy: 0.8561 - val_loss: 0.4685 - val_accuracy: 0.7880\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2946 - accuracy: 0.8693 - val_loss: 0.4573 - val_accuracy: 0.7964\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2555 - accuracy: 0.8982 - val_loss: 0.5037 - val_accuracy: 0.7889\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2095 - accuracy: 0.9178 - val_loss: 0.5392 - val_accuracy: 0.7927\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1791 - accuracy: 0.9303 - val_loss: 0.5951 - val_accuracy: 0.7889\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1429 - accuracy: 0.9476 - val_loss: 0.6450 - val_accuracy: 0.7777\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1199 - accuracy: 0.9534 - val_loss: 0.8061 - val_accuracy: 0.7664\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 79.83114719390869\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 29s 77ms/step - loss: 0.5890 - accuracy: 0.6662 - val_loss: 0.4834 - val_accuracy: 0.7711\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4362 - accuracy: 0.7880 - val_loss: 0.4660 - val_accuracy: 0.7805\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4093 - accuracy: 0.8129 - val_loss: 0.4629 - val_accuracy: 0.7720\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3929 - accuracy: 0.8177 - val_loss: 0.4615 - val_accuracy: 0.7899\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3737 - accuracy: 0.8268 - val_loss: 0.5134 - val_accuracy: 0.7627\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3416 - accuracy: 0.8455 - val_loss: 0.4615 - val_accuracy: 0.7974\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3060 - accuracy: 0.8685 - val_loss: 0.4972 - val_accuracy: 0.7917\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2687 - accuracy: 0.8887 - val_loss: 0.5250 - val_accuracy: 0.7711\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2259 - accuracy: 0.9109 - val_loss: 0.5200 - val_accuracy: 0.7974\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1948 - accuracy: 0.9234 - val_loss: 0.5666 - val_accuracy: 0.7908\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1562 - accuracy: 0.9405 - val_loss: 0.5879 - val_accuracy: 0.7824\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1212 - accuracy: 0.9551 - val_loss: 0.6690 - val_accuracy: 0.7842\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0996 - accuracy: 0.9657 - val_loss: 0.7710 - val_accuracy: 0.7786\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 20s 68ms/step - loss: 0.0702 - accuracy: 0.9782 - val_loss: 0.8477 - val_accuracy: 0.7777\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 79.7373354434967\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 30s 77ms/step - loss: 0.5972 - accuracy: 0.6646 - val_loss: 0.4613 - val_accuracy: 0.7720\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4441 - accuracy: 0.7901 - val_loss: 0.4468 - val_accuracy: 0.7824\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.4104 - accuracy: 0.8061 - val_loss: 0.4397 - val_accuracy: 0.7814\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4011 - accuracy: 0.8131 - val_loss: 0.4256 - val_accuracy: 0.7936\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.3565 - accuracy: 0.8405 - val_loss: 0.4213 - val_accuracy: 0.8030\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3352 - accuracy: 0.8450 - val_loss: 0.4442 - val_accuracy: 0.7992\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.3056 - accuracy: 0.8672 - val_loss: 0.4480 - val_accuracy: 0.8021\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.2502 - accuracy: 0.8921 - val_loss: 0.4786 - val_accuracy: 0.7805\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2314 - accuracy: 0.9024 - val_loss: 0.5339 - val_accuracy: 0.7795\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.2038 - accuracy: 0.9136 - val_loss: 0.5848 - val_accuracy: 0.7927\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.1575 - accuracy: 0.9387 - val_loss: 0.6374 - val_accuracy: 0.7842\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1157 - accuracy: 0.9574 - val_loss: 0.6586 - val_accuracy: 0.7880\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0934 - accuracy: 0.9691 - val_loss: 0.8230 - val_accuracy: 0.7814\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 80.3001880645752\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 33s 79ms/step - loss: 0.6037 - accuracy: 0.6445 - val_loss: 0.4691 - val_accuracy: 0.7664\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4483 - accuracy: 0.7852 - val_loss: 0.4753 - val_accuracy: 0.7589\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.4100 - accuracy: 0.8055 - val_loss: 0.4510 - val_accuracy: 0.7814\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3932 - accuracy: 0.8170 - val_loss: 0.4474 - val_accuracy: 0.7833\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3644 - accuracy: 0.8340 - val_loss: 0.4695 - val_accuracy: 0.7664\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3348 - accuracy: 0.8470 - val_loss: 0.4508 - val_accuracy: 0.7814\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3114 - accuracy: 0.8613 - val_loss: 0.4775 - val_accuracy: 0.7852\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.2672 - accuracy: 0.8854 - val_loss: 0.5185 - val_accuracy: 0.7570\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.2216 - accuracy: 0.9093 - val_loss: 0.5645 - val_accuracy: 0.7749\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1826 - accuracy: 0.9274 - val_loss: 0.5802 - val_accuracy: 0.7711\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1397 - accuracy: 0.9498 - val_loss: 0.6830 - val_accuracy: 0.7842\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.1135 - accuracy: 0.9569 - val_loss: 0.7501 - val_accuracy: 0.7664\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0912 - accuracy: 0.9678 - val_loss: 0.8385 - val_accuracy: 0.7627\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0674 - accuracy: 0.9785 - val_loss: 0.9199 - val_accuracy: 0.7730\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.0494 - accuracy: 0.9858 - val_loss: 1.0058 - val_accuracy: 0.7739\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 78.51782441139221\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 30s 78ms/step - loss: 0.5980 - accuracy: 0.6477 - val_loss: 0.4630 - val_accuracy: 0.7720\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.4483 - accuracy: 0.7825 - val_loss: 0.4546 - val_accuracy: 0.7674\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.4220 - accuracy: 0.8001 - val_loss: 0.4540 - val_accuracy: 0.7739\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 22s 72ms/step - loss: 0.3880 - accuracy: 0.8191 - val_loss: 0.4616 - val_accuracy: 0.7711\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 21s 72ms/step - loss: 0.3571 - accuracy: 0.8395 - val_loss: 0.4586 - val_accuracy: 0.7730\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3319 - accuracy: 0.8587 - val_loss: 0.5053 - val_accuracy: 0.7692\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 21s 72ms/step - loss: 0.2953 - accuracy: 0.8762 - val_loss: 0.4980 - val_accuracy: 0.7758\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.2739 - accuracy: 0.8829 - val_loss: 0.5176 - val_accuracy: 0.7749\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.2245 - accuracy: 0.9089 - val_loss: 0.6135 - val_accuracy: 0.7598\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 72ms/step - loss: 0.1951 - accuracy: 0.9195 - val_loss: 0.6144 - val_accuracy: 0.7645\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.1476 - accuracy: 0.9474 - val_loss: 0.7500 - val_accuracy: 0.7523\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.1222 - accuracy: 0.9542 - val_loss: 0.7706 - val_accuracy: 0.7430\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 22s 72ms/step - loss: 0.0958 - accuracy: 0.9671 - val_loss: 0.8950 - val_accuracy: 0.7636\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0693 - accuracy: 0.9754 - val_loss: 0.9385 - val_accuracy: 0.7505\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0502 - accuracy: 0.9861 - val_loss: 1.0833 - val_accuracy: 0.7598\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 77.57973670959473\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 30s 79ms/step - loss: 0.5915 - accuracy: 0.6702 - val_loss: 0.4681 - val_accuracy: 0.7758\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.4490 - accuracy: 0.7829 - val_loss: 0.4450 - val_accuracy: 0.7871\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 22s 72ms/step - loss: 0.4145 - accuracy: 0.8074 - val_loss: 0.4494 - val_accuracy: 0.7824\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3848 - accuracy: 0.8159 - val_loss: 0.4642 - val_accuracy: 0.7814\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.3694 - accuracy: 0.8338 - val_loss: 0.4533 - val_accuracy: 0.8002\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3242 - accuracy: 0.8547 - val_loss: 0.4445 - val_accuracy: 0.8030\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.3006 - accuracy: 0.8698 - val_loss: 0.4667 - val_accuracy: 0.8039\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.2704 - accuracy: 0.8846 - val_loss: 0.4749 - val_accuracy: 0.7964\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.2277 - accuracy: 0.9085 - val_loss: 0.5044 - val_accuracy: 0.7917\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.1832 - accuracy: 0.9257 - val_loss: 0.5641 - val_accuracy: 0.7824\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 21s 72ms/step - loss: 0.1542 - accuracy: 0.9390 - val_loss: 0.6447 - val_accuracy: 0.7927\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.1171 - accuracy: 0.9569 - val_loss: 0.7014 - val_accuracy: 0.7917\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0911 - accuracy: 0.9672 - val_loss: 0.8154 - val_accuracy: 0.7645\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 21s 71ms/step - loss: 0.0589 - accuracy: 0.9811 - val_loss: 0.8573 - val_accuracy: 0.7908\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 21s 72ms/step - loss: 0.0572 - accuracy: 0.9801 - val_loss: 0.9281 - val_accuracy: 0.7899\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 80.3939938545227\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
            "0  80.224931  78.725398  79.174483  ...  77.579737  80.393994  79.478523\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc8dHm5VLwko"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "F3NoZKDBLwks",
        "outputId": "fb781fc0-abbd-4051-923d-f5c37f6f3817"
      },
      "source": [
        "record2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.224931</td>\n",
              "      <td>78.725398</td>\n",
              "      <td>79.174483</td>\n",
              "      <td>80.300188</td>\n",
              "      <td>79.831147</td>\n",
              "      <td>79.737335</td>\n",
              "      <td>80.300188</td>\n",
              "      <td>78.517824</td>\n",
              "      <td>77.579737</td>\n",
              "      <td>80.393994</td>\n",
              "      <td>79.478523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
              "0  80.224931  78.725398  79.174483  ...  77.579737  80.393994  79.478523\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbinhZ5tLwk2"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('GRU_MR_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etan1RpiLwk3"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp_tq8MCLwk4"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2fhP8owLwk5"
      },
      "source": [
        "## GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGMPAKxtLwk6"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDNan8keLwk7",
        "outputId": "18efa08e-549b-4a2d-8308-9a33c7f6677d"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 128)               140544    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 440,673\n",
            "Trainable params: 440,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzrnZgcLwk8"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXlt_pRrLwk9"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcUhm7UBLwk-",
        "outputId": "6142ffa1-d603-426c-c87d-f2314dabec68"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record3)\n",
        "print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 50s 144ms/step - loss: 0.5836 - accuracy: 0.6649 - val_loss: 0.4268 - val_accuracy: 0.8041\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.2568 - accuracy: 0.9009 - val_loss: 0.4623 - val_accuracy: 0.7957\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0964 - accuracy: 0.9669 - val_loss: 0.6542 - val_accuracy: 0.7863\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.0373 - accuracy: 0.9907 - val_loss: 0.8982 - val_accuracy: 0.7882\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.0141 - accuracy: 0.9965 - val_loss: 1.0326 - val_accuracy: 0.7788\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 1.1725 - val_accuracy: 0.7798\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 1.3832 - val_accuracy: 0.7798\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 1.5032 - val_accuracy: 0.7788\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 7.4770e-04 - accuracy: 1.0000 - val_loss: 1.6068 - val_accuracy: 0.7723\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.41236996650696\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 50s 143ms/step - loss: 0.5742 - accuracy: 0.6821 - val_loss: 0.4350 - val_accuracy: 0.7844\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.2617 - accuracy: 0.8942 - val_loss: 0.4942 - val_accuracy: 0.7826\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.1136 - accuracy: 0.9592 - val_loss: 0.6471 - val_accuracy: 0.7732\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.0347 - accuracy: 0.9906 - val_loss: 0.8230 - val_accuracy: 0.7610\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 0.9349 - val_accuracy: 0.7648\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 1.2494 - val_accuracy: 0.7516\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.5803 - val_accuracy: 0.7676\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 1.3644 - val_accuracy: 0.7573\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0056 - accuracy: 0.9979 - val_loss: 1.4877 - val_accuracy: 0.7601\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 78.44423651695251\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 53s 145ms/step - loss: 0.5835 - accuracy: 0.6715 - val_loss: 0.4857 - val_accuracy: 0.7430\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 42s 142ms/step - loss: 0.2525 - accuracy: 0.8973 - val_loss: 0.5029 - val_accuracy: 0.7608\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.1062 - accuracy: 0.9627 - val_loss: 0.7249 - val_accuracy: 0.7655\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0300 - accuracy: 0.9921 - val_loss: 1.0159 - val_accuracy: 0.7561\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 42s 141ms/step - loss: 0.0149 - accuracy: 0.9961 - val_loss: 1.1428 - val_accuracy: 0.7430\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 1.3848 - val_accuracy: 0.7674\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 42s 140ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 1.5998 - val_accuracy: 0.7561\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.6941 - val_accuracy: 0.7552\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 42s 140ms/step - loss: 2.7172e-04 - accuracy: 1.0000 - val_loss: 1.8614 - val_accuracy: 0.7570\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 1.8975e-04 - accuracy: 1.0000 - val_loss: 1.9609 - val_accuracy: 0.7580\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 1.4905e-04 - accuracy: 1.0000 - val_loss: 1.7863 - val_accuracy: 0.7664\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 1.3558e-04 - accuracy: 1.0000 - val_loss: 2.0604 - val_accuracy: 0.7598\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 6.8347e-05 - accuracy: 1.0000 - val_loss: 2.0884 - val_accuracy: 0.7627\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 41s 136ms/step - loss: 4.5809e-05 - accuracy: 1.0000 - val_loss: 2.1736 - val_accuracy: 0.7608\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 76.73546075820923\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 50s 145ms/step - loss: 0.5759 - accuracy: 0.6800 - val_loss: 0.4250 - val_accuracy: 0.8021\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 43s 143ms/step - loss: 0.2489 - accuracy: 0.8985 - val_loss: 0.4622 - val_accuracy: 0.7871\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.1096 - accuracy: 0.9641 - val_loss: 0.6030 - val_accuracy: 0.7908\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0410 - accuracy: 0.9878 - val_loss: 0.8421 - val_accuracy: 0.7861\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0132 - accuracy: 0.9970 - val_loss: 1.0148 - val_accuracy: 0.7702\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 43s 143ms/step - loss: 0.0052 - accuracy: 0.9993 - val_loss: 1.2264 - val_accuracy: 0.7871\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 1.3742 - val_accuracy: 0.7767\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 1.4582 - val_accuracy: 0.7852\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 1.6566 - val_accuracy: 0.7767\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.20637631416321\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 49s 143ms/step - loss: 0.5836 - accuracy: 0.6733 - val_loss: 0.4407 - val_accuracy: 0.7889\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 41s 136ms/step - loss: 0.2575 - accuracy: 0.8953 - val_loss: 0.4693 - val_accuracy: 0.7814\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 41s 136ms/step - loss: 0.1064 - accuracy: 0.9658 - val_loss: 0.6630 - val_accuracy: 0.7899\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 0.7189 - val_accuracy: 0.7795\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 41s 136ms/step - loss: 0.0140 - accuracy: 0.9967 - val_loss: 0.8692 - val_accuracy: 0.7702\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 1.2145 - val_accuracy: 0.7692\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 1.0587 - val_accuracy: 0.7749\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 1.3480 - val_accuracy: 0.7871\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 41s 136ms/step - loss: 5.0930e-04 - accuracy: 1.0000 - val_loss: 1.4924 - val_accuracy: 0.7758\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 2.9846e-04 - accuracy: 1.0000 - val_loss: 1.5887 - val_accuracy: 0.7889\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 1.5967e-04 - accuracy: 1.0000 - val_loss: 1.6960 - val_accuracy: 0.7852\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 78.98686528205872\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 51s 150ms/step - loss: 0.5731 - accuracy: 0.6833 - val_loss: 0.4377 - val_accuracy: 0.7974\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.2425 - accuracy: 0.9037 - val_loss: 0.4849 - val_accuracy: 0.7842\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.0974 - accuracy: 0.9644 - val_loss: 0.6841 - val_accuracy: 0.7711\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.0312 - accuracy: 0.9910 - val_loss: 0.8649 - val_accuracy: 0.7683\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 1.0220 - val_accuracy: 0.7552\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 1.1966 - val_accuracy: 0.7739\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 1.6475 - val_accuracy: 0.7627\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 8.3345e-04 - accuracy: 0.9998 - val_loss: 1.6535 - val_accuracy: 0.7608\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 1.6985 - val_accuracy: 0.7645\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.7373354434967\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 52s 151ms/step - loss: 0.5783 - accuracy: 0.6788 - val_loss: 0.4507 - val_accuracy: 0.7739\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.2655 - accuracy: 0.8948 - val_loss: 0.4683 - val_accuracy: 0.7805\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.1067 - accuracy: 0.9666 - val_loss: 0.6418 - val_accuracy: 0.7580\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 43s 143ms/step - loss: 0.0363 - accuracy: 0.9897 - val_loss: 0.8648 - val_accuracy: 0.7477\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 1.1758 - val_accuracy: 0.7561\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 1.2836 - val_accuracy: 0.7664\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 43s 143ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 1.5729 - val_accuracy: 0.7580\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 1.4399 - val_accuracy: 0.7570\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 1.3626 - val_accuracy: 0.7533\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 1.5661 - val_accuracy: 0.7561\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 78.04877758026123\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 48s 140ms/step - loss: 0.5735 - accuracy: 0.6716 - val_loss: 0.3999 - val_accuracy: 0.8161\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.2568 - accuracy: 0.8987 - val_loss: 0.4149 - val_accuracy: 0.8086\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.1063 - accuracy: 0.9643 - val_loss: 0.5450 - val_accuracy: 0.7946\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 40s 133ms/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.7293 - val_accuracy: 0.7852\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.0128 - accuracy: 0.9975 - val_loss: 0.9602 - val_accuracy: 0.7786\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.9790 - val_accuracy: 0.7674\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.2552 - val_accuracy: 0.7683\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.3300 - val_accuracy: 0.7758\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 4.1350e-04 - accuracy: 1.0000 - val_loss: 1.4794 - val_accuracy: 0.7767\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 81.61351084709167\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 50s 145ms/step - loss: 0.5774 - accuracy: 0.6781 - val_loss: 0.4703 - val_accuracy: 0.7608\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.2421 - accuracy: 0.9042 - val_loss: 0.5122 - val_accuracy: 0.7767\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 41s 137ms/step - loss: 0.1018 - accuracy: 0.9635 - val_loss: 0.7218 - val_accuracy: 0.7674\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.0310 - accuracy: 0.9917 - val_loss: 0.8158 - val_accuracy: 0.7608\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 1.0279 - val_accuracy: 0.7664\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 1.2409 - val_accuracy: 0.7645\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 1.3421 - val_accuracy: 0.7730\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 42s 141ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 1.6799 - val_accuracy: 0.7711\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 42s 139ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 1.3138 - val_accuracy: 0.7655\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 41s 138ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 1.9042 - val_accuracy: 0.7645\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 77.67354846000671\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 49s 141ms/step - loss: 0.5716 - accuracy: 0.6893 - val_loss: 0.4191 - val_accuracy: 0.8030\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.2496 - accuracy: 0.9002 - val_loss: 0.4576 - val_accuracy: 0.7899\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 41s 135ms/step - loss: 0.0965 - accuracy: 0.9664 - val_loss: 0.6566 - val_accuracy: 0.7720\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 0.0404 - accuracy: 0.9885 - val_loss: 0.9188 - val_accuracy: 0.7758\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 40s 133ms/step - loss: 0.0152 - accuracy: 0.9964 - val_loss: 1.2070 - val_accuracy: 0.7777\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 1.3440 - val_accuracy: 0.7739\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 40s 134ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.4755 - val_accuracy: 0.7720\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 41s 135ms/step - loss: 9.1438e-04 - accuracy: 1.0000 - val_loss: 1.4720 - val_accuracy: 0.7730\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 40s 135ms/step - loss: 4.0180e-04 - accuracy: 1.0000 - val_loss: 1.6953 - val_accuracy: 0.7777\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.3001880645752\n",
            "\n",
            "       acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
            "0  80.41237  78.444237  76.735461  ...  77.673548  80.300188  79.215867\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KcyMTGKLwk_"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "CeoF8YMGLwlA",
        "outputId": "4db8e005-5cb8-49ad-b1df-4309c5964bcb"
      },
      "source": [
        "record3"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.41237</td>\n",
              "      <td>78.444237</td>\n",
              "      <td>76.735461</td>\n",
              "      <td>80.206376</td>\n",
              "      <td>78.986865</td>\n",
              "      <td>79.737335</td>\n",
              "      <td>78.048778</td>\n",
              "      <td>81.613511</td>\n",
              "      <td>77.673548</td>\n",
              "      <td>80.300188</td>\n",
              "      <td>79.215867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
              "0  80.41237  78.444237  76.735461  ...  77.673548  80.300188  79.215867\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v859Ar4ILwlB"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('GRU_MR_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}