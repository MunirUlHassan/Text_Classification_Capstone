{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "GRU_SUBJ.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WWlmjIgxs55-",
        "d4Z1Yyj4s56B",
        "uBdcqotMs56L"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqjkpB-6s55A"
      },
      "source": [
        "# LSTM Classification with SUBJ Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using LSTM model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53um38vJtJ3V",
        "outputId": "93d72cdc-7541-425f-a748-311a129f53b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaFtb-is55m",
        "outputId": "4984554b-67b6-4d2b-f009-d3412bf90511"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_LqgI2Gs55q",
        "outputId": "e27d71c6-b1ba-42a2-e00a-221b08be22bb"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dtcr9G-s55u"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "HUbuqgQUs55w",
        "outputId": "3d3370ca-af31-4388-eaab-23cec612b997"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Disertasi/0_data/SUBJ/SUBJ.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>smart and alert , thirteen conversations about...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>color , musical bounce and warm seas lapping o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it is not a mass market entertainment but an u...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a light hearted french film about the spiritua...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my wife is an actress has its moments in looki...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>in the end , they discover that balance in lif...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>enter the beautiful and mysterious secret agen...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>after listening to a missionary from china spe...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>looking for a short cut to fame , glass concoc...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label  split\n",
              "0     smart and alert , thirteen conversations about...      0  train\n",
              "1     color , musical bounce and warm seas lapping o...      0  train\n",
              "2     it is not a mass market entertainment but an u...      0  train\n",
              "3     a light hearted french film about the spiritua...      0  train\n",
              "4     my wife is an actress has its moments in looki...      0  train\n",
              "...                                                 ...    ...    ...\n",
              "9995  in the end , they discover that balance in lif...      1  train\n",
              "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
              "9997  enter the beautiful and mysterious secret agen...      1  train\n",
              "9998  after listening to a missionary from china spe...      1  train\n",
              "9999  looking for a short cut to fame , glass concoc...      1  train\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgJWnOLas55z",
        "outputId": "9d0ed88b-1de1-4f84-c04f-7373aee4cef4"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10000 non-null  object\n",
            " 1   label     10000 non-null  int64 \n",
            " 2   split     10000 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 234.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "QFzweIBHs550",
        "outputId": "65da168f-4b73-4ac9-b2b8-d0f1c7f12c52"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          5000   5000\n",
              "1          5000   5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjhWgUXvs552"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w2f4R1eYs553",
        "outputId": "ab678e81-4ffa-4a29-92b3-ec8d05272a48"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'smart and alert , thirteen conversations about one thing is a small gem .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkEuE6t7s553"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsdK5IGqs554"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUhk1ud4s555"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFxjawGRs556",
        "outputId": "d6180d4b-6f1a-43e8-81d2-f575d2ba406b"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
            "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
            "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
            "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
            "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HK2xCrKs558",
        "outputId": "b02c24f0-78af-4dc9-a522-a1b52fc0d6db"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "a 3\n",
            "and 4\n",
            "of 5\n",
            "to 6\n",
            "in 7\n",
            "is 8\n",
            "'s 9\n",
            "it 10\n",
            "21324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaIP_KARs559"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWlmjIgxs55-"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK9rAxMms55-"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8RBCTRhs55_",
        "outputId": "d5a7dde0-3853-4bb8-ff46-e6a26b507172"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               140544    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 440,673\n",
            "Trainable params: 440,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyLl9Yjfs56A"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Z1Yyj4s56B"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9GDadydns56B",
        "outputId": "9372c7ea-36ce-4442-9875-f8ee392572ea"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "282/282 [==============================] - 58s 149ms/step - loss: 0.4524 - accuracy: 0.7798 - val_loss: 0.2054 - val_accuracy: 0.9190\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 42s 150ms/step - loss: 0.0677 - accuracy: 0.9792 - val_loss: 0.2482 - val_accuracy: 0.9120\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 39s 139ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.3429 - val_accuracy: 0.9070\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.4184 - val_accuracy: 0.9110\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.0018 - accuracy: 0.9990 - val_loss: 0.5232 - val_accuracy: 0.8990\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 8.9157e-04 - accuracy: 0.9996 - val_loss: 0.3468 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.90000295639038\n",
            "Training 2: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 53s 143ms/step - loss: 0.4617 - accuracy: 0.7620 - val_loss: 0.2138 - val_accuracy: 0.9210\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 134ms/step - loss: 0.0530 - accuracy: 0.9824 - val_loss: 0.2431 - val_accuracy: 0.9110\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 37s 132ms/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.3538 - val_accuracy: 0.9100\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 133ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.3466 - val_accuracy: 0.9100\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.4132 - val_accuracy: 0.8970\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.5354 - val_accuracy: 0.8950\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.10000038146973\n",
            "Training 3: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 53s 145ms/step - loss: 0.4555 - accuracy: 0.7773 - val_loss: 0.2025 - val_accuracy: 0.9210\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 137ms/step - loss: 0.0623 - accuracy: 0.9785 - val_loss: 0.2036 - val_accuracy: 0.9180\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 38s 134ms/step - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.2442 - val_accuracy: 0.9250\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 134ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.3415 - val_accuracy: 0.9150\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 38s 133ms/step - loss: 7.8487e-04 - accuracy: 0.9998 - val_loss: 0.3079 - val_accuracy: 0.9170\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 38s 134ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3221 - val_accuracy: 0.9050\n",
            "Epoch 7/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.3831 - val_accuracy: 0.9130\n",
            "Epoch 8/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.4381 - val_accuracy: 0.9120\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00008: early stopping\n",
            "Test Accuracy: 92.5000011920929\n",
            "Training 4: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 54s 149ms/step - loss: 0.4564 - accuracy: 0.7700 - val_loss: 0.1872 - val_accuracy: 0.9240\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0574 - accuracy: 0.9822 - val_loss: 0.2301 - val_accuracy: 0.9170\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0124 - accuracy: 0.9975 - val_loss: 0.3875 - val_accuracy: 0.9070\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.3464 - val_accuracy: 0.9060\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.3648 - val_accuracy: 0.9010\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.3779 - val_accuracy: 0.8970\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.40000247955322\n",
            "Training 5: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 54s 148ms/step - loss: 0.4488 - accuracy: 0.7706 - val_loss: 0.2149 - val_accuracy: 0.9160\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0573 - accuracy: 0.9810 - val_loss: 0.2371 - val_accuracy: 0.9100\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.3791 - val_accuracy: 0.9000\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3223 - val_accuracy: 0.9040\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 8.4652e-04 - accuracy: 1.0000 - val_loss: 0.4995 - val_accuracy: 0.9060\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 2.0294e-04 - accuracy: 1.0000 - val_loss: 0.5578 - val_accuracy: 0.9000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.60000085830688\n",
            "Training 6: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 54s 149ms/step - loss: 0.4514 - accuracy: 0.7884 - val_loss: 0.1603 - val_accuracy: 0.9340\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0609 - accuracy: 0.9798 - val_loss: 0.1916 - val_accuracy: 0.9280\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 0.2295 - val_accuracy: 0.9260\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.3196 - val_accuracy: 0.9220\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 3.1037e-04 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9140\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 2.2742e-04 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.9140\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 93.4000015258789\n",
            "Training 7: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 54s 147ms/step - loss: 0.4532 - accuracy: 0.7802 - val_loss: 0.2059 - val_accuracy: 0.9210\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0689 - accuracy: 0.9757 - val_loss: 0.2211 - val_accuracy: 0.9190\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.2677 - val_accuracy: 0.9200\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.3278 - val_accuracy: 0.9170\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.3664 - val_accuracy: 0.9140\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 4.5215e-04 - accuracy: 1.0000 - val_loss: 0.4725 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.10000038146973\n",
            "Training 8: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 54s 150ms/step - loss: 0.4493 - accuracy: 0.7819 - val_loss: 0.1937 - val_accuracy: 0.9260\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0668 - accuracy: 0.9779 - val_loss: 0.1948 - val_accuracy: 0.9220\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0138 - accuracy: 0.9963 - val_loss: 0.3237 - val_accuracy: 0.9210\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.3260 - val_accuracy: 0.9130\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.4013 - val_accuracy: 0.9150\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 38s 135ms/step - loss: 6.6059e-04 - accuracy: 0.9999 - val_loss: 0.3969 - val_accuracy: 0.9010\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 92.59999990463257\n",
            "Training 9: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 54s 149ms/step - loss: 0.4631 - accuracy: 0.7548 - val_loss: 0.2117 - val_accuracy: 0.9110\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 41s 146ms/step - loss: 0.0613 - accuracy: 0.9777 - val_loss: 0.2590 - val_accuracy: 0.8990\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 40s 142ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.4775 - val_accuracy: 0.8960\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 40s 143ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.4438 - val_accuracy: 0.9020\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 48s 170ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.4691 - val_accuracy: 0.8970\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 6.6907e-04 - accuracy: 1.0000 - val_loss: 0.4628 - val_accuracy: 0.8960\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.10000133514404\n",
            "Training 10: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 60s 166ms/step - loss: 0.4568 - accuracy: 0.7694 - val_loss: 0.1840 - val_accuracy: 0.9240\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 41s 144ms/step - loss: 0.0586 - accuracy: 0.9810 - val_loss: 0.2150 - val_accuracy: 0.9130\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 41s 144ms/step - loss: 0.0143 - accuracy: 0.9966 - val_loss: 0.2631 - val_accuracy: 0.9120\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 38s 136ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.3009 - val_accuracy: 0.9120\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 41s 145ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3698 - val_accuracy: 0.9150\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 4.5446e-04 - accuracy: 1.0000 - val_loss: 0.3246 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.40000247955322\n",
            "\n",
            "        acc1  acc2       acc3       acc4       acc5       acc6  acc7  acc8  \\\n",
            "0  91.900003  92.1  92.500001  92.400002  91.600001  93.400002  92.1  92.6   \n",
            "\n",
            "        acc9      acc10        AVG  \n",
            "0  91.100001  92.400002  92.210001  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBdcqotMs56L"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckqA8T88s56S",
        "outputId": "3fc60121-6044-458a-ea40-c82994902804"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>91.900003</td>\n",
              "      <td>92.1</td>\n",
              "      <td>92.500001</td>\n",
              "      <td>92.400002</td>\n",
              "      <td>91.600001</td>\n",
              "      <td>93.400002</td>\n",
              "      <td>92.1</td>\n",
              "      <td>92.6</td>\n",
              "      <td>91.100001</td>\n",
              "      <td>92.400002</td>\n",
              "      <td>92.210001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1  acc2       acc3       acc4       acc5       acc6  acc7  acc8  \\\n",
              "0  91.900003  92.1  92.500001  92.400002  91.600001  93.400002  92.1  92.6   \n",
              "\n",
              "        acc9      acc10        AVG  \n",
              "0  91.100001  92.400002  92.210001  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEwcF1qYs56T"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('GRU_SUBJ.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-HGL9LPs56T"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqAZ6yCZs56U"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5qEpjHis56U"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s3QGGcEs56U"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Disertasi/WordEmbedding_Models/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbEWdqhcs56V",
        "outputId": "3bd47c5e-bfca-43b9-b722-64379cb9efdd"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0nEzhA4s56W"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7sC5babs56X"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIAiv1dqs56X",
        "outputId": "3d543664-5cd9-48c1-8c4b-b417535cad5e"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU0Jv4O-s56Y"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I65JjMWXs56Z",
        "outputId": "5c3f0671-3529-445f-c714-89a2dbe257d3"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpBM4ZMqs56Z"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig2obNb_s56a",
        "outputId": "f32937d3-12db-402d-9a3b-031977610c07"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9PuV6fls56b"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VXH2PAss56b"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r51-pxUMs56c",
        "outputId": "7ca8badc-625e-4d86-8640-dffaa92be696"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               140544    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 440,673\n",
            "Trainable params: 140,673\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EPTbAJvs56d"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1SxH7M5s56d"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-docWAFOs56e",
        "outputId": "f98fdc68-b40b-44fd-a133-fa258814912c"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 52s 160ms/step - loss: 0.4206 - accuracy: 0.8017 - val_loss: 0.2359 - val_accuracy: 0.9030\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.2109 - accuracy: 0.9151 - val_loss: 0.2219 - val_accuracy: 0.9140\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 42s 150ms/step - loss: 0.1811 - accuracy: 0.9268 - val_loss: 0.2231 - val_accuracy: 0.9180\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.1618 - accuracy: 0.9377 - val_loss: 0.2448 - val_accuracy: 0.9110\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.1440 - accuracy: 0.9468 - val_loss: 0.2267 - val_accuracy: 0.9140\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.1329 - accuracy: 0.9505 - val_loss: 0.2691 - val_accuracy: 0.9040\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.0897 - accuracy: 0.9657 - val_loss: 0.3063 - val_accuracy: 0.8910\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.0901 - accuracy: 0.9684 - val_loss: 0.2753 - val_accuracy: 0.9060\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.0617 - accuracy: 0.9816 - val_loss: 0.2966 - val_accuracy: 0.9040\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.0445 - accuracy: 0.9862 - val_loss: 0.3398 - val_accuracy: 0.9080\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.0383 - accuracy: 0.9864 - val_loss: 0.4169 - val_accuracy: 0.8980\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 91.79999828338623\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 50s 156ms/step - loss: 0.4106 - accuracy: 0.8052 - val_loss: 0.1992 - val_accuracy: 0.9240\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.2209 - accuracy: 0.9110 - val_loss: 0.1777 - val_accuracy: 0.9270\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.1756 - accuracy: 0.9310 - val_loss: 0.1912 - val_accuracy: 0.9220\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.1696 - accuracy: 0.9319 - val_loss: 0.1678 - val_accuracy: 0.9350\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.1509 - accuracy: 0.9440 - val_loss: 0.1885 - val_accuracy: 0.9270\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.1225 - accuracy: 0.9555 - val_loss: 0.1697 - val_accuracy: 0.9390\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.1048 - accuracy: 0.9600 - val_loss: 0.1751 - val_accuracy: 0.9350\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.0784 - accuracy: 0.9738 - val_loss: 0.1784 - val_accuracy: 0.9370\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 42s 149ms/step - loss: 0.0749 - accuracy: 0.9746 - val_loss: 0.2011 - val_accuracy: 0.9300\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.0627 - accuracy: 0.9771 - val_loss: 0.2360 - val_accuracy: 0.9240\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 42s 148ms/step - loss: 0.0399 - accuracy: 0.9881 - val_loss: 0.2378 - val_accuracy: 0.9310\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 44s 154ms/step - loss: 0.0212 - accuracy: 0.9941 - val_loss: 0.2663 - val_accuracy: 0.9280\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.0185 - accuracy: 0.9952 - val_loss: 0.2576 - val_accuracy: 0.9240\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.2472 - val_accuracy: 0.9300\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 93.90000104904175\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 54s 159ms/step - loss: 0.4123 - accuracy: 0.8042 - val_loss: 0.2492 - val_accuracy: 0.8940\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.2087 - accuracy: 0.9156 - val_loss: 0.2387 - val_accuracy: 0.9000\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.1840 - accuracy: 0.9302 - val_loss: 0.2563 - val_accuracy: 0.8970\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.1611 - accuracy: 0.9342 - val_loss: 0.2297 - val_accuracy: 0.9030\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.1403 - accuracy: 0.9468 - val_loss: 0.2361 - val_accuracy: 0.9070\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.1142 - accuracy: 0.9573 - val_loss: 0.2391 - val_accuracy: 0.9110\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.0968 - accuracy: 0.9615 - val_loss: 0.2656 - val_accuracy: 0.8990\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0763 - accuracy: 0.9732 - val_loss: 0.2733 - val_accuracy: 0.8960\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0584 - accuracy: 0.9800 - val_loss: 0.3694 - val_accuracy: 0.8810\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.0573 - accuracy: 0.9793 - val_loss: 0.3092 - val_accuracy: 0.8940\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.0334 - accuracy: 0.9899 - val_loss: 0.3598 - val_accuracy: 0.8980\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0223 - accuracy: 0.9945 - val_loss: 0.4080 - val_accuracy: 0.8910\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.0183 - accuracy: 0.9951 - val_loss: 0.3989 - val_accuracy: 0.8950\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.0095 - accuracy: 0.9988 - val_loss: 0.4539 - val_accuracy: 0.8920\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 91.10000133514404\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 52s 164ms/step - loss: 0.4222 - accuracy: 0.7992 - val_loss: 0.2503 - val_accuracy: 0.9070\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 45s 158ms/step - loss: 0.2032 - accuracy: 0.9218 - val_loss: 0.2383 - val_accuracy: 0.9060\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.1757 - accuracy: 0.9299 - val_loss: 0.2214 - val_accuracy: 0.9170\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.1641 - accuracy: 0.9374 - val_loss: 0.2189 - val_accuracy: 0.9220\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.1424 - accuracy: 0.9456 - val_loss: 0.2816 - val_accuracy: 0.9020\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.1145 - accuracy: 0.9564 - val_loss: 0.2328 - val_accuracy: 0.9120\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 44s 157ms/step - loss: 0.0955 - accuracy: 0.9666 - val_loss: 0.2560 - val_accuracy: 0.9080\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 45s 158ms/step - loss: 0.0752 - accuracy: 0.9736 - val_loss: 0.2630 - val_accuracy: 0.9060\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0556 - accuracy: 0.9816 - val_loss: 0.3231 - val_accuracy: 0.9020\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.0427 - accuracy: 0.9884 - val_loss: 0.3273 - val_accuracy: 0.9080\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.3682 - val_accuracy: 0.8940\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.0203 - accuracy: 0.9946 - val_loss: 0.4043 - val_accuracy: 0.9080\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 92.1999990940094\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 51s 160ms/step - loss: 0.4117 - accuracy: 0.8120 - val_loss: 0.2127 - val_accuracy: 0.9210\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 43s 151ms/step - loss: 0.2108 - accuracy: 0.9160 - val_loss: 0.2056 - val_accuracy: 0.9130\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.1830 - accuracy: 0.9286 - val_loss: 0.2244 - val_accuracy: 0.9120\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 43s 152ms/step - loss: 0.1750 - accuracy: 0.9322 - val_loss: 0.2020 - val_accuracy: 0.9220\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.1415 - accuracy: 0.9412 - val_loss: 0.2071 - val_accuracy: 0.9210\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.1231 - accuracy: 0.9542 - val_loss: 0.2570 - val_accuracy: 0.9070\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.1082 - accuracy: 0.9633 - val_loss: 0.2140 - val_accuracy: 0.9270\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0928 - accuracy: 0.9671 - val_loss: 0.2292 - val_accuracy: 0.9270\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0702 - accuracy: 0.9749 - val_loss: 0.2665 - val_accuracy: 0.9290\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0459 - accuracy: 0.9854 - val_loss: 0.2889 - val_accuracy: 0.9220\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0401 - accuracy: 0.9859 - val_loss: 0.2980 - val_accuracy: 0.9230\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 43s 153ms/step - loss: 0.0221 - accuracy: 0.9946 - val_loss: 0.3192 - val_accuracy: 0.9210\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 46s 164ms/step - loss: 0.0156 - accuracy: 0.9965 - val_loss: 0.3723 - val_accuracy: 0.9140\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 48s 171ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 0.3965 - val_accuracy: 0.9140\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 49s 173ms/step - loss: 0.0134 - accuracy: 0.9971 - val_loss: 0.3886 - val_accuracy: 0.9210\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 50s 178ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 0.3890 - val_accuracy: 0.9170\n",
            "Epoch 17/100\n",
            "282/282 [==============================] - 49s 174ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.4260 - val_accuracy: 0.9120\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00017: early stopping\n",
            "Test Accuracy: 92.90000200271606\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 55s 169ms/step - loss: 0.4283 - accuracy: 0.8001 - val_loss: 0.2812 - val_accuracy: 0.8820\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 44s 154ms/step - loss: 0.2062 - accuracy: 0.9165 - val_loss: 0.2555 - val_accuracy: 0.9010\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.1752 - accuracy: 0.9330 - val_loss: 0.2428 - val_accuracy: 0.9080\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 46s 164ms/step - loss: 0.1646 - accuracy: 0.9380 - val_loss: 0.2427 - val_accuracy: 0.9000\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 47s 165ms/step - loss: 0.1493 - accuracy: 0.9404 - val_loss: 0.2316 - val_accuracy: 0.9170\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 45s 160ms/step - loss: 0.1226 - accuracy: 0.9527 - val_loss: 0.2417 - val_accuracy: 0.9120\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 46s 163ms/step - loss: 0.1005 - accuracy: 0.9624 - val_loss: 0.2553 - val_accuracy: 0.9120\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 45s 160ms/step - loss: 0.0841 - accuracy: 0.9715 - val_loss: 0.2801 - val_accuracy: 0.9120\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.0718 - accuracy: 0.9737 - val_loss: 0.2874 - val_accuracy: 0.9100\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.0543 - accuracy: 0.9822 - val_loss: 0.2928 - val_accuracy: 0.9090\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0380 - accuracy: 0.9901 - val_loss: 0.3418 - val_accuracy: 0.9140\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0224 - accuracy: 0.9936 - val_loss: 0.3661 - val_accuracy: 0.9120\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.4543 - val_accuracy: 0.9100\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 91.69999957084656\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 54s 167ms/step - loss: 0.4208 - accuracy: 0.7972 - val_loss: 0.2237 - val_accuracy: 0.9030\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 44s 158ms/step - loss: 0.2041 - accuracy: 0.9208 - val_loss: 0.2775 - val_accuracy: 0.8860\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 47s 166ms/step - loss: 0.1905 - accuracy: 0.9256 - val_loss: 0.2155 - val_accuracy: 0.9110\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 50s 176ms/step - loss: 0.1560 - accuracy: 0.9383 - val_loss: 0.2153 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 48s 172ms/step - loss: 0.1468 - accuracy: 0.9469 - val_loss: 0.2286 - val_accuracy: 0.9010\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 47s 167ms/step - loss: 0.1320 - accuracy: 0.9516 - val_loss: 0.2194 - val_accuracy: 0.9050\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 49s 173ms/step - loss: 0.1068 - accuracy: 0.9626 - val_loss: 0.2491 - val_accuracy: 0.8980\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 51s 179ms/step - loss: 0.0744 - accuracy: 0.9745 - val_loss: 0.2365 - val_accuracy: 0.9060\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 48s 169ms/step - loss: 0.0593 - accuracy: 0.9817 - val_loss: 0.3040 - val_accuracy: 0.9040\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 47s 166ms/step - loss: 0.0523 - accuracy: 0.9841 - val_loss: 0.3073 - val_accuracy: 0.9100\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 46s 165ms/step - loss: 0.0379 - accuracy: 0.9864 - val_loss: 0.3370 - val_accuracy: 0.8990\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 46s 165ms/step - loss: 0.0247 - accuracy: 0.9933 - val_loss: 0.3406 - val_accuracy: 0.9090\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 91.20000004768372\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 47s 130ms/step - loss: 0.4184 - accuracy: 0.8040 - val_loss: 0.2167 - val_accuracy: 0.9050\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 0.2112 - accuracy: 0.9227 - val_loss: 0.2000 - val_accuracy: 0.9130\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.1856 - accuracy: 0.9262 - val_loss: 0.1935 - val_accuracy: 0.9190\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 36s 129ms/step - loss: 0.1587 - accuracy: 0.9356 - val_loss: 0.2040 - val_accuracy: 0.9090\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.1455 - accuracy: 0.9419 - val_loss: 0.1734 - val_accuracy: 0.9320\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 0.1290 - accuracy: 0.9525 - val_loss: 0.1968 - val_accuracy: 0.9260\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.1260 - accuracy: 0.9547 - val_loss: 0.1963 - val_accuracy: 0.9280\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.0897 - accuracy: 0.9705 - val_loss: 0.1964 - val_accuracy: 0.9190\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 34s 122ms/step - loss: 0.0683 - accuracy: 0.9772 - val_loss: 0.2126 - val_accuracy: 0.9280\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.0540 - accuracy: 0.9819 - val_loss: 0.2924 - val_accuracy: 0.9030\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 0.2687 - val_accuracy: 0.9130\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 35s 123ms/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 0.2828 - val_accuracy: 0.9260\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 35s 123ms/step - loss: 0.0277 - accuracy: 0.9903 - val_loss: 0.3145 - val_accuracy: 0.9210\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 93.19999814033508\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 55s 171ms/step - loss: 0.4067 - accuracy: 0.8254 - val_loss: 0.2820 - val_accuracy: 0.8760\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 46s 163ms/step - loss: 0.2024 - accuracy: 0.9167 - val_loss: 0.2822 - val_accuracy: 0.8950\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 45s 161ms/step - loss: 0.1781 - accuracy: 0.9281 - val_loss: 0.2385 - val_accuracy: 0.9130\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 46s 162ms/step - loss: 0.1595 - accuracy: 0.9379 - val_loss: 0.2590 - val_accuracy: 0.8990\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 46s 163ms/step - loss: 0.1432 - accuracy: 0.9449 - val_loss: 0.2475 - val_accuracy: 0.9140\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 45s 160ms/step - loss: 0.1238 - accuracy: 0.9516 - val_loss: 0.2482 - val_accuracy: 0.9190\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 46s 162ms/step - loss: 0.1095 - accuracy: 0.9587 - val_loss: 0.2635 - val_accuracy: 0.9200\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 46s 163ms/step - loss: 0.0788 - accuracy: 0.9731 - val_loss: 0.2900 - val_accuracy: 0.9240\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 46s 164ms/step - loss: 0.0674 - accuracy: 0.9786 - val_loss: 0.3093 - val_accuracy: 0.9210\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 45s 161ms/step - loss: 0.0472 - accuracy: 0.9839 - val_loss: 0.3315 - val_accuracy: 0.9190\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 46s 164ms/step - loss: 0.0396 - accuracy: 0.9882 - val_loss: 0.3972 - val_accuracy: 0.9180\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 47s 165ms/step - loss: 0.0286 - accuracy: 0.9924 - val_loss: 0.4379 - val_accuracy: 0.9040\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 46s 164ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.4546 - val_accuracy: 0.9020\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 46s 165ms/step - loss: 0.0164 - accuracy: 0.9958 - val_loss: 0.4624 - val_accuracy: 0.9120\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 47s 168ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.5139 - val_accuracy: 0.9140\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 46s 163ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.5056 - val_accuracy: 0.9090\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Test Accuracy: 92.40000247955322\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 57s 175ms/step - loss: 0.4180 - accuracy: 0.8087 - val_loss: 0.2413 - val_accuracy: 0.9060\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 46s 165ms/step - loss: 0.2138 - accuracy: 0.9152 - val_loss: 0.2084 - val_accuracy: 0.9160\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 47s 166ms/step - loss: 0.1903 - accuracy: 0.9250 - val_loss: 0.1953 - val_accuracy: 0.9180\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 47s 168ms/step - loss: 0.1624 - accuracy: 0.9348 - val_loss: 0.1915 - val_accuracy: 0.9170\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 47s 166ms/step - loss: 0.1503 - accuracy: 0.9429 - val_loss: 0.1896 - val_accuracy: 0.9210\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 47s 167ms/step - loss: 0.1400 - accuracy: 0.9457 - val_loss: 0.1963 - val_accuracy: 0.9230\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 47s 166ms/step - loss: 0.1084 - accuracy: 0.9597 - val_loss: 0.1892 - val_accuracy: 0.9220\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 47s 167ms/step - loss: 0.0908 - accuracy: 0.9675 - val_loss: 0.2365 - val_accuracy: 0.9140\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 48s 169ms/step - loss: 0.0729 - accuracy: 0.9774 - val_loss: 0.2408 - val_accuracy: 0.9110\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 49s 174ms/step - loss: 0.0444 - accuracy: 0.9855 - val_loss: 0.2745 - val_accuracy: 0.9050\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 51s 181ms/step - loss: 0.0357 - accuracy: 0.9873 - val_loss: 0.2444 - val_accuracy: 0.9180\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 50s 177ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 0.2986 - val_accuracy: 0.9080\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 52s 183ms/step - loss: 0.0217 - accuracy: 0.9940 - val_loss: 0.2759 - val_accuracy: 0.9170\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 47s 166ms/step - loss: 0.0123 - accuracy: 0.9973 - val_loss: 0.3750 - val_accuracy: 0.8990\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 92.29999780654907\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9      acc10    AVG\n",
            "0  91.799998  93.900001  91.100001  ...  92.400002  92.299998  92.27\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg7WGXk5s56g"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "R-7X0ul0s56h",
        "outputId": "076bef9a-d56e-488f-df2b-8020326f0a21"
      },
      "source": [
        "record2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>91.799998</td>\n",
              "      <td>93.900001</td>\n",
              "      <td>91.100001</td>\n",
              "      <td>92.199999</td>\n",
              "      <td>92.900002</td>\n",
              "      <td>91.7</td>\n",
              "      <td>91.2</td>\n",
              "      <td>93.199998</td>\n",
              "      <td>92.400002</td>\n",
              "      <td>92.299998</td>\n",
              "      <td>92.27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9      acc10    AVG\n",
              "0  91.799998  93.900001  91.100001  ...  92.400002  92.299998  92.27\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPE_hPus56j"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('GRU_SUBJ_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeFUb1Nqs56k"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1w-swpUs56m"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBbfwHQs56n"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIuR9P8xs56n"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3BvlEKIs56o",
        "outputId": "66571d89-282d-4a50-a693-023a92379e33"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 128)               140544    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 440,673\n",
            "Trainable params: 440,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxHBe_aJs56o"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1hoyINgs56o"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niRyl1j8s56p",
        "outputId": "b9aded37-1373-43a3-e02d-618adc630c4e"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record3)\n",
        "print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 83s 271ms/step - loss: 0.4025 - accuracy: 0.7955 - val_loss: 0.1517 - val_accuracy: 0.9410\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0867 - accuracy: 0.9679 - val_loss: 0.1337 - val_accuracy: 0.9520\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 73s 257ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.1785 - val_accuracy: 0.9340\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.2873 - val_accuracy: 0.9240\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2852 - val_accuracy: 0.9220\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 73s 259ms/step - loss: 4.0500e-04 - accuracy: 1.0000 - val_loss: 0.3729 - val_accuracy: 0.9150\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 1.2414e-04 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 0.9170\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 4.8444e-05 - accuracy: 1.0000 - val_loss: 0.3996 - val_accuracy: 0.9220\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 3.3964e-05 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9220\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 73s 259ms/step - loss: 2.6400e-05 - accuracy: 1.0000 - val_loss: 0.4155 - val_accuracy: 0.9230\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 95.20000219345093\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 85s 267ms/step - loss: 0.3894 - accuracy: 0.8191 - val_loss: 0.2098 - val_accuracy: 0.9210\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 72s 255ms/step - loss: 0.0783 - accuracy: 0.9715 - val_loss: 0.2365 - val_accuracy: 0.9160\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 0.0188 - accuracy: 0.9953 - val_loss: 0.2968 - val_accuracy: 0.9080\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 72s 256ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.4835 - val_accuracy: 0.9050\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 72s 256ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.4720 - val_accuracy: 0.9100\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 72s 256ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.5249 - val_accuracy: 0.9090\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 73s 259ms/step - loss: 2.1335e-04 - accuracy: 1.0000 - val_loss: 0.5614 - val_accuracy: 0.9110\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 6.1847e-05 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 0.9090\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 72s 255ms/step - loss: 3.2918e-05 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 0.9110\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 92.10000038146973\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 79s 258ms/step - loss: 0.3808 - accuracy: 0.8308 - val_loss: 0.1797 - val_accuracy: 0.9310\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 71s 252ms/step - loss: 0.0791 - accuracy: 0.9722 - val_loss: 0.2009 - val_accuracy: 0.9340\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 71s 252ms/step - loss: 0.0163 - accuracy: 0.9958 - val_loss: 0.2515 - val_accuracy: 0.9230\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 71s 251ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.3446 - val_accuracy: 0.9280\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 71s 251ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.3339 - val_accuracy: 0.9220\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 71s 251ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.3242 - val_accuracy: 0.9310\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 71s 252ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.4052 - val_accuracy: 0.9300\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 71s 251ms/step - loss: 9.9650e-05 - accuracy: 1.0000 - val_loss: 0.4302 - val_accuracy: 0.9280\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 71s 251ms/step - loss: 4.6656e-05 - accuracy: 1.0000 - val_loss: 0.4555 - val_accuracy: 0.9280\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 71s 252ms/step - loss: 3.8139e-05 - accuracy: 1.0000 - val_loss: 0.4743 - val_accuracy: 0.9280\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 93.4000015258789\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 64s 206ms/step - loss: 0.3812 - accuracy: 0.8129 - val_loss: 0.1922 - val_accuracy: 0.9250\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 57s 200ms/step - loss: 0.0827 - accuracy: 0.9707 - val_loss: 0.2052 - val_accuracy: 0.9130\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 57s 202ms/step - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.3024 - val_accuracy: 0.9110\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.3109 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 57s 202ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.3880 - val_accuracy: 0.9040\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 57s 202ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3827 - val_accuracy: 0.9070\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.4590 - val_accuracy: 0.9060\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.4388 - val_accuracy: 0.9080\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 3.4069e-04 - accuracy: 1.0000 - val_loss: 0.5075 - val_accuracy: 0.9030\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 92.5000011920929\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 83s 272ms/step - loss: 0.3882 - accuracy: 0.8231 - val_loss: 0.1955 - val_accuracy: 0.9160\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0857 - accuracy: 0.9722 - val_loss: 0.1903 - val_accuracy: 0.9370\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 74s 261ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.2206 - val_accuracy: 0.9280\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 74s 261ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.2883 - val_accuracy: 0.9190\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.3897 - val_accuracy: 0.9080\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.3275 - val_accuracy: 0.9300\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 73s 261ms/step - loss: 3.1453e-04 - accuracy: 1.0000 - val_loss: 0.3838 - val_accuracy: 0.9200\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 2.1352e-04 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.9130\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 74s 263ms/step - loss: 8.3139e-05 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.9190\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 75s 267ms/step - loss: 3.6346e-05 - accuracy: 1.0000 - val_loss: 0.4920 - val_accuracy: 0.9160\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 93.69999766349792\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 84s 275ms/step - loss: 0.3928 - accuracy: 0.8250 - val_loss: 0.1964 - val_accuracy: 0.9110\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 75s 265ms/step - loss: 0.0781 - accuracy: 0.9739 - val_loss: 0.2219 - val_accuracy: 0.9100\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 73s 261ms/step - loss: 0.0203 - accuracy: 0.9941 - val_loss: 0.2790 - val_accuracy: 0.9070\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 74s 262ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.2925 - val_accuracy: 0.9050\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.3281 - val_accuracy: 0.9110\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 75s 266ms/step - loss: 0.0063 - accuracy: 0.9974 - val_loss: 0.4646 - val_accuracy: 0.9030\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 76s 269ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.3895 - val_accuracy: 0.9050\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 76s 269ms/step - loss: 3.4272e-04 - accuracy: 1.0000 - val_loss: 0.5019 - val_accuracy: 0.9050\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 75s 266ms/step - loss: 6.0656e-05 - accuracy: 1.0000 - val_loss: 0.5522 - val_accuracy: 0.9050\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 91.10000133514404\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 95s 294ms/step - loss: 0.3946 - accuracy: 0.8125 - val_loss: 0.1990 - val_accuracy: 0.9220\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 77s 275ms/step - loss: 0.0819 - accuracy: 0.9713 - val_loss: 0.2268 - val_accuracy: 0.9240\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 79s 280ms/step - loss: 0.0194 - accuracy: 0.9957 - val_loss: 0.3210 - val_accuracy: 0.9150\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 78s 277ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.3504 - val_accuracy: 0.9160\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 77s 273ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.4103 - val_accuracy: 0.9200\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 78s 275ms/step - loss: 9.6612e-04 - accuracy: 0.9998 - val_loss: 0.5265 - val_accuracy: 0.9130\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 78s 277ms/step - loss: 1.4051e-04 - accuracy: 1.0000 - val_loss: 0.5631 - val_accuracy: 0.9120\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 77s 273ms/step - loss: 4.2647e-05 - accuracy: 1.0000 - val_loss: 0.5976 - val_accuracy: 0.9110\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 76s 270ms/step - loss: 3.0225e-05 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 0.9120\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 75s 266ms/step - loss: 1.9796e-05 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 92.40000247955322\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 81s 264ms/step - loss: 0.3937 - accuracy: 0.8156 - val_loss: 0.1653 - val_accuracy: 0.9350\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 72s 254ms/step - loss: 0.0839 - accuracy: 0.9701 - val_loss: 0.1730 - val_accuracy: 0.9400\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 72s 256ms/step - loss: 0.0139 - accuracy: 0.9964 - val_loss: 0.1931 - val_accuracy: 0.9350\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 72s 255ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.2965 - val_accuracy: 0.9310\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 71s 253ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.3438 - val_accuracy: 0.9250\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 71s 254ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.3151 - val_accuracy: 0.9210\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 72s 254ms/step - loss: 5.4270e-04 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9070\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 72s 255ms/step - loss: 2.7704e-04 - accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 0.9240\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 72s 255ms/step - loss: 8.7220e-05 - accuracy: 1.0000 - val_loss: 0.4161 - val_accuracy: 0.9300\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 72s 255ms/step - loss: 3.1441e-05 - accuracy: 1.0000 - val_loss: 0.4318 - val_accuracy: 0.9270\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 93.99999976158142\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 81s 263ms/step - loss: 0.3824 - accuracy: 0.8240 - val_loss: 0.2081 - val_accuracy: 0.9180\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 73s 257ms/step - loss: 0.0814 - accuracy: 0.9732 - val_loss: 0.2195 - val_accuracy: 0.9190\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 0.0206 - accuracy: 0.9944 - val_loss: 0.2637 - val_accuracy: 0.9120\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 72s 256ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 0.3368 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.3484 - val_accuracy: 0.9090\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 73s 259ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.4751 - val_accuracy: 0.9070\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 9.6317e-04 - accuracy: 0.9998 - val_loss: 0.4203 - val_accuracy: 0.9120\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.4328 - val_accuracy: 0.9050\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 4.4477e-04 - accuracy: 0.9999 - val_loss: 0.3411 - val_accuracy: 0.9100\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 6.5069e-04 - accuracy: 1.0000 - val_loss: 0.5395 - val_accuracy: 0.9010\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 91.90000295639038\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 81s 265ms/step - loss: 0.4036 - accuracy: 0.8064 - val_loss: 0.1788 - val_accuracy: 0.9280\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 73s 258ms/step - loss: 0.0786 - accuracy: 0.9731 - val_loss: 0.1995 - val_accuracy: 0.9280\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.2634 - val_accuracy: 0.9240\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.2934 - val_accuracy: 0.9270\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3179 - val_accuracy: 0.9290\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3377 - val_accuracy: 0.9200\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 76s 269ms/step - loss: 7.2997e-04 - accuracy: 1.0000 - val_loss: 0.4295 - val_accuracy: 0.9240\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 76s 271ms/step - loss: 8.8741e-05 - accuracy: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.9200\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 79s 280ms/step - loss: 2.7790e-05 - accuracy: 1.0000 - val_loss: 0.4722 - val_accuracy: 0.9190\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 75s 266ms/step - loss: 2.7296e-05 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 0.9200\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 74s 263ms/step - loss: 1.6877e-05 - accuracy: 1.0000 - val_loss: 0.5016 - val_accuracy: 0.9200\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 74s 264ms/step - loss: 1.2283e-05 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.9190\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 78s 277ms/step - loss: 1.1448e-05 - accuracy: 1.0000 - val_loss: 0.5200 - val_accuracy: 0.9180\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 92.90000200271606\n",
            "\n",
            "        acc1  acc2       acc3       acc4  ...  acc8       acc9      acc10        AVG\n",
            "0  95.200002  92.1  93.400002  92.500001  ...  94.0  91.900003  92.900002  92.920001\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_HXU3Es56p"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "DmatW9c_s56q",
        "outputId": "0c2b03fe-a8e2-43e6-fc97-25b91545afd6"
      },
      "source": [
        "record3"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>95.200002</td>\n",
              "      <td>92.1</td>\n",
              "      <td>93.400002</td>\n",
              "      <td>92.500001</td>\n",
              "      <td>93.699998</td>\n",
              "      <td>91.100001</td>\n",
              "      <td>92.400002</td>\n",
              "      <td>94.0</td>\n",
              "      <td>91.900003</td>\n",
              "      <td>92.900002</td>\n",
              "      <td>92.920001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1  acc2       acc3       acc4  ...  acc8       acc9      acc10        AVG\n",
              "0  95.200002  92.1  93.400002  92.500001  ...  94.0  91.900003  92.900002  92.920001\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5otSmiis56s"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('GRU_SUBJ_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}