{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classification with MPQA Dataset\n",
    "<hr>\n",
    "\n",
    "The __modus operandi__ for text classification is to use __word embedding__ for representing words and a Convolutional neural network to learn how to discriminate documents on classification problems. \n",
    "\n",
    "__Yoav Goldberg__ commented in _A Primer on Neural Network Models for Natural Language Processing, 2015._ :\n",
    "> _The non-linearity of the network, as well as the ability to easily integrate pre-trained\n",
    "word embeddings, often lead to superior classification accuracy._\n",
    "\n",
    "He also commented in _Neural Network Methods for Natural Language Processing, 2017_ :\n",
    "> ... _the CNN is in essence a feature-extracting architecture. ... . The CNNs layer's responsibility is to extract meaningful sub-structures that are useful for the overall prediction task at hand._\n",
    "\n",
    "We will build a text classification model using CNN model on the Customer Reviews Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "The CNN model is inspired by __Yoon Kim__ paper in his study on the use of Word Embedding + CNN for text classification. The hyperparameters we use based on his study are as follows:\n",
    "- Transfer function: rectified linear.\n",
    "- Kernel sizes: 1,2, 3, 4, 5.\n",
    "- Number of filters: 100.\n",
    "- Dropout rate: 0.5.\n",
    "- Weight regularization (L2) constraint: 3.\n",
    "- Batch Size: 50.\n",
    "- Update Rule: Adam\n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10606, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaining</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failing to support</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desperately needs</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many years of decay</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no quick fix</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>urged</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>hope</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10606 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sentence  label  split\n",
       "0              complaining      0  train\n",
       "1       failing to support      0  train\n",
       "2        desperately needs      0  train\n",
       "3      many years of decay      0  train\n",
       "4             no quick fix      0  train\n",
       "...                    ...    ...    ...\n",
       "10601                urged      1  train\n",
       "10602       strictly abide      1  train\n",
       "10603                 hope      1  train\n",
       "10604       strictly abide      1  train\n",
       "10605                           1  train\n",
       "\n",
       "[10606 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/SUBJ/SUBJ.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10606 non-null  object\n",
      " 1   label     10606 non-null  int32 \n",
      " 2   split     10606 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 207.3+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3312</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          7294   7294\n",
       "1          3312   3312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complaining'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  a very complicated process\n",
      "Into a sequence of int: [5, 44, 946, 581]\n",
      "Into a padded sequence: [  5  44 946 581   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "print(\"Example of sentence: \", sentences[8])\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[8])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "of 3\n",
      "to 4\n",
      "a 5\n",
      "and 6\n",
      "not 7\n",
      "is 8\n",
      "in 9\n",
      "be 10\n",
      "6236\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "A __standard model__ for document classification is to use (quoted from __Jason Brownlee__, the author of [machinelearningmastery.com](https://machinelearningmastery.com)):\n",
    ">- Word Embedding: A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation.\n",
    ">- Convolutional Model: A feature extraction model that learns to extract salient features from documents represented using a word embedding.\n",
    ">- Fully Connected Model: The interpretation of extracted features in terms of a predictive output.\n",
    "\n",
    "\n",
    "Therefore, the model is comprised of the following elements:\n",
    "- __Input layer__ that defines the length of input sequences.\n",
    "- __Embedding layer__ set to the size of the vocabulary and 100-dimensional real-valued representations.\n",
    "- __Conv1D layer__ with 32 filters and a kernel size set to the number of words to read at once.\n",
    "- __MaxPooling1D layer__ to consolidate the output from the convolutional layer.\n",
    "- __Flatten layer__ to reduce the three-dimensional output to two dimensional for concatenation.\n",
    "\n",
    "The CNN model is inspired by __Yoon Kim__ paper in his study on the use of Word Embedding + CNN for text classification. The hyperparameters we use based on his study are as follows:\n",
    "- Transfer function: rectified linear.\n",
    "- Kernel sizes: 3, 4, 5.\n",
    "- Number of filters: 100.\n",
    "- Dropout rate: 0.5.\n",
    "- Weight regularization (L2): 3.\n",
    "- Batch Size: 50.\n",
    "- Update Rule: Adam\n",
    "\n",
    "We will perform the best parameter using __grid search__ and 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "Now, we will build Convolutional Neural Network (CNN) models to classify encoded documents as either positive or negative.\n",
    "\n",
    "The model takes inspiration from `CNN for Sentence Classification` by *Yoon Kim*.\n",
    "\n",
    "Now, we will define our CNN model as follows:\n",
    "- One Conv layer with 100 filters, kernel size 5, and relu activation function;\n",
    "- One MaxPool layer with pool size = 2;\n",
    "- One Dropout layer after flattened;\n",
    "- Optimizer: Adam (The best learning algorithm so far)\n",
    "- Loss function: binary cross-entropy (suited for binary classification problem)\n",
    "\n",
    "**Note**: \n",
    "- The whole purpose of dropout layers is to tackle the problem of over-fitting and to introduce generalization to the model. Hence it is advisable to keep dropout parameter near 0.5 in hidden layers. \n",
    "- https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(filters = 100, kernel_size = 3, activation='relu', input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=filters, kernel_size = kernel_size, activation = activation, \n",
    "                               # set 'axis' value to the first and second axis of conv1D weights (rows, cols)\n",
    "                               kernel_constraint= MaxNorm( max_value=3, axis=[0,1])),\n",
    "        \n",
    "        tf.keras.layers.MaxPool1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=activation, \n",
    "                              # set axis to 0 to constrain each weight vector of length (input_dim,) in dense layer\n",
    "                              kernel_constraint = MaxNorm( max_value=3, axis=0)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          1870800   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 98, 100)           90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 49, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                49010     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,009,921\n",
      "Trainable params: 2,009,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "191/191 - 12s - loss: 0.5715 - accuracy: 0.7051 - val_loss: 0.4476 - val_accuracy: 0.8435\n",
      "Epoch 2/15\n",
      "191/191 - 6s - loss: 0.3333 - accuracy: 0.8698 - val_loss: 0.4152 - val_accuracy: 0.8549\n",
      "Epoch 3/15\n",
      "191/191 - 6s - loss: 0.2107 - accuracy: 0.9331 - val_loss: 0.4450 - val_accuracy: 0.8483\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1787 - accuracy: 0.9459 - val_loss: 0.5061 - val_accuracy: 0.8435\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1517 - accuracy: 0.9537 - val_loss: 0.5590 - val_accuracy: 0.8530\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1390 - accuracy: 0.9577 - val_loss: 0.5864 - val_accuracy: 0.8464\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1331 - accuracy: 0.9580 - val_loss: 0.6178 - val_accuracy: 0.8464\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.5902 - accuracy: 0.6865 - val_loss: 0.4577 - val_accuracy: 0.7936\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3521 - accuracy: 0.8749 - val_loss: 0.3611 - val_accuracy: 0.8379\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2231 - accuracy: 0.9219 - val_loss: 0.3746 - val_accuracy: 0.8285\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1833 - accuracy: 0.9350 - val_loss: 0.4187 - val_accuracy: 0.8238\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1524 - accuracy: 0.9402 - val_loss: 0.4697 - val_accuracy: 0.8586\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1359 - accuracy: 0.9480 - val_loss: 0.5402 - val_accuracy: 0.8181\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1269 - accuracy: 0.9542 - val_loss: 0.5797 - val_accuracy: 0.8464\n",
      "Epoch 8/15\n",
      "191/191 - 7s - loss: 0.1181 - accuracy: 0.9565 - val_loss: 0.6196 - val_accuracy: 0.8511\n",
      "Epoch 9/15\n",
      "191/191 - 7s - loss: 0.1055 - accuracy: 0.9595 - val_loss: 0.6530 - val_accuracy: 0.8501\n",
      "Epoch 10/15\n",
      "191/191 - 7s - loss: 0.0965 - accuracy: 0.9611 - val_loss: 0.7379 - val_accuracy: 0.8483\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 85.86239218711853\n",
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.6006 - accuracy: 0.6913 - val_loss: 0.4548 - val_accuracy: 0.8294\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3715 - accuracy: 0.8500 - val_loss: 0.3504 - val_accuracy: 0.8690\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2401 - accuracy: 0.9209 - val_loss: 0.3524 - val_accuracy: 0.8699\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1942 - accuracy: 0.9380 - val_loss: 0.3910 - val_accuracy: 0.8596\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1645 - accuracy: 0.9486 - val_loss: 0.4181 - val_accuracy: 0.8643\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1435 - accuracy: 0.9530 - val_loss: 0.5117 - val_accuracy: 0.8633\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1330 - accuracy: 0.9576 - val_loss: 0.5073 - val_accuracy: 0.8586\n",
      "Epoch 8/15\n",
      "191/191 - 7s - loss: 0.1239 - accuracy: 0.9581 - val_loss: 0.5570 - val_accuracy: 0.8558\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 86.99340224266052\n",
      "Epoch 1/15\n",
      "191/191 - 8s - loss: 0.5864 - accuracy: 0.7021 - val_loss: 0.4552 - val_accuracy: 0.8322\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3609 - accuracy: 0.8520 - val_loss: 0.3612 - val_accuracy: 0.8596\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2317 - accuracy: 0.9019 - val_loss: 0.4108 - val_accuracy: 0.8549\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1968 - accuracy: 0.9171 - val_loss: 0.4450 - val_accuracy: 0.8049\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1615 - accuracy: 0.9402 - val_loss: 0.5180 - val_accuracy: 0.8369\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1363 - accuracy: 0.9467 - val_loss: 0.5720 - val_accuracy: 0.8285\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1273 - accuracy: 0.9504 - val_loss: 0.6804 - val_accuracy: 0.8303\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.95664501190186\n",
      "Epoch 1/15\n",
      "191/191 - 8s - loss: 0.5907 - accuracy: 0.6916 - val_loss: 0.4614 - val_accuracy: 0.8322\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3579 - accuracy: 0.8634 - val_loss: 0.3542 - val_accuracy: 0.8662\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2229 - accuracy: 0.9261 - val_loss: 0.3848 - val_accuracy: 0.8633\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1815 - accuracy: 0.9392 - val_loss: 0.4518 - val_accuracy: 0.8558\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1553 - accuracy: 0.9463 - val_loss: 0.4702 - val_accuracy: 0.8624\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1362 - accuracy: 0.9542 - val_loss: 0.5335 - val_accuracy: 0.8539\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1279 - accuracy: 0.9556 - val_loss: 0.5444 - val_accuracy: 0.8520\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.6163969039917\n",
      "Epoch 1/15\n",
      "191/191 - 8s - loss: 0.5723 - accuracy: 0.6932 - val_loss: 0.4680 - val_accuracy: 0.8181\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3516 - accuracy: 0.8695 - val_loss: 0.4017 - val_accuracy: 0.8483\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2408 - accuracy: 0.9251 - val_loss: 0.4451 - val_accuracy: 0.8058\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1817 - accuracy: 0.9405 - val_loss: 0.5223 - val_accuracy: 0.8011\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1535 - accuracy: 0.9467 - val_loss: 0.5984 - val_accuracy: 0.8464\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1322 - accuracy: 0.9531 - val_loss: 0.6716 - val_accuracy: 0.8266\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1160 - accuracy: 0.9565 - val_loss: 0.7325 - val_accuracy: 0.8322\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 84.82563495635986\n",
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.5998 - accuracy: 0.7002 - val_loss: 0.4449 - val_accuracy: 0.8519\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3628 - accuracy: 0.8736 - val_loss: 0.3285 - val_accuracy: 0.8764\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2558 - accuracy: 0.9192 - val_loss: 0.3398 - val_accuracy: 0.8726\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.2188 - accuracy: 0.9348 - val_loss: 0.3757 - val_accuracy: 0.8679\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1923 - accuracy: 0.9371 - val_loss: 0.4261 - val_accuracy: 0.8689\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1674 - accuracy: 0.9514 - val_loss: 0.4506 - val_accuracy: 0.8651\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1523 - accuracy: 0.9545 - val_loss: 0.4553 - val_accuracy: 0.8689\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.64150738716125\n",
      "Epoch 1/15\n",
      "191/191 - 8s - loss: 0.5974 - accuracy: 0.6867 - val_loss: 0.4940 - val_accuracy: 0.8038\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3667 - accuracy: 0.8574 - val_loss: 0.4256 - val_accuracy: 0.8425\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2316 - accuracy: 0.9232 - val_loss: 0.4637 - val_accuracy: 0.8066\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1862 - accuracy: 0.9342 - val_loss: 0.5053 - val_accuracy: 0.8491\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1659 - accuracy: 0.9415 - val_loss: 0.5583 - val_accuracy: 0.8547\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1455 - accuracy: 0.9471 - val_loss: 0.5943 - val_accuracy: 0.8557\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1366 - accuracy: 0.9486 - val_loss: 0.6523 - val_accuracy: 0.8585\n",
      "Epoch 8/15\n",
      "191/191 - 7s - loss: 0.1226 - accuracy: 0.9522 - val_loss: 0.7147 - val_accuracy: 0.8566\n",
      "Epoch 9/15\n",
      "191/191 - 7s - loss: 0.1181 - accuracy: 0.9572 - val_loss: 0.7630 - val_accuracy: 0.8566\n",
      "Epoch 10/15\n",
      "191/191 - 7s - loss: 0.1002 - accuracy: 0.9646 - val_loss: 0.8988 - val_accuracy: 0.8557\n",
      "Epoch 11/15\n",
      "191/191 - 7s - loss: 0.0930 - accuracy: 0.9679 - val_loss: 0.9507 - val_accuracy: 0.8547\n",
      "Epoch 12/15\n",
      "191/191 - 7s - loss: 0.0911 - accuracy: 0.9702 - val_loss: 0.9647 - val_accuracy: 0.8500\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 85.84905862808228\n",
      "Epoch 1/15\n",
      "191/191 - 8s - loss: 0.5912 - accuracy: 0.6868 - val_loss: 0.4666 - val_accuracy: 0.7075\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.3653 - accuracy: 0.8474 - val_loss: 0.3229 - val_accuracy: 0.8802\n",
      "Epoch 3/15\n",
      "191/191 - 7s - loss: 0.2191 - accuracy: 0.9246 - val_loss: 0.3597 - val_accuracy: 0.8745\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.1797 - accuracy: 0.9343 - val_loss: 0.3671 - val_accuracy: 0.8755\n",
      "Epoch 5/15\n",
      "191/191 - 7s - loss: 0.1581 - accuracy: 0.9385 - val_loss: 0.3964 - val_accuracy: 0.8802\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1380 - accuracy: 0.9463 - val_loss: 0.4266 - val_accuracy: 0.8670\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1335 - accuracy: 0.9498 - val_loss: 0.4822 - val_accuracy: 0.8660\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.01887035369873\n",
      "Epoch 1/15\n",
      "191/191 - 8s - loss: 0.6219 - accuracy: 0.6873 - val_loss: 0.5207 - val_accuracy: 0.6745\n",
      "Epoch 2/15\n",
      "191/191 - 7s - loss: 0.4104 - accuracy: 0.8284 - val_loss: 0.3595 - val_accuracy: 0.8547\n",
      "Epoch 3/15\n",
      "191/191 - 11s - loss: 0.2541 - accuracy: 0.9301 - val_loss: 0.3622 - val_accuracy: 0.8651\n",
      "Epoch 4/15\n",
      "191/191 - 7s - loss: 0.2082 - accuracy: 0.9427 - val_loss: 0.4042 - val_accuracy: 0.8575\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1782 - accuracy: 0.9510 - val_loss: 0.4418 - val_accuracy: 0.8585\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1566 - accuracy: 0.9574 - val_loss: 0.5155 - val_accuracy: 0.8453\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1425 - accuracy: 0.9614 - val_loss: 0.5399 - val_accuracy: 0.8462\n",
      "Epoch 8/15\n",
      "191/191 - 8s - loss: 0.1327 - accuracy: 0.9616 - val_loss: 0.5727 - val_accuracy: 0.8443\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 86.50943636894226\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  85.485393  85.862392  86.993402  85.956645  86.616397   \n",
      "\n",
      "        acc6       acc7       acc8      acc9      acc10        AVG  \n",
      "0  84.825635  87.641507  85.849059  88.01887  86.509436  86.375874  \n",
      "\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.5788 - accuracy: 0.7081 - val_loss: 0.4396 - val_accuracy: 0.8388\n",
      "Epoch 2/15\n",
      "191/191 - 11s - loss: 0.3297 - accuracy: 0.8668 - val_loss: 0.3784 - val_accuracy: 0.8558\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2112 - accuracy: 0.9229 - val_loss: 0.4435 - val_accuracy: 0.8445\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1694 - accuracy: 0.9411 - val_loss: 0.4844 - val_accuracy: 0.8511\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1365 - accuracy: 0.9536 - val_loss: 0.5472 - val_accuracy: 0.8567\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1259 - accuracy: 0.9540 - val_loss: 0.5976 - val_accuracy: 0.8520\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.1088 - accuracy: 0.9618 - val_loss: 0.6975 - val_accuracy: 0.8388\n",
      "Epoch 8/15\n",
      "191/191 - 9s - loss: 0.0839 - accuracy: 0.9682 - val_loss: 0.8105 - val_accuracy: 0.8398\n",
      "Epoch 9/15\n",
      "191/191 - 10s - loss: 0.0831 - accuracy: 0.9653 - val_loss: 0.8046 - val_accuracy: 0.8332\n",
      "Epoch 10/15\n",
      "191/191 - 9s - loss: 0.0809 - accuracy: 0.9653 - val_loss: 0.8901 - val_accuracy: 0.8407\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 85.67389249801636\n",
      "Epoch 1/15\n",
      "191/191 - 10s - loss: 0.6006 - accuracy: 0.6950 - val_loss: 0.4648 - val_accuracy: 0.7747\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.3720 - accuracy: 0.8671 - val_loss: 0.3650 - val_accuracy: 0.8558\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2406 - accuracy: 0.9323 - val_loss: 0.3873 - val_accuracy: 0.8615\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1941 - accuracy: 0.9476 - val_loss: 0.4188 - val_accuracy: 0.8511\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1599 - accuracy: 0.9554 - val_loss: 0.4971 - val_accuracy: 0.7992\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1375 - accuracy: 0.9619 - val_loss: 0.5590 - val_accuracy: 0.8021\n",
      "Epoch 7/15\n",
      "191/191 - 9s - loss: 0.1276 - accuracy: 0.9650 - val_loss: 0.5822 - val_accuracy: 0.8087\n",
      "Epoch 8/15\n",
      "191/191 - 8s - loss: 0.1107 - accuracy: 0.9683 - val_loss: 0.6212 - val_accuracy: 0.8087\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 86.14514470100403\n",
      "Epoch 1/15\n",
      "191/191 - 10s - loss: 0.6274 - accuracy: 0.6823 - val_loss: 0.5062 - val_accuracy: 0.7003\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.4169 - accuracy: 0.8145 - val_loss: 0.3602 - val_accuracy: 0.8596\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2717 - accuracy: 0.9081 - val_loss: 0.3892 - val_accuracy: 0.8539\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.2129 - accuracy: 0.9405 - val_loss: 0.4236 - val_accuracy: 0.8520\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1766 - accuracy: 0.9532 - val_loss: 0.4651 - val_accuracy: 0.8511\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1551 - accuracy: 0.9583 - val_loss: 0.5653 - val_accuracy: 0.8530\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1362 - accuracy: 0.9627 - val_loss: 0.5658 - val_accuracy: 0.8473\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.95664501190186\n",
      "Epoch 1/15\n",
      "191/191 - 10s - loss: 0.5897 - accuracy: 0.6876 - val_loss: 0.4503 - val_accuracy: 0.8294\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3437 - accuracy: 0.8710 - val_loss: 0.3427 - val_accuracy: 0.8605\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2198 - accuracy: 0.9247 - val_loss: 0.3624 - val_accuracy: 0.8520\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1727 - accuracy: 0.9400 - val_loss: 0.4215 - val_accuracy: 0.8530\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1433 - accuracy: 0.9450 - val_loss: 0.4774 - val_accuracy: 0.8586\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1177 - accuracy: 0.9568 - val_loss: 0.5616 - val_accuracy: 0.8417\n",
      "Epoch 7/15\n",
      "191/191 - 7s - loss: 0.1077 - accuracy: 0.9592 - val_loss: 0.5934 - val_accuracy: 0.8577\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.05089783668518\n",
      "Epoch 1/15\n",
      "191/191 - 11s - loss: 0.6082 - accuracy: 0.6837 - val_loss: 0.4908 - val_accuracy: 0.6975\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3858 - accuracy: 0.8454 - val_loss: 0.3684 - val_accuracy: 0.8596\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2501 - accuracy: 0.9311 - val_loss: 0.3889 - val_accuracy: 0.8530\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.2031 - accuracy: 0.9435 - val_loss: 0.4415 - val_accuracy: 0.8558\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1636 - accuracy: 0.9541 - val_loss: 0.4940 - val_accuracy: 0.8454\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1469 - accuracy: 0.9596 - val_loss: 0.5626 - val_accuracy: 0.8492\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1334 - accuracy: 0.9629 - val_loss: 0.6028 - val_accuracy: 0.8473\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.95664501190186\n",
      "Epoch 1/15\n",
      "191/191 - 10s - loss: 0.6084 - accuracy: 0.6927 - val_loss: 0.4919 - val_accuracy: 0.8049\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3704 - accuracy: 0.8647 - val_loss: 0.4152 - val_accuracy: 0.8313\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2182 - accuracy: 0.9371 - val_loss: 0.4994 - val_accuracy: 0.8143\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1761 - accuracy: 0.9498 - val_loss: 0.5060 - val_accuracy: 0.8228\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1370 - accuracy: 0.9606 - val_loss: 0.6109 - val_accuracy: 0.8115\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1196 - accuracy: 0.9650 - val_loss: 0.6450 - val_accuracy: 0.8181\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1104 - accuracy: 0.9652 - val_loss: 0.7215 - val_accuracy: 0.8200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 83.12912583351135\n",
      "Epoch 1/15\n",
      "191/191 - 11s - loss: 0.6095 - accuracy: 0.6842 - val_loss: 0.4929 - val_accuracy: 0.6915\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.3848 - accuracy: 0.8610 - val_loss: 0.3724 - val_accuracy: 0.8594\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2407 - accuracy: 0.9359 - val_loss: 0.3934 - val_accuracy: 0.8415\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1915 - accuracy: 0.9502 - val_loss: 0.4306 - val_accuracy: 0.8472\n",
      "Epoch 5/15\n",
      "191/191 - 11s - loss: 0.1618 - accuracy: 0.9621 - val_loss: 0.4961 - val_accuracy: 0.8481\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1374 - accuracy: 0.9640 - val_loss: 0.5235 - val_accuracy: 0.8406\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1207 - accuracy: 0.9696 - val_loss: 0.6100 - val_accuracy: 0.8349\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.94339489936829\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.5867 - accuracy: 0.6943 - val_loss: 0.4158 - val_accuracy: 0.8500\n",
      "Epoch 2/15\n",
      "191/191 - 11s - loss: 0.3374 - accuracy: 0.8732 - val_loss: 0.3368 - val_accuracy: 0.8774\n",
      "Epoch 3/15\n",
      "191/191 - 12s - loss: 0.2047 - accuracy: 0.9369 - val_loss: 0.3930 - val_accuracy: 0.8538\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1634 - accuracy: 0.9524 - val_loss: 0.4497 - val_accuracy: 0.8443\n",
      "Epoch 5/15\n",
      "191/191 - 9s - loss: 0.1340 - accuracy: 0.9606 - val_loss: 0.4758 - val_accuracy: 0.8443\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1176 - accuracy: 0.9632 - val_loss: 0.5175 - val_accuracy: 0.8349\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.0997 - accuracy: 0.9695 - val_loss: 0.5796 - val_accuracy: 0.8377\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.73584961891174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.5795 - accuracy: 0.7132 - val_loss: 0.4173 - val_accuracy: 0.8519\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3426 - accuracy: 0.8757 - val_loss: 0.3596 - val_accuracy: 0.8708\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2087 - accuracy: 0.9321 - val_loss: 0.3973 - val_accuracy: 0.8575\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1678 - accuracy: 0.9463 - val_loss: 0.4674 - val_accuracy: 0.8594\n",
      "Epoch 5/15\n",
      "191/191 - 9s - loss: 0.1385 - accuracy: 0.9550 - val_loss: 0.5085 - val_accuracy: 0.8623\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1165 - accuracy: 0.9580 - val_loss: 0.5853 - val_accuracy: 0.8557\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1015 - accuracy: 0.9642 - val_loss: 0.6634 - val_accuracy: 0.8453\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.07547187805176\n",
      "Epoch 1/15\n",
      "191/191 - 11s - loss: 0.5824 - accuracy: 0.7064 - val_loss: 0.4451 - val_accuracy: 0.8274\n",
      "Epoch 2/15\n",
      "191/191 - 10s - loss: 0.3427 - accuracy: 0.8553 - val_loss: 0.3901 - val_accuracy: 0.8057\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2274 - accuracy: 0.9268 - val_loss: 0.4410 - val_accuracy: 0.8075\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1721 - accuracy: 0.9425 - val_loss: 0.4452 - val_accuracy: 0.8340\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1474 - accuracy: 0.9500 - val_loss: 0.5756 - val_accuracy: 0.8038\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1158 - accuracy: 0.9567 - val_loss: 0.5845 - val_accuracy: 0.8047\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.1065 - accuracy: 0.9613 - val_loss: 0.6572 - val_accuracy: 0.8274\n",
      "Epoch 8/15\n",
      "191/191 - 8s - loss: 0.0971 - accuracy: 0.9600 - val_loss: 0.6978 - val_accuracy: 0.7962\n",
      "Epoch 9/15\n",
      "191/191 - 7s - loss: 0.0817 - accuracy: 0.9677 - val_loss: 0.7584 - val_accuracy: 0.8330\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 83.39622616767883\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  85.485393  85.862392  86.993402  85.956645  86.616397   \n",
      "1       relu       2  85.673892  86.145145  85.956645  86.050898  85.956645   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  84.825635  87.641507  85.849059  88.018870  86.509436  86.375874  \n",
      "1  83.129126  85.943395  87.735850  87.075472  83.396226  85.706329  \n",
      "\n",
      "Epoch 1/15\n",
      "191/191 - 12s - loss: 0.5802 - accuracy: 0.7039 - val_loss: 0.4369 - val_accuracy: 0.8351\n",
      "Epoch 2/15\n",
      "191/191 - 10s - loss: 0.3345 - accuracy: 0.8684 - val_loss: 0.3568 - val_accuracy: 0.8633\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.1983 - accuracy: 0.9312 - val_loss: 0.4141 - val_accuracy: 0.8586\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1513 - accuracy: 0.9494 - val_loss: 0.4569 - val_accuracy: 0.8501\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1187 - accuracy: 0.9546 - val_loss: 0.5096 - val_accuracy: 0.8501\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1020 - accuracy: 0.9617 - val_loss: 0.6279 - val_accuracy: 0.8558\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.0934 - accuracy: 0.9647 - val_loss: 0.6475 - val_accuracy: 0.8388\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.33365035057068\n",
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.5980 - accuracy: 0.6926 - val_loss: 0.4877 - val_accuracy: 0.8087\n",
      "Epoch 2/15\n",
      "191/191 - 10s - loss: 0.3689 - accuracy: 0.8403 - val_loss: 0.4008 - val_accuracy: 0.8407\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2184 - accuracy: 0.9246 - val_loss: 0.4613 - val_accuracy: 0.8021\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1673 - accuracy: 0.9358 - val_loss: 0.5176 - val_accuracy: 0.7964\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1383 - accuracy: 0.9449 - val_loss: 0.5937 - val_accuracy: 0.7992\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1174 - accuracy: 0.9518 - val_loss: 0.6490 - val_accuracy: 0.7983\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.1047 - accuracy: 0.9563 - val_loss: 0.7148 - val_accuracy: 0.8322\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 84.0716302394867\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.5865 - accuracy: 0.7057 - val_loss: 0.4269 - val_accuracy: 0.8106\n",
      "Epoch 2/15\n",
      "191/191 - 10s - loss: 0.3352 - accuracy: 0.8754 - val_loss: 0.3695 - val_accuracy: 0.8586\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2095 - accuracy: 0.9273 - val_loss: 0.4094 - val_accuracy: 0.8407\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1545 - accuracy: 0.9523 - val_loss: 0.4972 - val_accuracy: 0.8464\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1209 - accuracy: 0.9579 - val_loss: 0.5892 - val_accuracy: 0.8049\n",
      "Epoch 6/15\n",
      "191/191 - 11s - loss: 0.1031 - accuracy: 0.9627 - val_loss: 0.6323 - val_accuracy: 0.8049\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.0951 - accuracy: 0.9639 - val_loss: 0.7201 - val_accuracy: 0.8077\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.86239218711853\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.5915 - accuracy: 0.6996 - val_loss: 0.4552 - val_accuracy: 0.8238\n",
      "Epoch 2/15\n",
      "191/191 - 10s - loss: 0.3520 - accuracy: 0.8749 - val_loss: 0.3861 - val_accuracy: 0.8445\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2058 - accuracy: 0.9394 - val_loss: 0.4159 - val_accuracy: 0.8483\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1589 - accuracy: 0.9560 - val_loss: 0.5021 - val_accuracy: 0.8369\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1268 - accuracy: 0.9629 - val_loss: 0.5883 - val_accuracy: 0.8388\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1012 - accuracy: 0.9672 - val_loss: 0.6719 - val_accuracy: 0.8360\n",
      "Epoch 7/15\n",
      "191/191 - 9s - loss: 0.0979 - accuracy: 0.9700 - val_loss: 0.7480 - val_accuracy: 0.8294\n",
      "Epoch 8/15\n",
      "191/191 - 9s - loss: 0.0838 - accuracy: 0.9733 - val_loss: 0.8513 - val_accuracy: 0.8275\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 84.82563495635986\n",
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.5869 - accuracy: 0.7001 - val_loss: 0.4220 - val_accuracy: 0.8313\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.3472 - accuracy: 0.8619 - val_loss: 0.3428 - val_accuracy: 0.8549\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2150 - accuracy: 0.9366 - val_loss: 0.3851 - val_accuracy: 0.8577\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1703 - accuracy: 0.9557 - val_loss: 0.4278 - val_accuracy: 0.8511\n",
      "Epoch 5/15\n",
      "191/191 - 11s - loss: 0.1389 - accuracy: 0.9630 - val_loss: 0.4818 - val_accuracy: 0.8492\n",
      "Epoch 6/15\n",
      "191/191 - 11s - loss: 0.1195 - accuracy: 0.9691 - val_loss: 0.5713 - val_accuracy: 0.8501\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.1028 - accuracy: 0.9722 - val_loss: 0.6232 - val_accuracy: 0.8511\n",
      "Epoch 8/15\n",
      "191/191 - 10s - loss: 0.0992 - accuracy: 0.9748 - val_loss: 0.6725 - val_accuracy: 0.8351\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 85.76814532279968\n",
      "Epoch 1/15\n",
      "191/191 - 12s - loss: 0.5789 - accuracy: 0.6931 - val_loss: 0.4863 - val_accuracy: 0.7851\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.3368 - accuracy: 0.8785 - val_loss: 0.4062 - val_accuracy: 0.8341\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2122 - accuracy: 0.9388 - val_loss: 0.4496 - val_accuracy: 0.7983\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1677 - accuracy: 0.9558 - val_loss: 0.5223 - val_accuracy: 0.8426\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1268 - accuracy: 0.9625 - val_loss: 0.6188 - val_accuracy: 0.8181\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1071 - accuracy: 0.9679 - val_loss: 0.6610 - val_accuracy: 0.8181\n",
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.0920 - accuracy: 0.9708 - val_loss: 0.7329 - val_accuracy: 0.7861\n",
      "Epoch 8/15\n",
      "191/191 - 9s - loss: 0.0829 - accuracy: 0.9729 - val_loss: 0.8213 - val_accuracy: 0.7879\n",
      "Epoch 9/15\n",
      "191/191 - 9s - loss: 0.0765 - accuracy: 0.9735 - val_loss: 0.8482 - val_accuracy: 0.8341\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 84.26012992858887\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.5916 - accuracy: 0.7009 - val_loss: 0.4394 - val_accuracy: 0.8406\n",
      "Epoch 2/15\n",
      "191/191 - 11s - loss: 0.3470 - accuracy: 0.8539 - val_loss: 0.3232 - val_accuracy: 0.8887\n",
      "Epoch 3/15\n",
      "191/191 - 8s - loss: 0.2062 - accuracy: 0.9364 - val_loss: 0.3490 - val_accuracy: 0.8849\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1550 - accuracy: 0.9511 - val_loss: 0.4110 - val_accuracy: 0.8764\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1292 - accuracy: 0.9607 - val_loss: 0.4583 - val_accuracy: 0.8783\n",
      "Epoch 6/15\n",
      "191/191 - 7s - loss: 0.1099 - accuracy: 0.9643 - val_loss: 0.5019 - val_accuracy: 0.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "191/191 - 8s - loss: 0.0950 - accuracy: 0.9697 - val_loss: 0.5684 - val_accuracy: 0.8736\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.86792659759521\n",
      "Epoch 1/15\n",
      "191/191 - 9s - loss: 0.5950 - accuracy: 0.6976 - val_loss: 0.4756 - val_accuracy: 0.8113\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3548 - accuracy: 0.8498 - val_loss: 0.4107 - val_accuracy: 0.8009\n",
      "Epoch 3/15\n",
      "191/191 - 11s - loss: 0.2177 - accuracy: 0.9279 - val_loss: 0.4689 - val_accuracy: 0.7943\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1593 - accuracy: 0.9470 - val_loss: 0.5767 - val_accuracy: 0.7840\n",
      "Epoch 5/15\n",
      "191/191 - 8s - loss: 0.1354 - accuracy: 0.9556 - val_loss: 0.6309 - val_accuracy: 0.7764\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1166 - accuracy: 0.9596 - val_loss: 0.6562 - val_accuracy: 0.7877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 81.13207817077637\n",
      "Epoch 1/15\n",
      "191/191 - 14s - loss: 0.6138 - accuracy: 0.6867 - val_loss: 0.4837 - val_accuracy: 0.8236\n",
      "Epoch 2/15\n",
      "191/191 - 12s - loss: 0.3757 - accuracy: 0.8672 - val_loss: 0.3631 - val_accuracy: 0.8538\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2353 - accuracy: 0.9383 - val_loss: 0.4075 - val_accuracy: 0.8500\n",
      "Epoch 4/15\n",
      "191/191 - 8s - loss: 0.1807 - accuracy: 0.9554 - val_loss: 0.4459 - val_accuracy: 0.8519\n",
      "Epoch 5/15\n",
      "191/191 - 9s - loss: 0.1462 - accuracy: 0.9635 - val_loss: 0.5068 - val_accuracy: 0.8330\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1279 - accuracy: 0.9669 - val_loss: 0.6303 - val_accuracy: 0.8302\n",
      "Epoch 7/15\n",
      "191/191 - 11s - loss: 0.1170 - accuracy: 0.9707 - val_loss: 0.6426 - val_accuracy: 0.8179\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.37735939025879\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.5937 - accuracy: 0.6889 - val_loss: 0.4536 - val_accuracy: 0.8415\n",
      "Epoch 2/15\n",
      "191/191 - 10s - loss: 0.3563 - accuracy: 0.8694 - val_loss: 0.3773 - val_accuracy: 0.8491\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2175 - accuracy: 0.9340 - val_loss: 0.4631 - val_accuracy: 0.8434\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1709 - accuracy: 0.9514 - val_loss: 0.5007 - val_accuracy: 0.8528\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1300 - accuracy: 0.9623 - val_loss: 0.5875 - val_accuracy: 0.8472\n",
      "Epoch 6/15\n",
      "191/191 - 8s - loss: 0.1098 - accuracy: 0.9676 - val_loss: 0.6774 - val_accuracy: 0.8472\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.0939 - accuracy: 0.9707 - val_loss: 0.6761 - val_accuracy: 0.8406\n",
      "Epoch 8/15\n",
      "191/191 - 8s - loss: 0.0864 - accuracy: 0.9735 - val_loss: 0.9145 - val_accuracy: 0.8415\n",
      "Epoch 9/15\n",
      "191/191 - 8s - loss: 0.0806 - accuracy: 0.9742 - val_loss: 0.9153 - val_accuracy: 0.8462\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 85.2830171585083\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  85.485393  85.862392  86.993402  85.956645  86.616397   \n",
      "1       relu       2  85.673892  86.145145  85.956645  86.050898  85.956645   \n",
      "2       relu       3  86.333650  84.071630  85.862392  84.825635  85.768145   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  84.825635  87.641507  85.849059  88.018870  86.509436  86.375874  \n",
      "1  83.129126  85.943395  87.735850  87.075472  83.396226  85.706329  \n",
      "2  84.260130  88.867927  81.132078  85.377359  85.283017  85.178196  \n",
      "\n",
      "Epoch 1/15\n",
      "191/191 - 14s - loss: 0.5923 - accuracy: 0.7009 - val_loss: 0.4535 - val_accuracy: 0.8285\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.3378 - accuracy: 0.8779 - val_loss: 0.3655 - val_accuracy: 0.8596\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2058 - accuracy: 0.9317 - val_loss: 0.4318 - val_accuracy: 0.8473\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1537 - accuracy: 0.9476 - val_loss: 0.4740 - val_accuracy: 0.8549\n",
      "Epoch 5/15\n",
      "191/191 - 9s - loss: 0.1199 - accuracy: 0.9587 - val_loss: 0.5201 - val_accuracy: 0.8652\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.0992 - accuracy: 0.9646 - val_loss: 0.6256 - val_accuracy: 0.8483\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.0876 - accuracy: 0.9677 - val_loss: 0.7261 - val_accuracy: 0.8492\n",
      "Epoch 8/15\n",
      "191/191 - 10s - loss: 0.0876 - accuracy: 0.9668 - val_loss: 0.7050 - val_accuracy: 0.8435\n",
      "Epoch 9/15\n",
      "191/191 - 8s - loss: 0.0782 - accuracy: 0.9669 - val_loss: 0.8056 - val_accuracy: 0.8162\n",
      "Epoch 10/15\n",
      "191/191 - 10s - loss: 0.0716 - accuracy: 0.9685 - val_loss: 0.8633 - val_accuracy: 0.8577\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 86.52215003967285\n",
      "Epoch 1/15\n",
      "191/191 - 12s - loss: 0.5906 - accuracy: 0.6888 - val_loss: 0.4773 - val_accuracy: 0.7587\n",
      "Epoch 2/15\n",
      "191/191 - 13s - loss: 0.3536 - accuracy: 0.8529 - val_loss: 0.3746 - val_accuracy: 0.8586\n",
      "Epoch 3/15\n",
      "191/191 - 12s - loss: 0.1996 - accuracy: 0.9326 - val_loss: 0.4534 - val_accuracy: 0.8539\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1557 - accuracy: 0.9500 - val_loss: 0.5173 - val_accuracy: 0.8530\n",
      "Epoch 5/15\n",
      "191/191 - 11s - loss: 0.1222 - accuracy: 0.9580 - val_loss: 0.6282 - val_accuracy: 0.8483\n",
      "Epoch 6/15\n",
      "191/191 - 10s - loss: 0.1005 - accuracy: 0.9611 - val_loss: 0.7144 - val_accuracy: 0.8435\n",
      "Epoch 7/15\n",
      "191/191 - 13s - loss: 0.0920 - accuracy: 0.9629 - val_loss: 0.7136 - val_accuracy: 0.8369\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 85.86239218711853\n",
      "Epoch 1/15\n",
      "191/191 - 14s - loss: 0.5971 - accuracy: 0.6982 - val_loss: 0.4304 - val_accuracy: 0.8238\n",
      "Epoch 2/15\n",
      "191/191 - 11s - loss: 0.3573 - accuracy: 0.8531 - val_loss: 0.3375 - val_accuracy: 0.8662\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2189 - accuracy: 0.9146 - val_loss: 0.3882 - val_accuracy: 0.8558\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1647 - accuracy: 0.9252 - val_loss: 0.4288 - val_accuracy: 0.8615\n",
      "Epoch 5/15\n",
      "191/191 - 9s - loss: 0.1300 - accuracy: 0.9519 - val_loss: 0.4700 - val_accuracy: 0.8615\n",
      "Epoch 6/15\n",
      "191/191 - 11s - loss: 0.1036 - accuracy: 0.9657 - val_loss: 0.5160 - val_accuracy: 0.8605\n",
      "Epoch 7/15\n",
      "191/191 - 11s - loss: 0.0910 - accuracy: 0.9654 - val_loss: 0.5745 - val_accuracy: 0.8605\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.6163969039917\n",
      "Epoch 1/15\n",
      "191/191 - 11s - loss: 0.5973 - accuracy: 0.6967 - val_loss: 0.4423 - val_accuracy: 0.8285\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3553 - accuracy: 0.8629 - val_loss: 0.3769 - val_accuracy: 0.8435\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2122 - accuracy: 0.9272 - val_loss: 0.3976 - val_accuracy: 0.8511\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1519 - accuracy: 0.9460 - val_loss: 0.4997 - val_accuracy: 0.8473\n",
      "Epoch 5/15\n",
      "191/191 - 13s - loss: 0.1162 - accuracy: 0.9619 - val_loss: 0.5473 - val_accuracy: 0.8398\n",
      "Epoch 6/15\n",
      "191/191 - 11s - loss: 0.1030 - accuracy: 0.9629 - val_loss: 0.6676 - val_accuracy: 0.8360\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.0875 - accuracy: 0.9662 - val_loss: 0.7185 - val_accuracy: 0.8313\n",
      "Epoch 8/15\n",
      "191/191 - 10s - loss: 0.0816 - accuracy: 0.9705 - val_loss: 0.7701 - val_accuracy: 0.8049\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 85.10838747024536\n",
      "Epoch 1/15\n",
      "191/191 - 15s - loss: 0.5886 - accuracy: 0.6999 - val_loss: 0.4788 - val_accuracy: 0.7484\n",
      "Epoch 2/15\n",
      "191/191 - 12s - loss: 0.3440 - accuracy: 0.8527 - val_loss: 0.3587 - val_accuracy: 0.8501\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2099 - accuracy: 0.9356 - val_loss: 0.4191 - val_accuracy: 0.8539\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.1505 - accuracy: 0.9500 - val_loss: 0.4560 - val_accuracy: 0.8473\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1242 - accuracy: 0.9599 - val_loss: 0.5681 - val_accuracy: 0.8435\n",
      "Epoch 6/15\n",
      "191/191 - 10s - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.5823 - val_accuracy: 0.8332\n",
      "Epoch 7/15\n",
      "191/191 - 9s - loss: 0.0881 - accuracy: 0.9633 - val_loss: 0.7290 - val_accuracy: 0.8379\n",
      "Epoch 8/15\n",
      "191/191 - 9s - loss: 0.0791 - accuracy: 0.9686 - val_loss: 0.7801 - val_accuracy: 0.8369\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 85.39113998413086\n",
      "Epoch 1/15\n",
      "191/191 - 10s - loss: 0.6049 - accuracy: 0.6915 - val_loss: 0.4407 - val_accuracy: 0.8369\n",
      "Epoch 2/15\n",
      "191/191 - 9s - loss: 0.3759 - accuracy: 0.8272 - val_loss: 0.3470 - val_accuracy: 0.8699\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 - 9s - loss: 0.2496 - accuracy: 0.8770 - val_loss: 0.3522 - val_accuracy: 0.8728\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.2137 - accuracy: 0.8983 - val_loss: 0.3822 - val_accuracy: 0.8586\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1672 - accuracy: 0.9252 - val_loss: 0.4353 - val_accuracy: 0.8605\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1339 - accuracy: 0.9417 - val_loss: 0.6284 - val_accuracy: 0.8351\n",
      "Epoch 7/15\n",
      "191/191 - 9s - loss: 0.1132 - accuracy: 0.9629 - val_loss: 0.6229 - val_accuracy: 0.8134\n",
      "Epoch 8/15\n",
      "191/191 - 9s - loss: 0.0980 - accuracy: 0.9661 - val_loss: 0.6948 - val_accuracy: 0.8407\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 87.27615475654602\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.6056 - accuracy: 0.6889 - val_loss: 0.5134 - val_accuracy: 0.6613\n",
      "Epoch 2/15\n",
      "191/191 - 11s - loss: 0.3700 - accuracy: 0.8467 - val_loss: 0.3830 - val_accuracy: 0.8462\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2254 - accuracy: 0.9298 - val_loss: 0.4095 - val_accuracy: 0.8453\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1575 - accuracy: 0.9555 - val_loss: 0.4939 - val_accuracy: 0.8443\n",
      "Epoch 5/15\n",
      "191/191 - 10s - loss: 0.1330 - accuracy: 0.9623 - val_loss: 0.5275 - val_accuracy: 0.8358\n",
      "Epoch 6/15\n",
      "191/191 - 9s - loss: 0.1153 - accuracy: 0.9680 - val_loss: 0.6259 - val_accuracy: 0.8292\n",
      "Epoch 7/15\n",
      "191/191 - 9s - loss: 0.0967 - accuracy: 0.9713 - val_loss: 0.7211 - val_accuracy: 0.7925\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 84.62263941764832\n",
      "Epoch 1/15\n",
      "191/191 - 10s - loss: 0.6124 - accuracy: 0.6872 - val_loss: 0.4590 - val_accuracy: 0.8264\n",
      "Epoch 2/15\n",
      "191/191 - 8s - loss: 0.3694 - accuracy: 0.8563 - val_loss: 0.3433 - val_accuracy: 0.8708\n",
      "Epoch 3/15\n",
      "191/191 - 9s - loss: 0.2270 - accuracy: 0.9299 - val_loss: 0.3749 - val_accuracy: 0.8689\n",
      "Epoch 4/15\n",
      "191/191 - 9s - loss: 0.1701 - accuracy: 0.9503 - val_loss: 0.4365 - val_accuracy: 0.8547\n",
      "Epoch 5/15\n",
      "191/191 - 9s - loss: 0.1398 - accuracy: 0.9579 - val_loss: 0.4915 - val_accuracy: 0.8528\n",
      "Epoch 6/15\n",
      "191/191 - 10s - loss: 0.1141 - accuracy: 0.9653 - val_loss: 0.6079 - val_accuracy: 0.8472\n",
      "Epoch 7/15\n",
      "191/191 - 10s - loss: 0.0986 - accuracy: 0.9714 - val_loss: 0.6591 - val_accuracy: 0.8462\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.07547187805176\n",
      "Epoch 1/15\n",
      "191/191 - 13s - loss: 0.6114 - accuracy: 0.6835 - val_loss: 0.4796 - val_accuracy: 0.6972\n",
      "Epoch 2/15\n",
      "191/191 - 11s - loss: 0.3878 - accuracy: 0.8484 - val_loss: 0.3681 - val_accuracy: 0.8443\n",
      "Epoch 3/15\n",
      "191/191 - 10s - loss: 0.2563 - accuracy: 0.9070 - val_loss: 0.3885 - val_accuracy: 0.8462\n",
      "Epoch 4/15\n",
      "191/191 - 10s - loss: 0.2054 - accuracy: 0.9358 - val_loss: 0.4429 - val_accuracy: 0.8425\n",
      "Epoch 5/15\n",
      "191/191 - 11s - loss: 0.1606 - accuracy: 0.9563 - val_loss: 0.5222 - val_accuracy: 0.8406\n",
      "Epoch 6/15\n",
      "191/191 - 11s - loss: 0.1322 - accuracy: 0.9645 - val_loss: 0.6175 - val_accuracy: 0.8425\n",
      "Epoch 7/15\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu', 'tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list = []\n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "\n",
    "            # Define the input shape\n",
    "            model = define_model(filters, kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(Xtrain, train_y, batch_size=50, epochs=15, verbose=2, \n",
    "                      callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "            print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "            acc_list.append(acc*100)\n",
    "            \n",
    "        mean_acc = np.array(acc_list).mean()\n",
    "        parameters = [activation, kernel_size]\n",
    "        entries = parameters + acc_list + [mean_acc]\n",
    "\n",
    "        temp = pd.DataFrame([entries], columns=columns)\n",
    "        record = record.append(temp, ignore_index=True)\n",
    "        print()\n",
    "        print(record)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "record.sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record[['Activation', 'AVG']].groupby(by='Activation').max().sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record.sort_values(by='AVG', ascending=False)\n",
    "report = report.to_excel('CNN_SUBJ.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4825 words present from 5100 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.7603335 ,  0.41298582,  1.6051669 , ...,  0.07348683,\n",
       "        -0.93163275, -0.64774868],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(filters = 100, kernel_size = 3, activation='relu', \n",
    "                 input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=filters, kernel_size = kernel_size, activation = activation, \n",
    "                               # set 'axis' value to the first and second axis of conv1D weights (rows, cols)\n",
    "                               kernel_constraint= MaxNorm( max_value=3, axis=[0,1])),\n",
    "        \n",
    "        tf.keras.layers.MaxPool1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=activation, \n",
    "                              # set axis to 0 to constrain each weight vector of length (input_dim,) in dense layer\n",
    "                              kernel_constraint = MaxNorm( max_value=3, axis=0)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1437\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1444 (Embedding)   (None, 100, 300)          1524600   \n",
      "_________________________________________________________________\n",
      "conv1d_1439 (Conv1D)         (None, 98, 100)           90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1439 (MaxPooli (None, 49, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1439 (Flatten)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2871 (Dropout)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_2869 (Dense)           (None, 10)                49010     \n",
      "_________________________________________________________________\n",
      "dropout_2872 (Dropout)       (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2870 (Dense)           (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,663,721\n",
      "Trainable params: 139,121\n",
      "Non-trainable params: 1,524,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(vocab_size, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 74.0740716457367\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 73.01587462425232\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 74.60317611694336\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 82.80423283576965\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 82.49337077140808\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 74.27055835723877\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 77.77777910232544\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 73.28042387962341\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Test Accuracy: 73.28042387962341\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 75.59681534767151\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 77.18833088874817\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 73.209547996521\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 82.49337077140808\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 75.86206793785095\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 76.1904776096344\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 73.8095223903656\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 76.4550268650055\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 73.01587462425232\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 74.86772537231445\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 73.209547996521\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 76.92307829856873\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.18833088874817\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 78.77984046936035\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.39682388305664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 79.89417910575867\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 74.60317611694336\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 72.75132536888123\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Test Accuracy: 73.8095223903656\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 78.77984046936035\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 73.74005317687988\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 72.22222089767456\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 71.16402387619019\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 76.98412537574768\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 72.67904281616211\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.45358347892761\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 74.86772537231445\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 77.77777910232544\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 66.1375641822815\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 66.1375641822815\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 75.39682388305664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 61.00795865058899\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 76.92307829856873\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.59681534767151\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "Test Accuracy: 69.49602365493774\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 63.49206566810608\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 63.22751045227051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 63.22751045227051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 70.82228064537048\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 68.43501329421997\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 77.71883010864258\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 62.59946823120117\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
      "6       relu       7  79.365081  63.492066  63.227510  77.248675  63.227510   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
      "6  70.822281  68.435013  77.718830  62.599468  75.066310  70.120274  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 76.1904776096344\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 71.69312238693237\n",
      "Test Accuracy: 72.48677015304565\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 65.6084656715393\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 70.37037014961243\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 70.82228064537048\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 72.41379022598267\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 63.92573118209839\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
      "6       relu       7  79.365081  63.492066  63.227510  77.248675  63.227510   \n",
      "7       relu       8  76.190478  71.693122  72.486770  65.608466  70.370370   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
      "6  70.822281  68.435013  77.718830  62.599468  75.066310  70.120274  \n",
      "7  75.066310  70.822281  72.413790  63.925731  77.984083  71.656140  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list = []\n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "            \n",
    "            # Define the input shape\n",
    "            model = define_model_2(filters, kernel_size, activation, input_dim=vocab_size, \n",
    "                                 max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(Xtrain, train_y, batch_size=50, epochs=30, verbose=0, \n",
    "                      callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "            print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "            acc_list.append(acc*100)\n",
    "            \n",
    "        mean_acc = np.array(acc_list).mean()\n",
    "        parameters = [activation, kernel_size]\n",
    "        entries = parameters + acc_list + [mean_acc]\n",
    "\n",
    "        temp = pd.DataFrame([entries], columns=columns)\n",
    "        record2 = record2.append(temp, ignore_index=True)\n",
    "        print()\n",
    "        print(record2)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>Filters</th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>74.074072</td>\n",
       "      <td>73.015875</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>74.603176</td>\n",
       "      <td>82.804233</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>82.493371</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>74.270558</td>\n",
       "      <td>77.670414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>77.777779</td>\n",
       "      <td>73.280424</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>73.280424</td>\n",
       "      <td>75.596815</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>73.209548</td>\n",
       "      <td>82.493371</td>\n",
       "      <td>75.862068</td>\n",
       "      <td>77.165172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>71.164024</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>76.984125</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>72.679043</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>77.453583</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>76.450114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>75.396824</td>\n",
       "      <td>79.894179</td>\n",
       "      <td>74.603176</td>\n",
       "      <td>72.751325</td>\n",
       "      <td>73.809522</td>\n",
       "      <td>78.779840</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>73.740053</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>75.735688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>76.190478</td>\n",
       "      <td>73.809522</td>\n",
       "      <td>76.455027</td>\n",
       "      <td>73.015875</td>\n",
       "      <td>74.867725</td>\n",
       "      <td>73.209548</td>\n",
       "      <td>76.923078</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>78.779840</td>\n",
       "      <td>75.577099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>6</td>\n",
       "      <td>74.867725</td>\n",
       "      <td>77.777779</td>\n",
       "      <td>66.137564</td>\n",
       "      <td>66.137564</td>\n",
       "      <td>75.396824</td>\n",
       "      <td>61.007959</td>\n",
       "      <td>76.923078</td>\n",
       "      <td>75.596815</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>69.496024</td>\n",
       "      <td>72.159067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>8</td>\n",
       "      <td>76.190478</td>\n",
       "      <td>71.693122</td>\n",
       "      <td>72.486770</td>\n",
       "      <td>65.608466</td>\n",
       "      <td>70.370370</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>70.822281</td>\n",
       "      <td>72.413790</td>\n",
       "      <td>63.925731</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>71.656140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>7</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>63.492066</td>\n",
       "      <td>63.227510</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>63.227510</td>\n",
       "      <td>70.822281</td>\n",
       "      <td>68.435013</td>\n",
       "      <td>77.718830</td>\n",
       "      <td>62.599468</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>70.120274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
       "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
       "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
       "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
       "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
       "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
       "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
       "7       relu       8  76.190478  71.693122  72.486770  65.608466  70.370370   \n",
       "6       relu       7  79.365081  63.492066  63.227510  77.248675  63.227510   \n",
       "\n",
       "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
       "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
       "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
       "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
       "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
       "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
       "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
       "7  75.066310  70.822281  72.413790  63.925731  77.984083  71.656140  \n",
       "6  70.822281  68.435013  77.718830  62.599468  75.066310  70.120274  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2.sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>77.670414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AVG\n",
       "Activation           \n",
       "relu        77.670414"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2[['Activation', 'AVG']].groupby(by='Activation').max().sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2.sort_values(by='AVG', ascending=False)\n",
    "report = report.to_excel('CNN_SUBJ_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(filters = 100, kernel_size = 3, activation='relu', \n",
    "                 input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=filters, kernel_size = kernel_size, activation = activation, \n",
    "                               # set 'axis' value to the first and second axis of conv1D weights (rows, cols)\n",
    "                               kernel_constraint= MaxNorm( max_value=3, axis=[0,1])),\n",
    "        \n",
    "        tf.keras.layers.MaxPool1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=activation, \n",
    "                              # set axis to 0 to constrain each weight vector of length (input_dim,) in dense layer\n",
    "                              kernel_constraint = MaxNorm( max_value=3, axis=0)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1518\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1525 (Embedding)   (None, 100, 300)          1527300   \n",
      "_________________________________________________________________\n",
      "conv1d_1520 (Conv1D)         (None, 98, 100)           90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1520 (MaxPooli (None, 49, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1520 (Flatten)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3033 (Dropout)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_3031 (Dense)           (None, 10)                49010     \n",
      "_________________________________________________________________\n",
      "dropout_3034 (Dropout)       (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3032 (Dense)           (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,666,421\n",
      "Trainable params: 1,666,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(vocab_size, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.42327761650085\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.51322984695435\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 83.59788656234741\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 74.80106353759766\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 79.84085083007812\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3      acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.51323  83.597887   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 81.7460298538208\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Test Accuracy: 79.10053133964539\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 82.22811818122864\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 76.71957612037659\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 81.7460298538208\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 80.95238208770752\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 76.71957612037659\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 77.18833088874817\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.95238208770752\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 79.0450930595398\n",
      "Test Accuracy: 79.0450930595398\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.16710782051086\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 64.55026268959045\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 78.30687761306763\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 76.65782570838928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Test Accuracy: 80.63660264015198\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 82.75862336158752\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 78.30687761306763\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 78.83597612380981\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.37135004997253\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 83.02386999130249\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 81.9628655910492\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
      "\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 79.10053133964539\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 74.27055835723877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Test Accuracy: 76.1273205280304\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
      "6       relu       7  80.158728  80.158728  80.687833  79.100531  80.158728   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
      "6  75.331563  74.270558  79.310346  79.575598  76.127321  78.487993  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 76.4550268650055\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 82.27513432502747\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.57142686843872\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 76.65782570838928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 79.0450930595398\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
      "6       relu       7  80.158728  80.158728  80.687833  79.100531  80.158728   \n",
      "7       relu       8  78.042328  76.455027  82.275134  78.571427  78.042328   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
      "6  75.331563  74.270558  79.310346  79.575598  76.127321  78.487993  \n",
      "7  78.249335  78.249335  81.432360  76.657826  79.045093  78.702019  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list = []\n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "            \n",
    "            # Define the input shape\n",
    "            model = define_model_3(filters, kernel_size, activation, input_dim=vocab_size, \n",
    "                                 max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(Xtrain, train_y, batch_size=50, epochs=20, verbose=0, \n",
    "                      callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "            print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "            acc_list.append(acc*100)\n",
    "            \n",
    "        mean_acc = np.array(acc_list).mean()\n",
    "        parameters = [activation, kernel_size]\n",
    "        entries = parameters + acc_list + [mean_acc]\n",
    "\n",
    "        temp = pd.DataFrame([entries], columns=columns)\n",
    "        record3 = record3.append(temp, ignore_index=True)\n",
    "        print()\n",
    "        print(record3)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>Filters</th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>81.746030</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>82.228118</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>80.715760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>6</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>78.835976</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>80.371350</td>\n",
       "      <td>83.023870</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>81.962866</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.239918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>80.952382</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>79.920706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>77.513230</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>74.801064</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>79.627525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>76.719576</td>\n",
       "      <td>81.746030</td>\n",
       "      <td>80.952382</td>\n",
       "      <td>76.719576</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>79.496723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>8</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>76.455027</td>\n",
       "      <td>82.275134</td>\n",
       "      <td>78.571427</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>78.702019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>7</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>74.270558</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>76.127321</td>\n",
       "      <td>78.487993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>64.550263</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>82.758623</td>\n",
       "      <td>78.041345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
       "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
       "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
       "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
       "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
       "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
       "7       relu       8  78.042328  76.455027  82.275134  78.571427  78.042328   \n",
       "6       relu       7  80.158728  80.158728  80.687833  79.100531  80.158728   \n",
       "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
       "\n",
       "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
       "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
       "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
       "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
       "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
       "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
       "7  78.249335  78.249335  81.432360  76.657826  79.045093  78.702019  \n",
       "6  75.331563  74.270558  79.310346  79.575598  76.127321  78.487993  \n",
       "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3.sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3.sort_values(by='AVG', ascending=False)\n",
    "report = report.to_excel('CNN_SUBJ_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
