{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LSTM_MPQA_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rqSplUob9K9M",
        "hlTDK7Bu9K9O"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSQpltfQ9K83"
      },
      "source": [
        "# LSTM Classification with MPQA Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using LSTM model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SfXEy1P9Qec",
        "outputId": "12fe7553-8d98-459a-b97f-9ee89efddc1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDPpNv9B9K9B",
        "outputId": "e019e396-a051-4850-8b81-f3baa1582b83"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkQK4Xab9K9D",
        "outputId": "7f554dbf-976d-44a3-dfcb-f054971c8beb"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6MNst_X9K9F"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "4Gsc-Xay9K9F",
        "outputId": "5fccc4b2-65d3-487f-92da-080d2329e83f"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Google Colab/0_data/MPQA/MPQA.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10606, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>complaining</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>failing to support</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>desperately needs</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>many years of decay</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no quick fix</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10601</th>\n",
              "      <td>urged</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10602</th>\n",
              "      <td>strictly abide</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10603</th>\n",
              "      <td>hope</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10604</th>\n",
              "      <td>strictly abide</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10605</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10606 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  sentence  label  split\n",
              "0              complaining      0  train\n",
              "1       failing to support      0  train\n",
              "2        desperately needs      0  train\n",
              "3      many years of decay      0  train\n",
              "4             no quick fix      0  train\n",
              "...                    ...    ...    ...\n",
              "10601                urged      1  train\n",
              "10602       strictly abide      1  train\n",
              "10603                 hope      1  train\n",
              "10604       strictly abide      1  train\n",
              "10605                           1  train\n",
              "\n",
              "[10606 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zACSP2Su9K9G",
        "outputId": "54390d8b-251a-4cb7-b743-3f42504f1ffa"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10606 entries, 0 to 10605\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10606 non-null  object\n",
            " 1   label     10606 non-null  int64 \n",
            " 2   split     10606 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 248.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "48-yE5rQ9K9H",
        "outputId": "7766d41f-633e-49a7-ad57-b9e3486027d9"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7294</td>\n",
              "      <td>7294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3312</td>\n",
              "      <td>3312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          7294   7294\n",
              "1          3312   3312"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Ow5cSM9K9H"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1Gz27SPD9K9I",
        "outputId": "782843d7-366e-4cc3-b35c-cb6ee92eb4cf"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'complaining'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sjZFfl39K9J"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqrcC7fh9K9J"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW7xgNUN9K9K"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfWzPu9w9K9K",
        "outputId": "df75b1d1-2ef6-42e1-e053-2a58e60a1843"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  no quick fix\n",
            "Into a sequence of int: [25, 945, 1476]\n",
            "Into a padded sequence: [  25  945 1476    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTiYgy8w9K9L",
        "outputId": "c34f850b-b85b-4a25-a32f-e3c6c71c24ac"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "of 3\n",
            "to 4\n",
            "a 5\n",
            "and 6\n",
            "not 7\n",
            "is 8\n",
            "in 9\n",
            "be 10\n",
            "6236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAWkKZD29K9M"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqSplUob9K9M"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKSgkVVx9K9N"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuJSw1iH9K9N",
        "outputId": "e5bcff66-d61d-47c7-8635-bb5dcb0b1601"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 585,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv70FFtv9K9O"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlTDK7Bu9K9O"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "miWbRl5z9K9P",
        "outputId": "47a282d5-96c4-4e99-f826-3d2a44c44647"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 109s 290ms/step - loss: 0.5525 - accuracy: 0.7437 - val_loss: 0.3866 - val_accuracy: 0.8303\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 72s 242ms/step - loss: 0.1998 - accuracy: 0.9338 - val_loss: 0.3882 - val_accuracy: 0.8426\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 70s 233ms/step - loss: 0.1369 - accuracy: 0.9524 - val_loss: 0.4199 - val_accuracy: 0.8445\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 68s 227ms/step - loss: 0.1041 - accuracy: 0.9646 - val_loss: 0.5014 - val_accuracy: 0.8407\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 70s 234ms/step - loss: 0.0834 - accuracy: 0.9716 - val_loss: 0.5926 - val_accuracy: 0.8435\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 69s 230ms/step - loss: 0.0602 - accuracy: 0.9757 - val_loss: 0.6757 - val_accuracy: 0.8398\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 69s 232ms/step - loss: 0.0481 - accuracy: 0.9804 - val_loss: 0.7265 - val_accuracy: 0.8426\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 69s 232ms/step - loss: 0.0423 - accuracy: 0.9816 - val_loss: 0.7143 - val_accuracy: 0.8379\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 69s 232ms/step - loss: 0.0347 - accuracy: 0.9853 - val_loss: 0.8435 - val_accuracy: 0.8313\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 69s 230ms/step - loss: 0.0388 - accuracy: 0.9818 - val_loss: 0.8383 - val_accuracy: 0.8417\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 69s 230ms/step - loss: 0.0336 - accuracy: 0.9844 - val_loss: 0.8901 - val_accuracy: 0.8256\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 69s 230ms/step - loss: 0.0339 - accuracy: 0.9863 - val_loss: 0.9577 - val_accuracy: 0.8351\n",
            "Epoch 13/40\n",
            "299/299 [==============================] - 69s 231ms/step - loss: 0.0331 - accuracy: 0.9847 - val_loss: 0.8530 - val_accuracy: 0.8266\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 84.44863557815552\n",
            "Training 2: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 116s 310ms/step - loss: 0.5624 - accuracy: 0.7339 - val_loss: 0.3442 - val_accuracy: 0.8615\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 83s 277ms/step - loss: 0.2135 - accuracy: 0.9270 - val_loss: 0.3603 - val_accuracy: 0.8558\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 83s 277ms/step - loss: 0.1447 - accuracy: 0.9488 - val_loss: 0.3905 - val_accuracy: 0.8520\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 82s 276ms/step - loss: 0.1108 - accuracy: 0.9612 - val_loss: 0.4084 - val_accuracy: 0.8520\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 83s 278ms/step - loss: 0.0884 - accuracy: 0.9699 - val_loss: 0.4632 - val_accuracy: 0.8558\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 83s 279ms/step - loss: 0.0646 - accuracy: 0.9760 - val_loss: 0.5043 - val_accuracy: 0.8615\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 83s 278ms/step - loss: 0.0501 - accuracy: 0.9788 - val_loss: 0.6092 - val_accuracy: 0.8492\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 83s 278ms/step - loss: 0.0489 - accuracy: 0.9787 - val_loss: 0.7052 - val_accuracy: 0.8473\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 83s 278ms/step - loss: 0.0347 - accuracy: 0.9844 - val_loss: 0.8098 - val_accuracy: 0.8483\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 83s 277ms/step - loss: 0.0297 - accuracy: 0.9862 - val_loss: 0.8112 - val_accuracy: 0.8435\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 83s 276ms/step - loss: 0.0323 - accuracy: 0.9846 - val_loss: 0.8015 - val_accuracy: 0.8435\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 86.14514470100403\n",
            "Training 3: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 139s 385ms/step - loss: 0.5678 - accuracy: 0.7297 - val_loss: 0.3558 - val_accuracy: 0.8624\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 108s 362ms/step - loss: 0.1996 - accuracy: 0.9312 - val_loss: 0.3628 - val_accuracy: 0.8680\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 108s 362ms/step - loss: 0.1335 - accuracy: 0.9553 - val_loss: 0.4235 - val_accuracy: 0.8426\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 108s 362ms/step - loss: 0.1175 - accuracy: 0.9589 - val_loss: 0.4760 - val_accuracy: 0.8426\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 108s 361ms/step - loss: 0.0828 - accuracy: 0.9712 - val_loss: 0.4995 - val_accuracy: 0.8520\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 108s 362ms/step - loss: 0.0702 - accuracy: 0.9752 - val_loss: 0.6756 - val_accuracy: 0.8426\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 111s 372ms/step - loss: 0.0536 - accuracy: 0.9766 - val_loss: 0.8256 - val_accuracy: 0.8426\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 113s 377ms/step - loss: 0.0435 - accuracy: 0.9801 - val_loss: 0.8174 - val_accuracy: 0.8417\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 113s 377ms/step - loss: 0.0441 - accuracy: 0.9817 - val_loss: 0.8978 - val_accuracy: 0.8445\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 114s 380ms/step - loss: 0.0318 - accuracy: 0.9848 - val_loss: 0.9961 - val_accuracy: 0.8435\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 113s 378ms/step - loss: 0.0355 - accuracy: 0.9825 - val_loss: 0.9313 - val_accuracy: 0.8445\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 117s 391ms/step - loss: 0.0302 - accuracy: 0.9861 - val_loss: 0.9549 - val_accuracy: 0.8426\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 86.80490255355835\n",
            "Training 4: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 163s 461ms/step - loss: 0.5623 - accuracy: 0.7407 - val_loss: 0.3673 - val_accuracy: 0.8511\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 110s 369ms/step - loss: 0.1992 - accuracy: 0.9306 - val_loss: 0.3839 - val_accuracy: 0.8520\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 111s 371ms/step - loss: 0.1418 - accuracy: 0.9526 - val_loss: 0.4391 - val_accuracy: 0.8407\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 111s 370ms/step - loss: 0.1142 - accuracy: 0.9618 - val_loss: 0.4932 - val_accuracy: 0.8417\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 110s 367ms/step - loss: 0.0955 - accuracy: 0.9624 - val_loss: 0.5062 - val_accuracy: 0.8483\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 111s 370ms/step - loss: 0.0671 - accuracy: 0.9741 - val_loss: 0.5927 - val_accuracy: 0.8435\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 112s 373ms/step - loss: 0.0601 - accuracy: 0.9763 - val_loss: 0.6693 - val_accuracy: 0.8483\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 110s 369ms/step - loss: 0.0456 - accuracy: 0.9817 - val_loss: 0.7902 - val_accuracy: 0.8435\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 111s 372ms/step - loss: 0.0424 - accuracy: 0.9815 - val_loss: 0.8139 - val_accuracy: 0.8501\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 112s 374ms/step - loss: 0.0337 - accuracy: 0.9832 - val_loss: 0.9097 - val_accuracy: 0.8445\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 111s 369ms/step - loss: 0.0333 - accuracy: 0.9858 - val_loss: 0.8747 - val_accuracy: 0.8492\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 111s 371ms/step - loss: 0.0350 - accuracy: 0.9826 - val_loss: 0.7258 - val_accuracy: 0.8435\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 85.20264029502869\n",
            "Training 5: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 140s 372ms/step - loss: 0.5615 - accuracy: 0.7406 - val_loss: 0.3544 - val_accuracy: 0.8624\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 113s 379ms/step - loss: 0.2115 - accuracy: 0.9258 - val_loss: 0.3718 - val_accuracy: 0.8643\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 108s 362ms/step - loss: 0.1388 - accuracy: 0.9511 - val_loss: 0.3872 - val_accuracy: 0.8577\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 106s 353ms/step - loss: 0.1042 - accuracy: 0.9636 - val_loss: 0.4084 - val_accuracy: 0.8549\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 105s 350ms/step - loss: 0.0892 - accuracy: 0.9703 - val_loss: 0.5188 - val_accuracy: 0.8549\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 104s 348ms/step - loss: 0.0682 - accuracy: 0.9758 - val_loss: 0.5637 - val_accuracy: 0.8558\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 104s 348ms/step - loss: 0.0480 - accuracy: 0.9799 - val_loss: 0.6639 - val_accuracy: 0.8492\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 103s 346ms/step - loss: 0.0447 - accuracy: 0.9803 - val_loss: 0.7027 - val_accuracy: 0.8454\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 105s 350ms/step - loss: 0.0369 - accuracy: 0.9831 - val_loss: 0.7941 - val_accuracy: 0.8501\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 103s 346ms/step - loss: 0.0363 - accuracy: 0.9828 - val_loss: 0.7940 - val_accuracy: 0.8501\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 102s 341ms/step - loss: 0.0346 - accuracy: 0.9845 - val_loss: 0.8739 - val_accuracy: 0.8483\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 101s 339ms/step - loss: 0.0361 - accuracy: 0.9847 - val_loss: 0.8246 - val_accuracy: 0.8501\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 86.42789721488953\n",
            "Training 6: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 152s 412ms/step - loss: 0.5566 - accuracy: 0.7339 - val_loss: 0.3549 - val_accuracy: 0.8662\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 115s 384ms/step - loss: 0.1982 - accuracy: 0.9306 - val_loss: 0.3683 - val_accuracy: 0.8662\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 117s 392ms/step - loss: 0.1318 - accuracy: 0.9565 - val_loss: 0.4323 - val_accuracy: 0.8577\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 118s 394ms/step - loss: 0.1123 - accuracy: 0.9606 - val_loss: 0.4832 - val_accuracy: 0.8454\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 118s 396ms/step - loss: 0.0720 - accuracy: 0.9748 - val_loss: 0.5607 - val_accuracy: 0.8511\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 116s 388ms/step - loss: 0.0530 - accuracy: 0.9778 - val_loss: 0.6014 - val_accuracy: 0.8483\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 116s 389ms/step - loss: 0.0513 - accuracy: 0.9804 - val_loss: 0.6608 - val_accuracy: 0.8435\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 118s 394ms/step - loss: 0.0403 - accuracy: 0.9837 - val_loss: 0.7532 - val_accuracy: 0.8464\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 116s 386ms/step - loss: 0.0338 - accuracy: 0.9857 - val_loss: 0.7982 - val_accuracy: 0.8501\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 112s 376ms/step - loss: 0.0342 - accuracy: 0.9868 - val_loss: 0.8244 - val_accuracy: 0.8435\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 112s 375ms/step - loss: 0.0332 - accuracy: 0.9842 - val_loss: 0.9060 - val_accuracy: 0.8351\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 86.6163969039917\n",
            "Training 7: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 167s 476ms/step - loss: 0.5601 - accuracy: 0.7316 - val_loss: 0.3525 - val_accuracy: 0.8547\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 118s 396ms/step - loss: 0.2126 - accuracy: 0.9283 - val_loss: 0.3463 - val_accuracy: 0.8377\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 119s 397ms/step - loss: 0.1488 - accuracy: 0.9488 - val_loss: 0.3807 - val_accuracy: 0.8406\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 119s 398ms/step - loss: 0.1231 - accuracy: 0.9576 - val_loss: 0.4114 - val_accuracy: 0.8585\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 120s 401ms/step - loss: 0.0829 - accuracy: 0.9690 - val_loss: 0.4375 - val_accuracy: 0.8566\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 118s 396ms/step - loss: 0.0663 - accuracy: 0.9752 - val_loss: 0.4759 - val_accuracy: 0.8528\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 122s 407ms/step - loss: 0.0566 - accuracy: 0.9771 - val_loss: 0.5541 - val_accuracy: 0.8585\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 120s 402ms/step - loss: 0.0518 - accuracy: 0.9771 - val_loss: 0.6261 - val_accuracy: 0.8519\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 121s 404ms/step - loss: 0.0382 - accuracy: 0.9850 - val_loss: 0.6509 - val_accuracy: 0.8632\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 121s 404ms/step - loss: 0.0396 - accuracy: 0.9832 - val_loss: 0.7098 - val_accuracy: 0.8585\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 120s 402ms/step - loss: 0.0385 - accuracy: 0.9822 - val_loss: 0.8035 - val_accuracy: 0.8415\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 122s 407ms/step - loss: 0.0364 - accuracy: 0.9851 - val_loss: 0.8027 - val_accuracy: 0.8481\n",
            "Epoch 13/40\n",
            "299/299 [==============================] - 120s 402ms/step - loss: 0.0288 - accuracy: 0.9863 - val_loss: 0.8340 - val_accuracy: 0.8481\n",
            "Epoch 14/40\n",
            "299/299 [==============================] - 121s 406ms/step - loss: 0.0327 - accuracy: 0.9834 - val_loss: 0.8667 - val_accuracy: 0.8509\n",
            "Epoch 15/40\n",
            "299/299 [==============================] - 120s 401ms/step - loss: 0.0299 - accuracy: 0.9847 - val_loss: 0.9146 - val_accuracy: 0.8500\n",
            "Epoch 16/40\n",
            "299/299 [==============================] - 117s 393ms/step - loss: 0.0292 - accuracy: 0.9869 - val_loss: 0.9196 - val_accuracy: 0.8538\n",
            "Epoch 17/40\n",
            "299/299 [==============================] - 119s 397ms/step - loss: 0.0308 - accuracy: 0.9829 - val_loss: 0.8628 - val_accuracy: 0.8538\n",
            "Epoch 18/40\n",
            "299/299 [==============================] - 120s 402ms/step - loss: 0.0325 - accuracy: 0.9844 - val_loss: 0.9286 - val_accuracy: 0.8557\n",
            "Epoch 19/40\n",
            "299/299 [==============================] - 119s 399ms/step - loss: 0.0281 - accuracy: 0.9868 - val_loss: 0.9189 - val_accuracy: 0.8528\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00019: early stopping\n",
            "Test Accuracy: 86.32075190544128\n",
            "Training 8: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 174s 502ms/step - loss: 0.5607 - accuracy: 0.7332 - val_loss: 0.3765 - val_accuracy: 0.8538\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 131s 437ms/step - loss: 0.2068 - accuracy: 0.9237 - val_loss: 0.3809 - val_accuracy: 0.8604\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 131s 437ms/step - loss: 0.1366 - accuracy: 0.9539 - val_loss: 0.4247 - val_accuracy: 0.8642\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 133s 444ms/step - loss: 0.1098 - accuracy: 0.9631 - val_loss: 0.4795 - val_accuracy: 0.8604\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 135s 451ms/step - loss: 0.0851 - accuracy: 0.9677 - val_loss: 0.4913 - val_accuracy: 0.8566\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 133s 445ms/step - loss: 0.0623 - accuracy: 0.9756 - val_loss: 0.6056 - val_accuracy: 0.8443\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 134s 448ms/step - loss: 0.0491 - accuracy: 0.9803 - val_loss: 0.7166 - val_accuracy: 0.8425\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 133s 447ms/step - loss: 0.0416 - accuracy: 0.9818 - val_loss: 0.7484 - val_accuracy: 0.8500\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 130s 436ms/step - loss: 0.0361 - accuracy: 0.9843 - val_loss: 0.7811 - val_accuracy: 0.8311\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 130s 435ms/step - loss: 0.0337 - accuracy: 0.9836 - val_loss: 0.8495 - val_accuracy: 0.8396\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 130s 436ms/step - loss: 0.0298 - accuracy: 0.9866 - val_loss: 0.8950 - val_accuracy: 0.8500\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 131s 438ms/step - loss: 0.0298 - accuracy: 0.9850 - val_loss: 0.9690 - val_accuracy: 0.8434\n",
            "Epoch 13/40\n",
            "299/299 [==============================] - 131s 440ms/step - loss: 0.0250 - accuracy: 0.9873 - val_loss: 1.0523 - val_accuracy: 0.8425\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 86.41509413719177\n",
            "Training 9: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 161s 449ms/step - loss: 0.5585 - accuracy: 0.7418 - val_loss: 0.3547 - val_accuracy: 0.8585\n",
            "Epoch 2/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "299/299 [==============================] - 119s 399ms/step - loss: 0.2065 - accuracy: 0.9297 - val_loss: 0.3972 - val_accuracy: 0.8481\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 115s 384ms/step - loss: 0.1368 - accuracy: 0.9531 - val_loss: 0.4553 - val_accuracy: 0.8406\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 114s 380ms/step - loss: 0.1094 - accuracy: 0.9639 - val_loss: 0.4892 - val_accuracy: 0.8462\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 113s 379ms/step - loss: 0.0854 - accuracy: 0.9687 - val_loss: 0.5569 - val_accuracy: 0.8415\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 114s 381ms/step - loss: 0.0682 - accuracy: 0.9730 - val_loss: 0.7185 - val_accuracy: 0.8358\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 113s 379ms/step - loss: 0.0525 - accuracy: 0.9775 - val_loss: 0.7469 - val_accuracy: 0.8387\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 113s 379ms/step - loss: 0.0366 - accuracy: 0.9849 - val_loss: 0.8872 - val_accuracy: 0.8349\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 114s 382ms/step - loss: 0.0385 - accuracy: 0.9846 - val_loss: 0.8437 - val_accuracy: 0.8491\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 115s 386ms/step - loss: 0.0351 - accuracy: 0.9837 - val_loss: 0.8219 - val_accuracy: 0.8443\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 119s 397ms/step - loss: 0.0333 - accuracy: 0.9840 - val_loss: 0.9220 - val_accuracy: 0.8415\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 85.84905862808228\n",
            "Training 10: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 177s 499ms/step - loss: 0.5572 - accuracy: 0.7417 - val_loss: 0.3517 - val_accuracy: 0.8547\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 141s 472ms/step - loss: 0.2025 - accuracy: 0.9293 - val_loss: 0.3542 - val_accuracy: 0.8623\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 142s 474ms/step - loss: 0.1401 - accuracy: 0.9522 - val_loss: 0.3689 - val_accuracy: 0.8660\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 140s 470ms/step - loss: 0.1046 - accuracy: 0.9652 - val_loss: 0.4575 - val_accuracy: 0.8623\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 140s 469ms/step - loss: 0.0846 - accuracy: 0.9706 - val_loss: 0.4913 - val_accuracy: 0.8500\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 141s 470ms/step - loss: 0.0619 - accuracy: 0.9780 - val_loss: 0.5496 - val_accuracy: 0.8566\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 142s 476ms/step - loss: 0.0481 - accuracy: 0.9793 - val_loss: 0.7100 - val_accuracy: 0.8481\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 142s 476ms/step - loss: 0.0421 - accuracy: 0.9824 - val_loss: 0.6702 - val_accuracy: 0.8443\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 142s 475ms/step - loss: 0.0394 - accuracy: 0.9829 - val_loss: 0.8470 - val_accuracy: 0.8519\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 143s 478ms/step - loss: 0.0351 - accuracy: 0.9845 - val_loss: 0.8085 - val_accuracy: 0.8377\n",
            "Epoch 11/40\n",
            "299/299 [==============================] - 143s 478ms/step - loss: 0.0367 - accuracy: 0.9827 - val_loss: 0.8683 - val_accuracy: 0.8358\n",
            "Epoch 12/40\n",
            "299/299 [==============================] - 142s 476ms/step - loss: 0.0283 - accuracy: 0.9868 - val_loss: 0.9301 - val_accuracy: 0.8283\n",
            "Epoch 13/40\n",
            "299/299 [==============================] - 142s 474ms/step - loss: 0.0249 - accuracy: 0.9886 - val_loss: 0.9834 - val_accuracy: 0.8302\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 86.60377264022827\n",
            "\n",
            "        acc1       acc2       acc3      acc4       acc5       acc6       acc7  \\\n",
            "0  84.448636  86.145145  86.804903  85.20264  86.427897  86.616397  86.320752   \n",
            "\n",
            "        acc8       acc9      acc10        AVG  \n",
            "0  86.415094  85.849059  86.603773  86.083429  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbpJ4LZz9K9Q"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA-CbVMD9K9R",
        "outputId": "ed968b0b-8634-42fe-a727-d9669c068987"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>84.448636</td>\n",
              "      <td>86.145145</td>\n",
              "      <td>86.804903</td>\n",
              "      <td>85.20264</td>\n",
              "      <td>86.427897</td>\n",
              "      <td>86.616397</td>\n",
              "      <td>86.320752</td>\n",
              "      <td>86.415094</td>\n",
              "      <td>85.849059</td>\n",
              "      <td>86.603773</td>\n",
              "      <td>86.083429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3      acc4       acc5       acc6       acc7  \\\n",
              "0  84.448636  86.145145  86.804903  85.20264  86.427897  86.616397  86.320752   \n",
              "\n",
              "        acc8       acc9      acc10        AVG  \n",
              "0  86.415094  85.849059  86.603773  86.083429  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwnmOEL59K9S"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('LSTM_MPQA_v2.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POFWsonq9K9S"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNdqNc5E9K9T"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6omUq89K9T"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpC6bP2n9K9U"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Google Colab/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4f2NSLl9K9U",
        "outputId": "e078bba7-c273-421f-afa8-e50ec5d17ac6"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAcxLz9s9K9V"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGIXoouh9K9W"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVkVRN1w9K9W",
        "outputId": "a24e6561-f31f-4450-8c69-c29da4221fbc"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6083 words present from 6236 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_QGLFtL9K9X"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLpGWkBX9K9X",
        "outputId": "93679f74-56d4-4114-e258-e245b83a58ec"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a5uyUrB9K9X"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTnwp2Cv9K9Y",
        "outputId": "6d455d69-8cbb-4dd4-8820-e007a0643511"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV7qbZZw9K9Y"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZTuqdEY9K9Y"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVjm23C89K9Z",
        "outputId": "d7a8d774-405a-4e5f-bd03-0388837821f6"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 285,825\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5en7XslE9K9Z"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPUVVWLF9K9Z"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0s7sndK9K9a",
        "outputId": "71358543-3de1-4857-9cae-d9d33d79a163"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 58s 151ms/step - loss: 0.4629 - accuracy: 0.7860 - val_loss: 0.3023 - val_accuracy: 0.8784\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 30s 101ms/step - loss: 0.2800 - accuracy: 0.8903 - val_loss: 0.2979 - val_accuracy: 0.8784\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 42s 139ms/step - loss: 0.2408 - accuracy: 0.9094 - val_loss: 0.3034 - val_accuracy: 0.8812\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 30s 102ms/step - loss: 0.2250 - accuracy: 0.9157 - val_loss: 0.3057 - val_accuracy: 0.8746\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 30s 101ms/step - loss: 0.2014 - accuracy: 0.9230 - val_loss: 0.3060 - val_accuracy: 0.8822\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 39s 131ms/step - loss: 0.1688 - accuracy: 0.9337 - val_loss: 0.3328 - val_accuracy: 0.8652\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.1379 - accuracy: 0.9473 - val_loss: 0.3500 - val_accuracy: 0.8728\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.1277 - accuracy: 0.9498 - val_loss: 0.3509 - val_accuracy: 0.8784\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 41s 136ms/step - loss: 0.1118 - accuracy: 0.9554 - val_loss: 0.3802 - val_accuracy: 0.8728\n",
            "Epoch 10/100\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.1073 - accuracy: 0.9588 - val_loss: 0.4263 - val_accuracy: 0.8737\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 88.21865916252136\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 59s 153ms/step - loss: 0.4573 - accuracy: 0.7941 - val_loss: 0.3144 - val_accuracy: 0.8728\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 38s 127ms/step - loss: 0.2716 - accuracy: 0.8969 - val_loss: 0.3198 - val_accuracy: 0.8671\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.2481 - accuracy: 0.9033 - val_loss: 0.3024 - val_accuracy: 0.8794\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 31s 103ms/step - loss: 0.2258 - accuracy: 0.9138 - val_loss: 0.2983 - val_accuracy: 0.8860\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 42s 140ms/step - loss: 0.2019 - accuracy: 0.9222 - val_loss: 0.3087 - val_accuracy: 0.8822\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 43s 143ms/step - loss: 0.1812 - accuracy: 0.9300 - val_loss: 0.3402 - val_accuracy: 0.8850\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 39s 131ms/step - loss: 0.1417 - accuracy: 0.9439 - val_loss: 0.3520 - val_accuracy: 0.8888\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.1363 - accuracy: 0.9452 - val_loss: 0.3923 - val_accuracy: 0.8878\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 36s 121ms/step - loss: 0.1181 - accuracy: 0.9510 - val_loss: 0.4257 - val_accuracy: 0.8756\n",
            "Epoch 10/100\n",
            "299/299 [==============================] - 34s 113ms/step - loss: 0.1013 - accuracy: 0.9576 - val_loss: 0.4834 - val_accuracy: 0.8718\n",
            "Epoch 11/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.0809 - accuracy: 0.9693 - val_loss: 0.5243 - val_accuracy: 0.8860\n",
            "Epoch 12/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.0705 - accuracy: 0.9743 - val_loss: 0.5342 - val_accuracy: 0.8775\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 88.87841701507568\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 60s 149ms/step - loss: 0.4672 - accuracy: 0.7794 - val_loss: 0.2820 - val_accuracy: 0.8897\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 35s 116ms/step - loss: 0.2700 - accuracy: 0.8946 - val_loss: 0.2798 - val_accuracy: 0.8897\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 38s 129ms/step - loss: 0.2518 - accuracy: 0.9006 - val_loss: 0.2720 - val_accuracy: 0.8888\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 35s 118ms/step - loss: 0.2238 - accuracy: 0.9114 - val_loss: 0.2818 - val_accuracy: 0.8812\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 41s 138ms/step - loss: 0.1947 - accuracy: 0.9225 - val_loss: 0.2848 - val_accuracy: 0.8944\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 38s 127ms/step - loss: 0.1700 - accuracy: 0.9305 - val_loss: 0.2893 - val_accuracy: 0.8944\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 38s 127ms/step - loss: 0.1493 - accuracy: 0.9418 - val_loss: 0.3199 - val_accuracy: 0.8973\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.1295 - accuracy: 0.9513 - val_loss: 0.3416 - val_accuracy: 0.8954\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1112 - accuracy: 0.9568 - val_loss: 0.3806 - val_accuracy: 0.8841\n",
            "Epoch 10/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.0930 - accuracy: 0.9664 - val_loss: 0.4090 - val_accuracy: 0.8822\n",
            "Epoch 11/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0961 - accuracy: 0.9649 - val_loss: 0.4176 - val_accuracy: 0.8963\n",
            "Epoch 12/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0795 - accuracy: 0.9703 - val_loss: 0.4987 - val_accuracy: 0.8737\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 89.72667455673218\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 55s 123ms/step - loss: 0.4602 - accuracy: 0.7844 - val_loss: 0.3116 - val_accuracy: 0.8709\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.2648 - accuracy: 0.8976 - val_loss: 0.3177 - val_accuracy: 0.8699\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.2469 - accuracy: 0.9068 - val_loss: 0.3080 - val_accuracy: 0.8662\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.2296 - accuracy: 0.9145 - val_loss: 0.2978 - val_accuracy: 0.8756\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.2074 - accuracy: 0.9162 - val_loss: 0.3071 - val_accuracy: 0.8718\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1720 - accuracy: 0.9331 - val_loss: 0.3187 - val_accuracy: 0.8746\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.1660 - accuracy: 0.9341 - val_loss: 0.3655 - val_accuracy: 0.8728\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1260 - accuracy: 0.9495 - val_loss: 0.3937 - val_accuracy: 0.8652\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.1190 - accuracy: 0.9549 - val_loss: 0.4458 - val_accuracy: 0.8718\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 87.55890727043152\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 50s 124ms/step - loss: 0.4524 - accuracy: 0.7866 - val_loss: 0.3399 - val_accuracy: 0.8577\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.2654 - accuracy: 0.8990 - val_loss: 0.3323 - val_accuracy: 0.8652\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.2484 - accuracy: 0.9044 - val_loss: 0.3248 - val_accuracy: 0.8652\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.2235 - accuracy: 0.9164 - val_loss: 0.3284 - val_accuracy: 0.8633\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.1930 - accuracy: 0.9263 - val_loss: 0.3266 - val_accuracy: 0.8690\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.1762 - accuracy: 0.9348 - val_loss: 0.3418 - val_accuracy: 0.8662\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.1563 - accuracy: 0.9380 - val_loss: 0.3732 - val_accuracy: 0.8728\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.1357 - accuracy: 0.9461 - val_loss: 0.4257 - val_accuracy: 0.8633\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.1211 - accuracy: 0.9515 - val_loss: 0.4210 - val_accuracy: 0.8662\n",
            "Epoch 10/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0989 - accuracy: 0.9620 - val_loss: 0.4178 - val_accuracy: 0.8765\n",
            "Epoch 11/100\n",
            "299/299 [==============================] - 32s 109ms/step - loss: 0.0851 - accuracy: 0.9662 - val_loss: 0.4724 - val_accuracy: 0.8756\n",
            "Epoch 12/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0750 - accuracy: 0.9723 - val_loss: 0.5254 - val_accuracy: 0.8709\n",
            "Epoch 13/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.0745 - accuracy: 0.9716 - val_loss: 0.5825 - val_accuracy: 0.8775\n",
            "Epoch 14/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.0610 - accuracy: 0.9787 - val_loss: 0.5338 - val_accuracy: 0.8737\n",
            "Epoch 15/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.6144 - val_accuracy: 0.8737\n",
            "Epoch 16/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.0500 - accuracy: 0.9809 - val_loss: 0.6180 - val_accuracy: 0.8784\n",
            "Epoch 17/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0644 - accuracy: 0.9763 - val_loss: 0.6160 - val_accuracy: 0.8728\n",
            "Epoch 18/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0396 - accuracy: 0.9835 - val_loss: 0.7096 - val_accuracy: 0.8746\n",
            "Epoch 19/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.0467 - accuracy: 0.9801 - val_loss: 0.7439 - val_accuracy: 0.8756\n",
            "Epoch 20/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0411 - accuracy: 0.9833 - val_loss: 0.7551 - val_accuracy: 0.8765\n",
            "Epoch 21/100\n",
            "299/299 [==============================] - 33s 112ms/step - loss: 0.0417 - accuracy: 0.9825 - val_loss: 0.8053 - val_accuracy: 0.8803\n",
            "Epoch 22/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.0351 - accuracy: 0.9847 - val_loss: 0.8612 - val_accuracy: 0.8756\n",
            "Epoch 23/100\n",
            "299/299 [==============================] - 32s 106ms/step - loss: 0.0429 - accuracy: 0.9824 - val_loss: 0.7754 - val_accuracy: 0.8737\n",
            "Epoch 24/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0411 - accuracy: 0.9832 - val_loss: 0.7525 - val_accuracy: 0.8737\n",
            "Epoch 25/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.0351 - accuracy: 0.9838 - val_loss: 0.7917 - val_accuracy: 0.8662\n",
            "Epoch 26/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.0321 - accuracy: 0.9852 - val_loss: 0.8848 - val_accuracy: 0.8794\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00026: early stopping\n",
            "Test Accuracy: 88.03015947341919\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 51s 124ms/step - loss: 0.4695 - accuracy: 0.7869 - val_loss: 0.2971 - val_accuracy: 0.8737\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.2785 - accuracy: 0.8915 - val_loss: 0.2874 - val_accuracy: 0.8869\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.2539 - accuracy: 0.9005 - val_loss: 0.2760 - val_accuracy: 0.8897\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 32s 107ms/step - loss: 0.2239 - accuracy: 0.9126 - val_loss: 0.2741 - val_accuracy: 0.8907\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.2096 - accuracy: 0.9206 - val_loss: 0.2919 - val_accuracy: 0.8907\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1806 - accuracy: 0.9281 - val_loss: 0.3098 - val_accuracy: 0.8878\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 32s 109ms/step - loss: 0.1574 - accuracy: 0.9405 - val_loss: 0.3204 - val_accuracy: 0.8794\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 34s 113ms/step - loss: 0.1320 - accuracy: 0.9513 - val_loss: 0.3570 - val_accuracy: 0.8784\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 35s 117ms/step - loss: 0.1211 - accuracy: 0.9541 - val_loss: 0.3982 - val_accuracy: 0.8831\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 89.06691670417786\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 51s 113ms/step - loss: 0.4742 - accuracy: 0.7810 - val_loss: 0.3074 - val_accuracy: 0.8840\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 29s 98ms/step - loss: 0.2657 - accuracy: 0.8976 - val_loss: 0.2942 - val_accuracy: 0.8896\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 31s 105ms/step - loss: 0.2449 - accuracy: 0.9052 - val_loss: 0.2902 - val_accuracy: 0.8972\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 29s 97ms/step - loss: 0.2206 - accuracy: 0.9127 - val_loss: 0.2899 - val_accuracy: 0.8981\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 29s 97ms/step - loss: 0.1894 - accuracy: 0.9230 - val_loss: 0.2913 - val_accuracy: 0.8925\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 29s 96ms/step - loss: 0.1697 - accuracy: 0.9327 - val_loss: 0.3242 - val_accuracy: 0.8896\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 29s 96ms/step - loss: 0.1462 - accuracy: 0.9390 - val_loss: 0.3711 - val_accuracy: 0.8802\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 29s 96ms/step - loss: 0.1377 - accuracy: 0.9439 - val_loss: 0.3606 - val_accuracy: 0.8840\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 28s 95ms/step - loss: 0.1090 - accuracy: 0.9585 - val_loss: 0.3845 - val_accuracy: 0.8830\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 89.81131911277771\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 51s 125ms/step - loss: 0.4588 - accuracy: 0.7823 - val_loss: 0.3311 - val_accuracy: 0.8670\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.2694 - accuracy: 0.8976 - val_loss: 0.3224 - val_accuracy: 0.8660\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 33s 111ms/step - loss: 0.2418 - accuracy: 0.9047 - val_loss: 0.3075 - val_accuracy: 0.8726\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.2069 - accuracy: 0.9260 - val_loss: 0.3115 - val_accuracy: 0.8745\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.1885 - accuracy: 0.9241 - val_loss: 0.3132 - val_accuracy: 0.8726\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.1715 - accuracy: 0.9314 - val_loss: 0.3300 - val_accuracy: 0.8726\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1538 - accuracy: 0.9349 - val_loss: 0.3946 - val_accuracy: 0.8623\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1309 - accuracy: 0.9484 - val_loss: 0.3991 - val_accuracy: 0.8594\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.1083 - accuracy: 0.9581 - val_loss: 0.4255 - val_accuracy: 0.8642\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 87.45282888412476\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 54s 135ms/step - loss: 0.4709 - accuracy: 0.7829 - val_loss: 0.3453 - val_accuracy: 0.8651\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.2617 - accuracy: 0.8969 - val_loss: 0.3300 - val_accuracy: 0.8623\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 33s 111ms/step - loss: 0.2427 - accuracy: 0.9086 - val_loss: 0.3320 - val_accuracy: 0.8613\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.2154 - accuracy: 0.9183 - val_loss: 0.3484 - val_accuracy: 0.8660\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 33s 112ms/step - loss: 0.1940 - accuracy: 0.9264 - val_loss: 0.3597 - val_accuracy: 0.8604\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 33s 111ms/step - loss: 0.1685 - accuracy: 0.9303 - val_loss: 0.3710 - val_accuracy: 0.8623\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.1553 - accuracy: 0.9385 - val_loss: 0.4133 - val_accuracy: 0.8604\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 33s 111ms/step - loss: 0.1294 - accuracy: 0.9489 - val_loss: 0.4106 - val_accuracy: 0.8632\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 34s 113ms/step - loss: 0.1119 - accuracy: 0.9562 - val_loss: 0.5059 - val_accuracy: 0.8528\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 86.60377264022827\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "299/299 [==============================] - 51s 127ms/step - loss: 0.4630 - accuracy: 0.7805 - val_loss: 0.3777 - val_accuracy: 0.8538\n",
            "Epoch 2/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.2581 - accuracy: 0.9043 - val_loss: 0.3554 - val_accuracy: 0.8491\n",
            "Epoch 3/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.2349 - accuracy: 0.9127 - val_loss: 0.3591 - val_accuracy: 0.8528\n",
            "Epoch 4/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.2107 - accuracy: 0.9178 - val_loss: 0.3629 - val_accuracy: 0.8481\n",
            "Epoch 5/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.1992 - accuracy: 0.9210 - val_loss: 0.3749 - val_accuracy: 0.8557\n",
            "Epoch 6/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.1709 - accuracy: 0.9309 - val_loss: 0.3770 - val_accuracy: 0.8557\n",
            "Epoch 7/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.1396 - accuracy: 0.9419 - val_loss: 0.4060 - val_accuracy: 0.8528\n",
            "Epoch 8/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.1232 - accuracy: 0.9489 - val_loss: 0.4065 - val_accuracy: 0.8575\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.1035 - accuracy: 0.9557 - val_loss: 0.4697 - val_accuracy: 0.8500\n",
            "Epoch 10/100\n",
            "299/299 [==============================] - 33s 110ms/step - loss: 0.0928 - accuracy: 0.9628 - val_loss: 0.5054 - val_accuracy: 0.8453\n",
            "Epoch 11/100\n",
            "299/299 [==============================] - 32s 108ms/step - loss: 0.0925 - accuracy: 0.9652 - val_loss: 0.5996 - val_accuracy: 0.8481\n",
            "Epoch 12/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.0735 - accuracy: 0.9734 - val_loss: 0.6074 - val_accuracy: 0.8509\n",
            "Epoch 13/100\n",
            "299/299 [==============================] - 33s 109ms/step - loss: 0.0635 - accuracy: 0.9750 - val_loss: 0.7077 - val_accuracy: 0.8472\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 85.75471639633179\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
            "0  88.218659  88.878417  89.726675  ...  86.603773  85.754716  88.110237\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urr3lwEk9K9a"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "nCorwcHG9K9b",
        "outputId": "56b7f8ed-ffc7-4cfd-81c2-758a87e829aa"
      },
      "source": [
        "record2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88.218659</td>\n",
              "      <td>88.878417</td>\n",
              "      <td>89.726675</td>\n",
              "      <td>87.558907</td>\n",
              "      <td>88.030159</td>\n",
              "      <td>89.066917</td>\n",
              "      <td>89.811319</td>\n",
              "      <td>87.452829</td>\n",
              "      <td>86.603773</td>\n",
              "      <td>85.754716</td>\n",
              "      <td>88.110237</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
              "0  88.218659  88.878417  89.726675  ...  86.603773  85.754716  88.110237\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfib-8r-9K9c"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('LSTM_MPQA_v2_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZYAyBZd9K9c"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBTAjnxp9K9c"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-xLGESZ9K9c"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqRRR7jL9K9d"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCosjEzy9K9d",
        "outputId": "2e4dd128-c104-45ed-ef7d-c331c5ae0238"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_22 (Bidirectio (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_23 (Bidirectio (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 585,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shKmZRl99K9d"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD8uIwfi9K9e"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8NekkBQy9K9e",
        "outputId": "4e184663-68aa-479a-fa5e-03bc577e433d"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 58s 149ms/step - loss: 0.4563 - accuracy: 0.7912 - val_loss: 0.2823 - val_accuracy: 0.8935\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 39s 132ms/step - loss: 0.1978 - accuracy: 0.9277 - val_loss: 0.3177 - val_accuracy: 0.8728\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 39s 131ms/step - loss: 0.1282 - accuracy: 0.9545 - val_loss: 0.3759 - val_accuracy: 0.8718\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 132ms/step - loss: 0.0907 - accuracy: 0.9638 - val_loss: 0.4525 - val_accuracy: 0.8690\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 39s 132ms/step - loss: 0.0634 - accuracy: 0.9765 - val_loss: 0.4358 - val_accuracy: 0.8652\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 39s 131ms/step - loss: 0.0571 - accuracy: 0.9772 - val_loss: 0.5092 - val_accuracy: 0.8633\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 89.34966921806335\n",
            "Training 2: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 58s 149ms/step - loss: 0.4518 - accuracy: 0.7861 - val_loss: 0.3034 - val_accuracy: 0.8775\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.1775 - accuracy: 0.9394 - val_loss: 0.3386 - val_accuracy: 0.8718\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.1246 - accuracy: 0.9601 - val_loss: 0.4055 - val_accuracy: 0.8615\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.0815 - accuracy: 0.9700 - val_loss: 0.4639 - val_accuracy: 0.8586\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.0600 - accuracy: 0.9759 - val_loss: 0.5278 - val_accuracy: 0.8520\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0502 - accuracy: 0.9806 - val_loss: 0.6155 - val_accuracy: 0.8577\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 87.74740695953369\n",
            "Training 3: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 58s 150ms/step - loss: 0.4527 - accuracy: 0.7984 - val_loss: 0.3117 - val_accuracy: 0.8671\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.1812 - accuracy: 0.9369 - val_loss: 0.3342 - val_accuracy: 0.8652\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 40s 135ms/step - loss: 0.1180 - accuracy: 0.9597 - val_loss: 0.3790 - val_accuracy: 0.8530\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0909 - accuracy: 0.9631 - val_loss: 0.4464 - val_accuracy: 0.8567\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.0698 - accuracy: 0.9725 - val_loss: 0.5550 - val_accuracy: 0.8586\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 40s 132ms/step - loss: 0.0513 - accuracy: 0.9801 - val_loss: 0.6024 - val_accuracy: 0.8549\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 86.71064972877502\n",
            "Training 4: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 62s 150ms/step - loss: 0.4607 - accuracy: 0.7901 - val_loss: 0.2942 - val_accuracy: 0.8869\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.1857 - accuracy: 0.9347 - val_loss: 0.2940 - val_accuracy: 0.8888\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.1218 - accuracy: 0.9589 - val_loss: 0.3570 - val_accuracy: 0.8850\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 135ms/step - loss: 0.0987 - accuracy: 0.9614 - val_loss: 0.4062 - val_accuracy: 0.8756\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0687 - accuracy: 0.9704 - val_loss: 0.4523 - val_accuracy: 0.8784\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.0510 - accuracy: 0.9795 - val_loss: 0.5245 - val_accuracy: 0.8784\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 39s 132ms/step - loss: 0.0471 - accuracy: 0.9799 - val_loss: 0.5215 - val_accuracy: 0.8690\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 88.87841701507568\n",
            "Training 5: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 58s 150ms/step - loss: 0.4550 - accuracy: 0.7868 - val_loss: 0.3423 - val_accuracy: 0.8652\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 40s 132ms/step - loss: 0.1806 - accuracy: 0.9405 - val_loss: 0.3623 - val_accuracy: 0.8567\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 40s 132ms/step - loss: 0.1241 - accuracy: 0.9555 - val_loss: 0.3876 - val_accuracy: 0.8605\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0838 - accuracy: 0.9684 - val_loss: 0.4615 - val_accuracy: 0.8558\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.0706 - accuracy: 0.9709 - val_loss: 0.5293 - val_accuracy: 0.8539\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.0452 - accuracy: 0.9828 - val_loss: 0.6563 - val_accuracy: 0.8360\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 86.52215003967285\n",
            "Training 6: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 59s 152ms/step - loss: 0.4517 - accuracy: 0.7888 - val_loss: 0.2639 - val_accuracy: 0.8973\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.1939 - accuracy: 0.9326 - val_loss: 0.2711 - val_accuracy: 0.8963\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 41s 136ms/step - loss: 0.1314 - accuracy: 0.9551 - val_loss: 0.3064 - val_accuracy: 0.8926\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.0968 - accuracy: 0.9595 - val_loss: 0.3943 - val_accuracy: 0.8878\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.0648 - accuracy: 0.9751 - val_loss: 0.4528 - val_accuracy: 0.8784\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.0540 - accuracy: 0.9785 - val_loss: 0.4613 - val_accuracy: 0.8746\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 89.72667455673218\n",
            "Training 7: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 62s 151ms/step - loss: 0.4496 - accuracy: 0.7892 - val_loss: 0.3254 - val_accuracy: 0.8783\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.1747 - accuracy: 0.9334 - val_loss: 0.3188 - val_accuracy: 0.8858\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 40s 133ms/step - loss: 0.1201 - accuracy: 0.9600 - val_loss: 0.3543 - val_accuracy: 0.8698\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0934 - accuracy: 0.9657 - val_loss: 0.4356 - val_accuracy: 0.8726\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 135ms/step - loss: 0.0671 - accuracy: 0.9717 - val_loss: 0.4749 - val_accuracy: 0.8660\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 41s 136ms/step - loss: 0.0554 - accuracy: 0.9749 - val_loss: 0.6228 - val_accuracy: 0.8642\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0437 - accuracy: 0.9823 - val_loss: 0.6685 - val_accuracy: 0.8557\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 88.58490586280823\n",
            "Training 8: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 54s 136ms/step - loss: 0.4538 - accuracy: 0.7955 - val_loss: 0.3099 - val_accuracy: 0.8726\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 36s 120ms/step - loss: 0.1935 - accuracy: 0.9350 - val_loss: 0.3148 - val_accuracy: 0.8774\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 35s 119ms/step - loss: 0.1299 - accuracy: 0.9523 - val_loss: 0.3449 - val_accuracy: 0.8792\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 36s 120ms/step - loss: 0.0938 - accuracy: 0.9645 - val_loss: 0.4128 - val_accuracy: 0.8717\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 36s 119ms/step - loss: 0.0709 - accuracy: 0.9698 - val_loss: 0.4585 - val_accuracy: 0.8679\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 36s 121ms/step - loss: 0.0562 - accuracy: 0.9758 - val_loss: 0.5112 - val_accuracy: 0.8708\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 36s 122ms/step - loss: 0.0528 - accuracy: 0.9790 - val_loss: 0.5486 - val_accuracy: 0.8679\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 36s 121ms/step - loss: 0.0567 - accuracy: 0.9754 - val_loss: 0.5886 - val_accuracy: 0.8670\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00008: early stopping\n",
            "Test Accuracy: 87.92452812194824\n",
            "Training 9: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 62s 160ms/step - loss: 0.4597 - accuracy: 0.7918 - val_loss: 0.3147 - val_accuracy: 0.8830\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 45s 150ms/step - loss: 0.1928 - accuracy: 0.9317 - val_loss: 0.3234 - val_accuracy: 0.8821\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 41s 136ms/step - loss: 0.1250 - accuracy: 0.9536 - val_loss: 0.3928 - val_accuracy: 0.8745\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 40s 135ms/step - loss: 0.0880 - accuracy: 0.9662 - val_loss: 0.4392 - val_accuracy: 0.8802\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0656 - accuracy: 0.9732 - val_loss: 0.5101 - val_accuracy: 0.8745\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 40s 134ms/step - loss: 0.0554 - accuracy: 0.9769 - val_loss: 0.6196 - val_accuracy: 0.8726\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 88.30188512802124\n",
            "Training 10: \n",
            "Epoch 1/40\n",
            "299/299 [==============================] - 59s 153ms/step - loss: 0.4389 - accuracy: 0.8030 - val_loss: 0.3189 - val_accuracy: 0.8632\n",
            "Epoch 2/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.1967 - accuracy: 0.9297 - val_loss: 0.3348 - val_accuracy: 0.8708\n",
            "Epoch 3/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.1237 - accuracy: 0.9560 - val_loss: 0.3879 - val_accuracy: 0.8679\n",
            "Epoch 4/40\n",
            "299/299 [==============================] - 41s 136ms/step - loss: 0.0896 - accuracy: 0.9661 - val_loss: 0.4598 - val_accuracy: 0.8717\n",
            "Epoch 5/40\n",
            "299/299 [==============================] - 40s 135ms/step - loss: 0.0658 - accuracy: 0.9708 - val_loss: 0.4885 - val_accuracy: 0.8745\n",
            "Epoch 6/40\n",
            "299/299 [==============================] - 41s 136ms/step - loss: 0.0517 - accuracy: 0.9774 - val_loss: 0.5418 - val_accuracy: 0.8689\n",
            "Epoch 7/40\n",
            "299/299 [==============================] - 41s 138ms/step - loss: 0.0473 - accuracy: 0.9781 - val_loss: 0.6174 - val_accuracy: 0.8604\n",
            "Epoch 8/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.0395 - accuracy: 0.9835 - val_loss: 0.7266 - val_accuracy: 0.8509\n",
            "Epoch 9/40\n",
            "299/299 [==============================] - 40s 135ms/step - loss: 0.0326 - accuracy: 0.9845 - val_loss: 0.8175 - val_accuracy: 0.8585\n",
            "Epoch 10/40\n",
            "299/299 [==============================] - 41s 137ms/step - loss: 0.0312 - accuracy: 0.9862 - val_loss: 0.7079 - val_accuracy: 0.8604\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 87.45282888412476\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-837fe5f4ceb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mrecord3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'record' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UHsDPfI9K9e"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "rJimVZoo9K9f",
        "outputId": "1328f1ee-f5d6-40de-87ba-ba5cb3f4c020"
      },
      "source": [
        "record3"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>89.349669</td>\n",
              "      <td>87.747407</td>\n",
              "      <td>86.71065</td>\n",
              "      <td>88.878417</td>\n",
              "      <td>86.52215</td>\n",
              "      <td>89.726675</td>\n",
              "      <td>88.584906</td>\n",
              "      <td>87.924528</td>\n",
              "      <td>88.301885</td>\n",
              "      <td>87.452829</td>\n",
              "      <td>88.119912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2      acc3  ...       acc9      acc10        AVG\n",
              "0  89.349669  87.747407  86.71065  ...  88.301885  87.452829  88.119912\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXP3dpaq9K9f"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('LSTM_MPQA_v2_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}