{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LSTM_SUBJ_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mCiO4kiofqlE",
        "W0WuuutofqlG"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKMUc7xfqkv"
      },
      "source": [
        "# LSTM Classification with SUBJ Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using LSTM model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-7px_ChfuFU",
        "outputId": "ff1cab77-7f12-480c-b3c0-5de1706d5be2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ0AcYQNfqk5",
        "outputId": "d30eb056-2071-4cb6-c422-635695b3978f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OVKI2rxfqk7",
        "outputId": "26794730-2db4-47a5-ad8e-0b712c5663de"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L53LyOwKfqk8"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "r1nSkjAMfqk9",
        "outputId": "a6a6b736-b6cf-44d5-966a-8b8e4e039fb4"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Google Colab/0_data/SUBJ/SUBJ.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>smart and alert , thirteen conversations about...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>color , musical bounce and warm seas lapping o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it is not a mass market entertainment but an u...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a light hearted french film about the spiritua...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my wife is an actress has its moments in looki...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>in the end , they discover that balance in lif...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>enter the beautiful and mysterious secret agen...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>after listening to a missionary from china spe...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>looking for a short cut to fame , glass concoc...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label  split\n",
              "0     smart and alert , thirteen conversations about...      0  train\n",
              "1     color , musical bounce and warm seas lapping o...      0  train\n",
              "2     it is not a mass market entertainment but an u...      0  train\n",
              "3     a light hearted french film about the spiritua...      0  train\n",
              "4     my wife is an actress has its moments in looki...      0  train\n",
              "...                                                 ...    ...    ...\n",
              "9995  in the end , they discover that balance in lif...      1  train\n",
              "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
              "9997  enter the beautiful and mysterious secret agen...      1  train\n",
              "9998  after listening to a missionary from china spe...      1  train\n",
              "9999  looking for a short cut to fame , glass concoc...      1  train\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2r92_h4fqk-",
        "outputId": "aaed5972-4101-462b-d74c-bc25140aa3fa"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10000 non-null  object\n",
            " 1   label     10000 non-null  int64 \n",
            " 2   split     10000 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 234.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "CBWeU78tfqk-",
        "outputId": "782c7ce7-d4aa-46af-af9c-f78b84824bb7"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          5000   5000\n",
              "1          5000   5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Szp2Grfqk_"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-5zUXp1_fqk_",
        "outputId": "856a9ea1-e59a-411b-8e77-74adf1e53684"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'smart and alert , thirteen conversations about one thing is a small gem .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyiGh8e6fqlA"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I96hU8fgfqlB"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6FJgGuvfqlB"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAAELi_AfqlC",
        "outputId": "ef20d42b-7776-474c-fff8-65ee950dedfc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
            "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
            "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
            "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
            "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6JukAFXfqlD",
        "outputId": "32cc73ff-eec6-4f5b-d7fd-1b2412814aa5"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "a 3\n",
            "and 4\n",
            "of 5\n",
            "to 6\n",
            "in 7\n",
            "is 8\n",
            "'s 9\n",
            "it 10\n",
            "21324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XkM9hj2fqlD"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCiO4kiofqlE"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-FWT0wdfqlE"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEQawok9fqlF",
        "outputId": "1e3b6ae7-1ea5-426b-da4f-960b1b465b1a"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 585,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7j_kqwrfqlF"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0WuuutofqlG"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_UNmX73zfqlG",
        "outputId": "8b26d7a3-b2af-4536-ab19-9833adc841de"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 130s 397ms/step - loss: 0.4078 - accuracy: 0.8065 - val_loss: 0.1930 - val_accuracy: 0.9210\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 96s 340ms/step - loss: 0.0618 - accuracy: 0.9814 - val_loss: 0.2342 - val_accuracy: 0.9110\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.4000 - val_accuracy: 0.9060\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 113s 401ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.5076 - val_accuracy: 0.8960\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 109s 385ms/step - loss: 0.0127 - accuracy: 0.9953 - val_loss: 0.5245 - val_accuracy: 0.8930\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 114s 405ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.6017 - val_accuracy: 0.9060\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.10000038146973\n",
            "Training 2: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 170s 533ms/step - loss: 0.4113 - accuracy: 0.8022 - val_loss: 0.2386 - val_accuracy: 0.9090\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 160s 568ms/step - loss: 0.0614 - accuracy: 0.9800 - val_loss: 0.2324 - val_accuracy: 0.9100\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 178s 632ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.3529 - val_accuracy: 0.9100\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 171s 606ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.4162 - val_accuracy: 0.9010\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 167s 594ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.4261 - val_accuracy: 0.9120\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 166s 588ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.4449 - val_accuracy: 0.9090\n",
            "Epoch 7/15\n",
            "282/282 [==============================] - 178s 632ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.4160 - val_accuracy: 0.9040\n",
            "Epoch 8/15\n",
            "282/282 [==============================] - 178s 631ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.6138 - val_accuracy: 0.8990\n",
            "Epoch 9/15\n",
            "282/282 [==============================] - 176s 625ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.6523 - val_accuracy: 0.8960\n",
            "Epoch 10/15\n",
            "282/282 [==============================] - 169s 597ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.4325 - val_accuracy: 0.9000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 91.20000004768372\n",
            "Training 3: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 195s 624ms/step - loss: 0.3996 - accuracy: 0.8057 - val_loss: 0.2171 - val_accuracy: 0.9180\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 175s 621ms/step - loss: 0.0644 - accuracy: 0.9809 - val_loss: 0.2343 - val_accuracy: 0.9020\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 200s 709ms/step - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.3329 - val_accuracy: 0.9020\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 202s 717ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.3881 - val_accuracy: 0.9000\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 204s 724ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.4302 - val_accuracy: 0.9090\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 206s 729ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.4703 - val_accuracy: 0.9000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.79999828338623\n",
            "Training 4: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 231s 733ms/step - loss: 0.4020 - accuracy: 0.8121 - val_loss: 0.2241 - val_accuracy: 0.9140\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 217s 770ms/step - loss: 0.0600 - accuracy: 0.9812 - val_loss: 0.2922 - val_accuracy: 0.9080\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 268s 951ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.3681 - val_accuracy: 0.8930\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 268s 950ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.4110 - val_accuracy: 0.9010\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 265s 941ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.5283 - val_accuracy: 0.8900\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 265s 938ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.6074 - val_accuracy: 0.8910\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.39999747276306\n",
            "Training 5: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 243s 776ms/step - loss: 0.4052 - accuracy: 0.8133 - val_loss: 0.2251 - val_accuracy: 0.9190\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 227s 806ms/step - loss: 0.0638 - accuracy: 0.9798 - val_loss: 0.3034 - val_accuracy: 0.9020\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 236s 836ms/step - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.3234 - val_accuracy: 0.8980\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 235s 832ms/step - loss: 0.0083 - accuracy: 0.9979 - val_loss: 0.3728 - val_accuracy: 0.9050\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 236s 837ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.4304 - val_accuracy: 0.8930\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 237s 839ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.5679 - val_accuracy: 0.8930\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.90000295639038\n",
            "Training 6: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 241s 771ms/step - loss: 0.4035 - accuracy: 0.8002 - val_loss: 0.1897 - val_accuracy: 0.9310\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 186s 658ms/step - loss: 0.0677 - accuracy: 0.9775 - val_loss: 0.2345 - val_accuracy: 0.9110\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 214s 759ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.3264 - val_accuracy: 0.9220\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 211s 748ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.4088 - val_accuracy: 0.9210\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 211s 749ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.3710 - val_accuracy: 0.9210\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 209s 741ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.4993 - val_accuracy: 0.9060\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 93.09999942779541\n",
            "Training 7: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 260s 835ms/step - loss: 0.4088 - accuracy: 0.7990 - val_loss: 0.1954 - val_accuracy: 0.9250\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 222s 786ms/step - loss: 0.0660 - accuracy: 0.9798 - val_loss: 0.2221 - val_accuracy: 0.9080\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 275s 976ms/step - loss: 0.0218 - accuracy: 0.9943 - val_loss: 0.2992 - val_accuracy: 0.9100\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 273s 969ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.3181 - val_accuracy: 0.9220\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 273s 967ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.3233 - val_accuracy: 0.9120\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 271s 961ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.4045 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.5000011920929\n",
            "Training 8: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 283s 921ms/step - loss: 0.4128 - accuracy: 0.7985 - val_loss: 0.2412 - val_accuracy: 0.9010\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 231s 818ms/step - loss: 0.0557 - accuracy: 0.9835 - val_loss: 0.3165 - val_accuracy: 0.9020\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 268s 951ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.4410 - val_accuracy: 0.8910\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 267s 948ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 0.5342 - val_accuracy: 0.8910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/15\n",
            "282/282 [==============================] - 266s 945ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.5366 - val_accuracy: 0.8840\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 265s 940ms/step - loss: 0.0052 - accuracy: 0.9979 - val_loss: 0.5926 - val_accuracy: 0.8860\n",
            "Epoch 7/15\n",
            "282/282 [==============================] - 265s 940ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.6639 - val_accuracy: 0.8920\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 90.20000100135803\n",
            "Training 9: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 190s 596ms/step - loss: 0.4161 - accuracy: 0.8121 - val_loss: 0.1984 - val_accuracy: 0.9200\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 169s 600ms/step - loss: 0.0561 - accuracy: 0.9829 - val_loss: 0.2353 - val_accuracy: 0.9080\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 204s 723ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.3332 - val_accuracy: 0.9070\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 204s 723ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.4130 - val_accuracy: 0.8840\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 204s 724ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.5258 - val_accuracy: 0.9100\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 205s 726ms/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.4146 - val_accuracy: 0.9040\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.00000166893005\n",
            "Training 10: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 245s 782ms/step - loss: 0.4106 - accuracy: 0.8159 - val_loss: 0.1811 - val_accuracy: 0.9260\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 186s 659ms/step - loss: 0.0586 - accuracy: 0.9803 - val_loss: 0.2264 - val_accuracy: 0.9040\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 204s 722ms/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 0.2715 - val_accuracy: 0.9090\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 205s 726ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.3183 - val_accuracy: 0.9190\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 206s 731ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.3222 - val_accuracy: 0.8990\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 207s 733ms/step - loss: 0.0058 - accuracy: 0.9977 - val_loss: 0.3876 - val_accuracy: 0.9080\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.59999990463257\n",
            "\n",
            "   acc1  acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
            "0  92.1  91.2  91.799998  91.399997  91.900003  93.099999  92.500001   \n",
            "\n",
            "        acc8       acc9  acc10    AVG  \n",
            "0  90.200001  92.000002   92.6  91.88  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUzQvwvXfqlH"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV305bEmfqlJ",
        "outputId": "390ce3cd-ddd5-464a-b7d0-adb6a49e3e30"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>92.1</td>\n",
              "      <td>91.2</td>\n",
              "      <td>91.799998</td>\n",
              "      <td>91.399997</td>\n",
              "      <td>91.900003</td>\n",
              "      <td>93.099999</td>\n",
              "      <td>92.500001</td>\n",
              "      <td>90.200001</td>\n",
              "      <td>92.000002</td>\n",
              "      <td>92.6</td>\n",
              "      <td>91.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   acc1  acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
              "0  92.1  91.2  91.799998  91.399997  91.900003  93.099999  92.500001   \n",
              "\n",
              "        acc8       acc9  acc10    AVG  \n",
              "0  90.200001  92.000002   92.6  91.88  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWUhu0PyfqlJ"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('LSTM_SUBJ_v2.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fgJtRb7fqlK"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSVPvuJYfqlK"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhp9R-s3fqlL"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EyflDn1fqlL"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Google Colab/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me0qH2wbfqlL",
        "outputId": "44016f15-ca88-4069-ff6c-4e5a6fca30db"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC-enJ-RfqlM"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5bUFLHNfqlM"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9ujUTGjfqlN",
        "outputId": "d6920589-fd80-4bbe-9d4b-b983c10edb26"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY5w1oPQfqlN"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJa7T8ACfqlN",
        "outputId": "a0641c0f-cc3e-4445-fc61-aaf52922f255"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlhl_T47fqlO"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUOd74E2fqlO",
        "outputId": "701d82c9-f29f-4e0b-cb6e-ba6c6b864005"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNcwbanyfqlP"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icO82s29fqlP"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQ9KoIVfqlP",
        "outputId": "3ee77f8a-8eb8-4f89-835c-275f62d2bbb8"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 285,825\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZwfYtefqlQ"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BDzVuUfqlQ"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=6, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPx8HMjWfqlQ",
        "outputId": "8ac8fb83-ac58-4850-de05-257ccbd8f0f6"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 130s 411ms/step - loss: 0.3799 - accuracy: 0.8207 - val_loss: 0.2816 - val_accuracy: 0.8800\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.2083 - accuracy: 0.9172 - val_loss: 0.2493 - val_accuracy: 0.8970\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 108s 383ms/step - loss: 0.1731 - accuracy: 0.9340 - val_loss: 0.2360 - val_accuracy: 0.9020\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 109s 387ms/step - loss: 0.1455 - accuracy: 0.9422 - val_loss: 0.2474 - val_accuracy: 0.9080\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 109s 388ms/step - loss: 0.1231 - accuracy: 0.9529 - val_loss: 0.2421 - val_accuracy: 0.9070\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0989 - accuracy: 0.9627 - val_loss: 0.2497 - val_accuracy: 0.9030\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.0761 - accuracy: 0.9713 - val_loss: 0.2928 - val_accuracy: 0.9060\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 109s 388ms/step - loss: 0.0523 - accuracy: 0.9799 - val_loss: 0.2871 - val_accuracy: 0.9060\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 110s 389ms/step - loss: 0.0446 - accuracy: 0.9851 - val_loss: 0.3300 - val_accuracy: 0.9080\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0256 - accuracy: 0.9943 - val_loss: 0.3566 - val_accuracy: 0.9070\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 90.79999923706055\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 131s 417ms/step - loss: 0.3617 - accuracy: 0.8306 - val_loss: 0.2331 - val_accuracy: 0.8990\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 116s 412ms/step - loss: 0.2136 - accuracy: 0.9187 - val_loss: 0.1823 - val_accuracy: 0.9290\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.1843 - accuracy: 0.9258 - val_loss: 0.2231 - val_accuracy: 0.9110\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.1598 - accuracy: 0.9386 - val_loss: 0.1710 - val_accuracy: 0.9320\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 115s 408ms/step - loss: 0.1227 - accuracy: 0.9529 - val_loss: 0.1861 - val_accuracy: 0.9290\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 115s 408ms/step - loss: 0.1024 - accuracy: 0.9636 - val_loss: 0.1729 - val_accuracy: 0.9310\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 114s 404ms/step - loss: 0.0818 - accuracy: 0.9700 - val_loss: 0.1997 - val_accuracy: 0.9270\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 115s 408ms/step - loss: 0.0620 - accuracy: 0.9798 - val_loss: 0.2374 - val_accuracy: 0.9220\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 115s 408ms/step - loss: 0.0536 - accuracy: 0.9813 - val_loss: 0.2322 - val_accuracy: 0.9320\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 115s 408ms/step - loss: 0.0306 - accuracy: 0.9905 - val_loss: 0.3158 - val_accuracy: 0.9240\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 93.19999814033508\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 133s 418ms/step - loss: 0.3725 - accuracy: 0.8430 - val_loss: 0.2636 - val_accuracy: 0.8890\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 116s 409ms/step - loss: 0.2040 - accuracy: 0.9169 - val_loss: 0.2610 - val_accuracy: 0.9030\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 112s 398ms/step - loss: 0.1819 - accuracy: 0.9259 - val_loss: 0.2517 - val_accuracy: 0.8930\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 113s 399ms/step - loss: 0.1511 - accuracy: 0.9422 - val_loss: 0.2289 - val_accuracy: 0.9020\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 113s 401ms/step - loss: 0.1229 - accuracy: 0.9544 - val_loss: 0.2824 - val_accuracy: 0.8940\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 115s 407ms/step - loss: 0.0997 - accuracy: 0.9643 - val_loss: 0.2470 - val_accuracy: 0.9120\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.0795 - accuracy: 0.9716 - val_loss: 0.3277 - val_accuracy: 0.8990\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 112s 399ms/step - loss: 0.0772 - accuracy: 0.9703 - val_loss: 0.3891 - val_accuracy: 0.8750\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.0454 - accuracy: 0.9825 - val_loss: 0.3971 - val_accuracy: 0.8880\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 112s 397ms/step - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.3409 - val_accuracy: 0.9080\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0195 - accuracy: 0.9931 - val_loss: 0.3851 - val_accuracy: 0.9030\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.0231 - accuracy: 0.9908 - val_loss: 0.3944 - val_accuracy: 0.9030\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 91.20000004768372\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 137s 421ms/step - loss: 0.3749 - accuracy: 0.8195 - val_loss: 0.2534 - val_accuracy: 0.9020\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.2036 - accuracy: 0.9170 - val_loss: 0.2412 - val_accuracy: 0.9130\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.1793 - accuracy: 0.9284 - val_loss: 0.2574 - val_accuracy: 0.9060\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.1413 - accuracy: 0.9438 - val_loss: 0.2236 - val_accuracy: 0.9160\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.1229 - accuracy: 0.9513 - val_loss: 0.2481 - val_accuracy: 0.9130\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0858 - accuracy: 0.9685 - val_loss: 0.2660 - val_accuracy: 0.9140\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 115s 410ms/step - loss: 0.0856 - accuracy: 0.9688 - val_loss: 0.3509 - val_accuracy: 0.9040\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.0577 - accuracy: 0.9777 - val_loss: 0.3103 - val_accuracy: 0.9110\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.0393 - accuracy: 0.9873 - val_loss: 0.3089 - val_accuracy: 0.8980\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.0318 - accuracy: 0.9893 - val_loss: 0.3347 - val_accuracy: 0.9160\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 91.60000085830688\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 130s 411ms/step - loss: 0.3756 - accuracy: 0.8365 - val_loss: 0.2181 - val_accuracy: 0.9100\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.2134 - accuracy: 0.9171 - val_loss: 0.2189 - val_accuracy: 0.9140\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.1815 - accuracy: 0.9287 - val_loss: 0.1876 - val_accuracy: 0.9220\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.1433 - accuracy: 0.9445 - val_loss: 0.1988 - val_accuracy: 0.9220\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 110s 390ms/step - loss: 0.1223 - accuracy: 0.9552 - val_loss: 0.1806 - val_accuracy: 0.9300\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0987 - accuracy: 0.9647 - val_loss: 0.1766 - val_accuracy: 0.9310\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0810 - accuracy: 0.9715 - val_loss: 0.2118 - val_accuracy: 0.9220\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 110s 392ms/step - loss: 0.0617 - accuracy: 0.9782 - val_loss: 0.2135 - val_accuracy: 0.9230\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0531 - accuracy: 0.9810 - val_loss: 0.2236 - val_accuracy: 0.9250\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0248 - accuracy: 0.9904 - val_loss: 0.2672 - val_accuracy: 0.9240\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0345 - accuracy: 0.9874 - val_loss: 0.2312 - val_accuracy: 0.9380\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.2428 - val_accuracy: 0.9430\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 110s 392ms/step - loss: 0.0128 - accuracy: 0.9970 - val_loss: 0.2526 - val_accuracy: 0.9320\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.0201 - accuracy: 0.9948 - val_loss: 0.3457 - val_accuracy: 0.9200\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.3008 - val_accuracy: 0.9310\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.3455 - val_accuracy: 0.9290\n",
            "Epoch 17/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.3194 - val_accuracy: 0.9330\n",
            "Epoch 18/100\n",
            "282/282 [==============================] - 112s 397ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 0.4081 - val_accuracy: 0.9250\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00018: early stopping\n",
            "Test Accuracy: 94.30000185966492\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 129s 409ms/step - loss: 0.3640 - accuracy: 0.8467 - val_loss: 0.2880 - val_accuracy: 0.8880\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 110s 389ms/step - loss: 0.2032 - accuracy: 0.9218 - val_loss: 0.2415 - val_accuracy: 0.8950\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 110s 388ms/step - loss: 0.1757 - accuracy: 0.9301 - val_loss: 0.2667 - val_accuracy: 0.9030\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 111s 393ms/step - loss: 0.1612 - accuracy: 0.9354 - val_loss: 0.2942 - val_accuracy: 0.8990\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 110s 389ms/step - loss: 0.1276 - accuracy: 0.9507 - val_loss: 0.2427 - val_accuracy: 0.9130\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.1029 - accuracy: 0.9639 - val_loss: 0.2744 - val_accuracy: 0.9030\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0777 - accuracy: 0.9714 - val_loss: 0.2485 - val_accuracy: 0.9180\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0612 - accuracy: 0.9766 - val_loss: 0.2919 - val_accuracy: 0.9200\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0562 - accuracy: 0.9779 - val_loss: 0.3566 - val_accuracy: 0.9100\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 110s 392ms/step - loss: 0.0364 - accuracy: 0.9869 - val_loss: 0.3403 - val_accuracy: 0.9170\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0277 - accuracy: 0.9906 - val_loss: 0.3795 - val_accuracy: 0.9050\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0240 - accuracy: 0.9920 - val_loss: 0.4346 - val_accuracy: 0.9120\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.0172 - accuracy: 0.9950 - val_loss: 0.4237 - val_accuracy: 0.9090\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0092 - accuracy: 0.9979 - val_loss: 0.4110 - val_accuracy: 0.9180\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 92.00000166893005\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 133s 407ms/step - loss: 0.3613 - accuracy: 0.8339 - val_loss: 0.2340 - val_accuracy: 0.8980\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 109s 385ms/step - loss: 0.2245 - accuracy: 0.9139 - val_loss: 0.2031 - val_accuracy: 0.9120\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 109s 387ms/step - loss: 0.1794 - accuracy: 0.9293 - val_loss: 0.1930 - val_accuracy: 0.9210\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 109s 386ms/step - loss: 0.1533 - accuracy: 0.9435 - val_loss: 0.1899 - val_accuracy: 0.9290\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 109s 385ms/step - loss: 0.1375 - accuracy: 0.9466 - val_loss: 0.1899 - val_accuracy: 0.9250\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 110s 389ms/step - loss: 0.1008 - accuracy: 0.9633 - val_loss: 0.2508 - val_accuracy: 0.9130\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 110s 390ms/step - loss: 0.0896 - accuracy: 0.9679 - val_loss: 0.1996 - val_accuracy: 0.9200\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 110s 390ms/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 0.2612 - val_accuracy: 0.9230\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 110s 391ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 0.2519 - val_accuracy: 0.9080\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0333 - accuracy: 0.9896 - val_loss: 0.2582 - val_accuracy: 0.9100\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 92.90000200271606\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 99s 304ms/step - loss: 0.3722 - accuracy: 0.8367 - val_loss: 0.2353 - val_accuracy: 0.8960\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 80s 285ms/step - loss: 0.2189 - accuracy: 0.9155 - val_loss: 0.2005 - val_accuracy: 0.9150\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 81s 286ms/step - loss: 0.1872 - accuracy: 0.9261 - val_loss: 0.1909 - val_accuracy: 0.9140\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 81s 287ms/step - loss: 0.1590 - accuracy: 0.9412 - val_loss: 0.2146 - val_accuracy: 0.9030\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 80s 285ms/step - loss: 0.1318 - accuracy: 0.9492 - val_loss: 0.2065 - val_accuracy: 0.9130\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 81s 286ms/step - loss: 0.1034 - accuracy: 0.9636 - val_loss: 0.2041 - val_accuracy: 0.9190\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 81s 287ms/step - loss: 0.0765 - accuracy: 0.9737 - val_loss: 0.2624 - val_accuracy: 0.9100\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 81s 287ms/step - loss: 0.0580 - accuracy: 0.9807 - val_loss: 0.2641 - val_accuracy: 0.9110\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 81s 288ms/step - loss: 0.0454 - accuracy: 0.9842 - val_loss: 0.2955 - val_accuracy: 0.9140\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 81s 287ms/step - loss: 0.0316 - accuracy: 0.9903 - val_loss: 0.3781 - val_accuracy: 0.9030\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 81s 288ms/step - loss: 0.0330 - accuracy: 0.9863 - val_loss: 0.4049 - val_accuracy: 0.8970\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 81s 288ms/step - loss: 0.0425 - accuracy: 0.9844 - val_loss: 0.3768 - val_accuracy: 0.9180\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 91.90000295639038\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 134s 425ms/step - loss: 0.3529 - accuracy: 0.8488 - val_loss: 0.2683 - val_accuracy: 0.8960\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 113s 402ms/step - loss: 0.2080 - accuracy: 0.9212 - val_loss: 0.2526 - val_accuracy: 0.9050\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.1782 - accuracy: 0.9260 - val_loss: 0.2719 - val_accuracy: 0.8930\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.1433 - accuracy: 0.9455 - val_loss: 0.2686 - val_accuracy: 0.9150\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 111s 395ms/step - loss: 0.1150 - accuracy: 0.9542 - val_loss: 0.2718 - val_accuracy: 0.9100\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 111s 394ms/step - loss: 0.0991 - accuracy: 0.9659 - val_loss: 0.2789 - val_accuracy: 0.9110\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0755 - accuracy: 0.9700 - val_loss: 0.2610 - val_accuracy: 0.9130\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0562 - accuracy: 0.9790 - val_loss: 0.3416 - val_accuracy: 0.9090\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 112s 398ms/step - loss: 0.0450 - accuracy: 0.9841 - val_loss: 0.3218 - val_accuracy: 0.9160\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 113s 399ms/step - loss: 0.0319 - accuracy: 0.9888 - val_loss: 0.3842 - val_accuracy: 0.8930\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 112s 399ms/step - loss: 0.0239 - accuracy: 0.9935 - val_loss: 0.3992 - val_accuracy: 0.9090\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 113s 399ms/step - loss: 0.0325 - accuracy: 0.9869 - val_loss: 0.4144 - val_accuracy: 0.9110\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.0237 - accuracy: 0.9908 - val_loss: 0.4718 - val_accuracy: 0.9160\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 114s 403ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.4925 - val_accuracy: 0.9100\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 113s 401ms/step - loss: 0.0165 - accuracy: 0.9944 - val_loss: 0.4273 - val_accuracy: 0.9090\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 91.60000085830688\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 132s 418ms/step - loss: 0.3726 - accuracy: 0.8322 - val_loss: 0.2232 - val_accuracy: 0.9120\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.2188 - accuracy: 0.9112 - val_loss: 0.2383 - val_accuracy: 0.9020\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.1841 - accuracy: 0.9281 - val_loss: 0.2026 - val_accuracy: 0.9200\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.1505 - accuracy: 0.9435 - val_loss: 0.1956 - val_accuracy: 0.9160\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 112s 398ms/step - loss: 0.1302 - accuracy: 0.9509 - val_loss: 0.2135 - val_accuracy: 0.9120\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.1059 - accuracy: 0.9623 - val_loss: 0.2174 - val_accuracy: 0.9140\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 112s 399ms/step - loss: 0.0849 - accuracy: 0.9668 - val_loss: 0.2466 - val_accuracy: 0.9110\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 112s 397ms/step - loss: 0.0589 - accuracy: 0.9795 - val_loss: 0.2943 - val_accuracy: 0.8970\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 112s 399ms/step - loss: 0.0437 - accuracy: 0.9854 - val_loss: 0.2812 - val_accuracy: 0.9120\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 92.00000166893005\n",
            "\n",
            "        acc1       acc2  acc3  ...       acc9      acc10        AVG\n",
            "0  90.799999  93.199998  91.2  ...  91.600001  92.000002  92.150001\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6heodHtBfqlR"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "sqJiDvWefqlS",
        "outputId": "4e743a81-e3e8-4bd9-c13a-4d6d52c17a77"
      },
      "source": [
        "record2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90.799999</td>\n",
              "      <td>93.199998</td>\n",
              "      <td>91.2</td>\n",
              "      <td>91.600001</td>\n",
              "      <td>94.300002</td>\n",
              "      <td>92.000002</td>\n",
              "      <td>92.900002</td>\n",
              "      <td>91.900003</td>\n",
              "      <td>91.600001</td>\n",
              "      <td>92.000002</td>\n",
              "      <td>92.150001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2  acc3  ...       acc9      acc10        AVG\n",
              "0  90.799999  93.199998  91.2  ...  91.600001  92.000002  92.150001\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahsr4wwqfqlS"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('LSTM_SUBJ_v2_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHa1XeBTfqlS"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqzRLYDufqlT"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kesQ5kAcfqlT"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTyLX7HjfqlT"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeV8wpv7fqlU",
        "outputId": "cfb46cef-6a3b-4526-ebdd-6698d5221608"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_22 (Bidirectio (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_23 (Bidirectio (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 585,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZDhmcJUfqlU"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV2t16fafqlU"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUgn--1XfqlV",
        "outputId": "34ead25e-b1d5-494d-a851-b7c161af6765"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record3)\n",
        "print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 168s 544ms/step - loss: 0.3393 - accuracy: 0.8420 - val_loss: 0.1669 - val_accuracy: 0.9310\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0765 - accuracy: 0.9747 - val_loss: 0.1863 - val_accuracy: 0.9340\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 148s 526ms/step - loss: 0.0245 - accuracy: 0.9931 - val_loss: 0.2646 - val_accuracy: 0.9190\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 150s 531ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.3202 - val_accuracy: 0.9280\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 150s 531ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.2671 - val_accuracy: 0.9240\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.4702 - val_accuracy: 0.9180\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.4825 - val_accuracy: 0.9230\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 147s 522ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.4823 - val_accuracy: 0.9080\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 146s 519ms/step - loss: 8.1186e-04 - accuracy: 1.0000 - val_loss: 0.4929 - val_accuracy: 0.9200\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 146s 518ms/step - loss: 6.5625e-05 - accuracy: 1.0000 - val_loss: 0.5757 - val_accuracy: 0.9200\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 93.4000015258789\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 164s 532ms/step - loss: 0.3429 - accuracy: 0.8509 - val_loss: 0.2281 - val_accuracy: 0.9030\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 146s 518ms/step - loss: 0.0797 - accuracy: 0.9732 - val_loss: 0.3059 - val_accuracy: 0.9140\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 146s 518ms/step - loss: 0.0274 - accuracy: 0.9911 - val_loss: 0.3584 - val_accuracy: 0.9080\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 145s 515ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.4697 - val_accuracy: 0.9130\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 146s 519ms/step - loss: 0.0050 - accuracy: 0.9981 - val_loss: 0.4512 - val_accuracy: 0.9090\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 147s 521ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.4531 - val_accuracy: 0.9070\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.7483 - val_accuracy: 0.8980\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 147s 521ms/step - loss: 6.8720e-04 - accuracy: 0.9998 - val_loss: 0.5471 - val_accuracy: 0.8980\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.7802 - val_accuracy: 0.9010\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 1.4000e-04 - accuracy: 1.0000 - val_loss: 0.8503 - val_accuracy: 0.8960\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 91.39999747276306\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 165s 537ms/step - loss: 0.3470 - accuracy: 0.8419 - val_loss: 0.1870 - val_accuracy: 0.9200\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 146s 518ms/step - loss: 0.0808 - accuracy: 0.9741 - val_loss: 0.1986 - val_accuracy: 0.9270\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 146s 519ms/step - loss: 0.0234 - accuracy: 0.9943 - val_loss: 0.3575 - val_accuracy: 0.9140\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 145s 514ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.3950 - val_accuracy: 0.9060\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 146s 517ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.3950 - val_accuracy: 0.9120\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 146s 516ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.4678 - val_accuracy: 0.9010\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.4642 - val_accuracy: 0.9060\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 146s 519ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.5390 - val_accuracy: 0.9140\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 146s 518ms/step - loss: 1.0526e-04 - accuracy: 1.0000 - val_loss: 0.6252 - val_accuracy: 0.9000\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 146s 519ms/step - loss: 0.0078 - accuracy: 0.9972 - val_loss: 0.5788 - val_accuracy: 0.8970\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 92.69999861717224\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 139s 418ms/step - loss: 0.3380 - accuracy: 0.8479 - val_loss: 0.2159 - val_accuracy: 0.9180\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 112s 397ms/step - loss: 0.0827 - accuracy: 0.9733 - val_loss: 0.2411 - val_accuracy: 0.9080\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 112s 396ms/step - loss: 0.0256 - accuracy: 0.9913 - val_loss: 0.3110 - val_accuracy: 0.8880\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 112s 399ms/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 0.4008 - val_accuracy: 0.9030\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 112s 397ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.3531 - val_accuracy: 0.9100\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.4378 - val_accuracy: 0.9060\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 3.7752e-04 - accuracy: 1.0000 - val_loss: 0.5673 - val_accuracy: 0.9060\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 1.0221e-04 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 0.9050\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 113s 401ms/step - loss: 4.7545e-05 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 0.9060\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 91.79999828338623\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 167s 544ms/step - loss: 0.3452 - accuracy: 0.8476 - val_loss: 0.1853 - val_accuracy: 0.9320\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 153s 541ms/step - loss: 0.0783 - accuracy: 0.9757 - val_loss: 0.2055 - val_accuracy: 0.9310\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0257 - accuracy: 0.9937 - val_loss: 0.2662 - val_accuracy: 0.9240\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.3442 - val_accuracy: 0.9230\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 148s 524ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.4003 - val_accuracy: 0.9180\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.3343 - val_accuracy: 0.9180\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 149s 527ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.3625 - val_accuracy: 0.9290\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.4517 - val_accuracy: 0.9200\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.3444 - val_accuracy: 0.9300\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 93.19999814033508\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 170s 549ms/step - loss: 0.3384 - accuracy: 0.8415 - val_loss: 0.1988 - val_accuracy: 0.9220\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0753 - accuracy: 0.9729 - val_loss: 0.2795 - val_accuracy: 0.9060\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 150s 532ms/step - loss: 0.0230 - accuracy: 0.9933 - val_loss: 0.3273 - val_accuracy: 0.9070\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 0.0068 - accuracy: 0.9974 - val_loss: 0.4186 - val_accuracy: 0.9030\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 150s 530ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.5189 - val_accuracy: 0.9030\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.6853 - val_accuracy: 0.8940\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 148s 524ms/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 0.4560 - val_accuracy: 0.9080\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.5359 - val_accuracy: 0.9010\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 147s 522ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.6179 - val_accuracy: 0.8990\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 92.1999990940094\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 172s 537ms/step - loss: 0.3485 - accuracy: 0.8322 - val_loss: 0.1932 - val_accuracy: 0.9240\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 146s 518ms/step - loss: 0.0812 - accuracy: 0.9695 - val_loss: 0.2333 - val_accuracy: 0.9200\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 147s 520ms/step - loss: 0.0177 - accuracy: 0.9950 - val_loss: 0.3290 - val_accuracy: 0.9120\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 147s 523ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.3074 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 154s 544ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.4832 - val_accuracy: 0.9090\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.3464 - val_accuracy: 0.9140\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.4665 - val_accuracy: 0.9000\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.5387 - val_accuracy: 0.9140\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.5304 - val_accuracy: 0.9190\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 92.40000247955322\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 169s 549ms/step - loss: 0.3532 - accuracy: 0.8317 - val_loss: 0.1892 - val_accuracy: 0.9250\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0829 - accuracy: 0.9737 - val_loss: 0.2157 - val_accuracy: 0.9240\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 149s 527ms/step - loss: 0.0260 - accuracy: 0.9926 - val_loss: 0.2990 - val_accuracy: 0.9280\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 148s 526ms/step - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.3315 - val_accuracy: 0.9180\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.3660 - val_accuracy: 0.9170\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 148s 526ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.5662 - val_accuracy: 0.8930\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.4571 - val_accuracy: 0.9150\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 148s 526ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.5063 - val_accuracy: 0.9190\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 0.0010 - accuracy: 0.9994 - val_loss: 0.5073 - val_accuracy: 0.9130\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 149s 527ms/step - loss: 1.1298e-04 - accuracy: 1.0000 - val_loss: 0.5503 - val_accuracy: 0.9130\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 149s 527ms/step - loss: 3.2031e-05 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 92.79999732971191\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 175s 568ms/step - loss: 0.3372 - accuracy: 0.8620 - val_loss: 0.2207 - val_accuracy: 0.9110\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0792 - accuracy: 0.9745 - val_loss: 0.2691 - val_accuracy: 0.9080\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.3009 - val_accuracy: 0.9060\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 151s 535ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.4061 - val_accuracy: 0.9100\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.5139 - val_accuracy: 0.9040\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 150s 531ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.4661 - val_accuracy: 0.9020\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.4257 - val_accuracy: 0.9090\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 149s 529ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.4277 - val_accuracy: 0.8950\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 150s 531ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.5630 - val_accuracy: 0.9000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 91.10000133514404\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 166s 540ms/step - loss: 0.3590 - accuracy: 0.8232 - val_loss: 0.1890 - val_accuracy: 0.9350\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 148s 525ms/step - loss: 0.0826 - accuracy: 0.9711 - val_loss: 0.1969 - val_accuracy: 0.9350\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 150s 533ms/step - loss: 0.0172 - accuracy: 0.9955 - val_loss: 0.2926 - val_accuracy: 0.9240\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.3348 - val_accuracy: 0.9280\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.3958 - val_accuracy: 0.9250\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 149s 528ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3779 - val_accuracy: 0.9240\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 148s 527ms/step - loss: 5.3607e-04 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.9190\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 149s 530ms/step - loss: 9.7584e-05 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.9210\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 149s 527ms/step - loss: 3.3834e-05 - accuracy: 1.0000 - val_loss: 0.5471 - val_accuracy: 0.9210\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 93.50000023841858\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9  acc10        AVG\n",
            "0  93.400002  91.399997  92.699999  ...  91.100001   93.5  92.449999\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhW1oHRsfqlV"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "wu8iz0TCfqlV",
        "outputId": "762f0f56-bb10-428e-af8e-2f6ad5d7be8f"
      },
      "source": [
        "record3"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>93.400002</td>\n",
              "      <td>91.399997</td>\n",
              "      <td>92.699999</td>\n",
              "      <td>91.799998</td>\n",
              "      <td>93.199998</td>\n",
              "      <td>92.199999</td>\n",
              "      <td>92.400002</td>\n",
              "      <td>92.799997</td>\n",
              "      <td>91.100001</td>\n",
              "      <td>93.5</td>\n",
              "      <td>92.449999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9  acc10        AVG\n",
              "0  93.400002  91.399997  92.699999  ...  91.100001   93.5  92.449999\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5zUyEEnfqlW"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('LSTM_SUBJ_v2_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}