{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with SUBJ Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smart and alert , thirteen conversations about...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>color , musical bounce and warm seas lapping o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is not a mass market entertainment but an u...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a light hearted french film about the spiritua...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my wife is an actress has its moments in looki...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>in the end , they discover that balance in lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>enter the beautiful and mysterious secret agen...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>after listening to a missionary from china spe...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>looking for a short cut to fame , glass concoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     smart and alert , thirteen conversations about...      0  train\n",
       "1     color , musical bounce and warm seas lapping o...      0  train\n",
       "2     it is not a mass market entertainment but an u...      0  train\n",
       "3     a light hearted french film about the spiritua...      0  train\n",
       "4     my wife is an actress has its moments in looki...      0  train\n",
       "...                                                 ...    ...    ...\n",
       "9995  in the end , they discover that balance in lif...      1  train\n",
       "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
       "9997  enter the beautiful and mysterious secret agen...      1  train\n",
       "9998  after listening to a missionary from china spe...      1  train\n",
       "9999  looking for a short cut to fame , glass concoc...      1  train\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/SUBJ/SUBJ.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10000 non-null  object\n",
      " 1   label     10000 non-null  int32 \n",
      " 2   split     10000 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          5000   5000\n",
       "1          5000   5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smart and alert , thirteen conversations about one thing is a small gem .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
      "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
      "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
      "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
      "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "a 3\n",
      "and 4\n",
      "of 5\n",
      "to 6\n",
      "in 7\n",
      "is 8\n",
      "'s 9\n",
      "it 10\n",
      "21324\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 130s 397ms/step - loss: 0.4078 - accuracy: 0.8065 - val_loss: 0.1930 - val_accuracy: 0.9210\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 96s 340ms/step - loss: 0.0618 - accuracy: 0.9814 - val_loss: 0.2342 - val_accuracy: 0.9110\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 111s 394ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.4000 - val_accuracy: 0.9060\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 113s 401ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.5076 - val_accuracy: 0.8960\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 109s 385ms/step - loss: 0.0127 - accuracy: 0.9953 - val_loss: 0.5245 - val_accuracy: 0.8930\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 114s 405ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.6017 - val_accuracy: 0.9060\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.10000038146973\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 170s 533ms/step - loss: 0.4113 - accuracy: 0.8022 - val_loss: 0.2386 - val_accuracy: 0.9090\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 160s 568ms/step - loss: 0.0614 - accuracy: 0.9800 - val_loss: 0.2324 - val_accuracy: 0.9100\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 178s 632ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.3529 - val_accuracy: 0.9100\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 171s 606ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.4162 - val_accuracy: 0.9010\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 167s 594ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.4261 - val_accuracy: 0.9120\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 166s 588ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.4449 - val_accuracy: 0.9090\n",
      "Epoch 7/15\n",
      "282/282 [==============================] - 178s 632ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.4160 - val_accuracy: 0.9040\n",
      "Epoch 8/15\n",
      "282/282 [==============================] - 178s 631ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.6138 - val_accuracy: 0.8990\n",
      "Epoch 9/15\n",
      "282/282 [==============================] - 176s 625ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.6523 - val_accuracy: 0.8960\n",
      "Epoch 10/15\n",
      "282/282 [==============================] - 169s 597ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.4325 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 91.20000004768372\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 195s 624ms/step - loss: 0.3996 - accuracy: 0.8057 - val_loss: 0.2171 - val_accuracy: 0.9180\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 175s 621ms/step - loss: 0.0644 - accuracy: 0.9809 - val_loss: 0.2343 - val_accuracy: 0.9020\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 200s 709ms/step - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.3329 - val_accuracy: 0.9020\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 202s 717ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.3881 - val_accuracy: 0.9000\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 204s 724ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.4302 - val_accuracy: 0.9090\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 206s 729ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.4703 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.79999828338623\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 231s 733ms/step - loss: 0.4020 - accuracy: 0.8121 - val_loss: 0.2241 - val_accuracy: 0.9140\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 217s 770ms/step - loss: 0.0600 - accuracy: 0.9812 - val_loss: 0.2922 - val_accuracy: 0.9080\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 268s 951ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.3681 - val_accuracy: 0.8930\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 268s 950ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.4110 - val_accuracy: 0.9010\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 265s 941ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.5283 - val_accuracy: 0.8900\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 265s 938ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.6074 - val_accuracy: 0.8910\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.39999747276306\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 243s 776ms/step - loss: 0.4052 - accuracy: 0.8133 - val_loss: 0.2251 - val_accuracy: 0.9190\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 227s 806ms/step - loss: 0.0638 - accuracy: 0.9798 - val_loss: 0.3034 - val_accuracy: 0.9020\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 236s 836ms/step - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.3234 - val_accuracy: 0.8980\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 235s 832ms/step - loss: 0.0083 - accuracy: 0.9979 - val_loss: 0.3728 - val_accuracy: 0.9050\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 236s 837ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.4304 - val_accuracy: 0.8930\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 237s 839ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.5679 - val_accuracy: 0.8930\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.90000295639038\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 241s 771ms/step - loss: 0.4035 - accuracy: 0.8002 - val_loss: 0.1897 - val_accuracy: 0.9310\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 186s 658ms/step - loss: 0.0677 - accuracy: 0.9775 - val_loss: 0.2345 - val_accuracy: 0.9110\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 214s 759ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.3264 - val_accuracy: 0.9220\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 211s 748ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.4088 - val_accuracy: 0.9210\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 211s 749ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.3710 - val_accuracy: 0.9210\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 209s 741ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.4993 - val_accuracy: 0.9060\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 93.09999942779541\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 260s 835ms/step - loss: 0.4088 - accuracy: 0.7990 - val_loss: 0.1954 - val_accuracy: 0.9250\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 222s 786ms/step - loss: 0.0660 - accuracy: 0.9798 - val_loss: 0.2221 - val_accuracy: 0.9080\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 275s 976ms/step - loss: 0.0218 - accuracy: 0.9943 - val_loss: 0.2992 - val_accuracy: 0.9100\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 273s 969ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.3181 - val_accuracy: 0.9220\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 273s 967ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.3233 - val_accuracy: 0.9120\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 271s 961ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.4045 - val_accuracy: 0.9130\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 283s 921ms/step - loss: 0.4128 - accuracy: 0.7985 - val_loss: 0.2412 - val_accuracy: 0.9010\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 231s 818ms/step - loss: 0.0557 - accuracy: 0.9835 - val_loss: 0.3165 - val_accuracy: 0.9020\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 268s 951ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.4410 - val_accuracy: 0.8910\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 267s 948ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 0.5342 - val_accuracy: 0.8910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "282/282 [==============================] - 266s 945ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.5366 - val_accuracy: 0.8840\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 265s 940ms/step - loss: 0.0052 - accuracy: 0.9979 - val_loss: 0.5926 - val_accuracy: 0.8860\n",
      "Epoch 7/15\n",
      "282/282 [==============================] - 265s 940ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.6639 - val_accuracy: 0.8920\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 90.20000100135803\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 190s 596ms/step - loss: 0.4161 - accuracy: 0.8121 - val_loss: 0.1984 - val_accuracy: 0.9200\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 169s 600ms/step - loss: 0.0561 - accuracy: 0.9829 - val_loss: 0.2353 - val_accuracy: 0.9080\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 204s 723ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.3332 - val_accuracy: 0.9070\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 204s 723ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.4130 - val_accuracy: 0.8840\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 204s 724ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.5258 - val_accuracy: 0.9100\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 205s 726ms/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.4146 - val_accuracy: 0.9040\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.00000166893005\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 245s 782ms/step - loss: 0.4106 - accuracy: 0.8159 - val_loss: 0.1811 - val_accuracy: 0.9260\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 186s 659ms/step - loss: 0.0586 - accuracy: 0.9803 - val_loss: 0.2264 - val_accuracy: 0.9040\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 204s 722ms/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 0.2715 - val_accuracy: 0.9090\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 205s 726ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.3183 - val_accuracy: 0.9190\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 206s 731ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.3222 - val_accuracy: 0.8990\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 207s 733ms/step - loss: 0.0058 - accuracy: 0.9977 - val_loss: 0.3876 - val_accuracy: 0.9080\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.59999990463257\n",
      "\n",
      "   acc1  acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  92.1  91.2  91.799998  91.399997  91.900003  93.099999  92.500001   \n",
      "\n",
      "        acc8       acc9  acc10    AVG  \n",
      "0  90.200001  92.000002   92.6  91.88  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.1</td>\n",
       "      <td>91.2</td>\n",
       "      <td>91.799998</td>\n",
       "      <td>91.399997</td>\n",
       "      <td>91.900003</td>\n",
       "      <td>93.099999</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>90.200001</td>\n",
       "      <td>92.000002</td>\n",
       "      <td>92.6</td>\n",
       "      <td>91.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc1  acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  92.1  91.2  91.799998  91.399997  91.900003  93.099999  92.500001   \n",
       "\n",
       "        acc8       acc9  acc10    AVG  \n",
       "0  90.200001  92.000002   92.6  91.88  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_SUBJ_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.77412221, -0.66578379, -2.36541218, ..., -0.07308953,\n",
       "        -1.27580538,  1.21141937],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 285,825\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 123s 359ms/step - loss: 0.4387 - accuracy: 0.7690 - val_loss: 0.4515 - val_accuracy: 0.8330\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 87s 310ms/step - loss: 0.2242 - accuracy: 0.9135 - val_loss: 0.5378 - val_accuracy: 0.8240\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 94s 334ms/step - loss: 0.1630 - accuracy: 0.9394 - val_loss: 0.7060 - val_accuracy: 0.8000\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 97s 343ms/step - loss: 0.1343 - accuracy: 0.9542 - val_loss: 0.9440 - val_accuracy: 0.7750\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 96s 341ms/step - loss: 0.0976 - accuracy: 0.9642 - val_loss: 0.9601 - val_accuracy: 0.7900\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 96s 341ms/step - loss: 0.0754 - accuracy: 0.9749 - val_loss: 1.0075 - val_accuracy: 0.8070\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 95s 337ms/step - loss: 0.0546 - accuracy: 0.9816 - val_loss: 1.0871 - val_accuracy: 0.8120\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 96s 341ms/step - loss: 0.0431 - accuracy: 0.9866 - val_loss: 1.3934 - val_accuracy: 0.7930\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 96s 342ms/step - loss: 0.0331 - accuracy: 0.9880 - val_loss: 1.4441 - val_accuracy: 0.7970\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 95s 338ms/step - loss: 0.0410 - accuracy: 0.9846 - val_loss: 1.7181 - val_accuracy: 0.7850\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 96s 341ms/step - loss: 0.0249 - accuracy: 0.9905 - val_loss: 1.5387 - val_accuracy: 0.8060\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 83.30000042915344\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 112s 324ms/step - loss: 0.4180 - accuracy: 0.7935 - val_loss: 0.3659 - val_accuracy: 0.8320\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 83s 295ms/step - loss: 0.2202 - accuracy: 0.9102 - val_loss: 0.2688 - val_accuracy: 0.8790\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 88s 312ms/step - loss: 0.1727 - accuracy: 0.9316 - val_loss: 0.2814 - val_accuracy: 0.8970\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 88s 312ms/step - loss: 0.1491 - accuracy: 0.9465 - val_loss: 0.3176 - val_accuracy: 0.8710\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 88s 312ms/step - loss: 0.1036 - accuracy: 0.9636 - val_loss: 0.2786 - val_accuracy: 0.8900\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0880 - accuracy: 0.9675 - val_loss: 0.3127 - val_accuracy: 0.8890\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 88s 314ms/step - loss: 0.0708 - accuracy: 0.9739 - val_loss: 0.3817 - val_accuracy: 0.8780\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.0543 - accuracy: 0.9806 - val_loss: 0.4445 - val_accuracy: 0.8600\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 87s 309ms/step - loss: 0.0467 - accuracy: 0.9815 - val_loss: 0.5394 - val_accuracy: 0.8390\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 87s 308ms/step - loss: 0.0378 - accuracy: 0.9874 - val_loss: 0.4779 - val_accuracy: 0.8820\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.0175 - accuracy: 0.9931 - val_loss: 0.5116 - val_accuracy: 0.8790\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 87s 309ms/step - loss: 0.0183 - accuracy: 0.9932 - val_loss: 0.4279 - val_accuracy: 0.8910\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 87s 309ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.6677 - val_accuracy: 0.8470\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 89.70000147819519\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 250s 809ms/step - loss: 0.4110 - accuracy: 0.8013 - val_loss: 0.4733 - val_accuracy: 0.8020\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 210s 745ms/step - loss: 0.2239 - accuracy: 0.9080 - val_loss: 0.3251 - val_accuracy: 0.8820\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 189s 669ms/step - loss: 0.1694 - accuracy: 0.9368 - val_loss: 0.2787 - val_accuracy: 0.8900\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 186s 660ms/step - loss: 0.1481 - accuracy: 0.9467 - val_loss: 0.3437 - val_accuracy: 0.8710\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 186s 661ms/step - loss: 0.1114 - accuracy: 0.9592 - val_loss: 0.3310 - val_accuracy: 0.8830\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 187s 663ms/step - loss: 0.0889 - accuracy: 0.9715 - val_loss: 0.3919 - val_accuracy: 0.8730\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 187s 662ms/step - loss: 0.0670 - accuracy: 0.9751 - val_loss: 0.4087 - val_accuracy: 0.8790\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 187s 662ms/step - loss: 0.0481 - accuracy: 0.9843 - val_loss: 0.3775 - val_accuracy: 0.8820\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 186s 661ms/step - loss: 0.0317 - accuracy: 0.9898 - val_loss: 0.8080 - val_accuracy: 0.8090\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 187s 663ms/step - loss: 0.0314 - accuracy: 0.9897 - val_loss: 0.5316 - val_accuracy: 0.8650\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 188s 666ms/step - loss: 0.0237 - accuracy: 0.9903 - val_loss: 0.6756 - val_accuracy: 0.8750\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 187s 662ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 0.6335 - val_accuracy: 0.8670\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 187s 662ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.5647 - val_accuracy: 0.8840\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 88.99999856948853\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 241s 783ms/step - loss: 0.4310 - accuracy: 0.7870 - val_loss: 0.2716 - val_accuracy: 0.8930\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 211s 750ms/step - loss: 0.2162 - accuracy: 0.9098 - val_loss: 0.2416 - val_accuracy: 0.8930\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 260s 921ms/step - loss: 0.1716 - accuracy: 0.9372 - val_loss: 0.2316 - val_accuracy: 0.9080\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 260s 921ms/step - loss: 0.1349 - accuracy: 0.9498 - val_loss: 0.2171 - val_accuracy: 0.9220\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 260s 921ms/step - loss: 0.1008 - accuracy: 0.9620 - val_loss: 0.2202 - val_accuracy: 0.9160\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 257s 913ms/step - loss: 0.0859 - accuracy: 0.9703 - val_loss: 0.2762 - val_accuracy: 0.9160\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 258s 913ms/step - loss: 0.0565 - accuracy: 0.9809 - val_loss: 0.2711 - val_accuracy: 0.9080\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 258s 915ms/step - loss: 0.0392 - accuracy: 0.9855 - val_loss: 0.3071 - val_accuracy: 0.9140\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 258s 915ms/step - loss: 0.0325 - accuracy: 0.9886 - val_loss: 0.2903 - val_accuracy: 0.9210\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 258s 916ms/step - loss: 0.0385 - accuracy: 0.9863 - val_loss: 0.3273 - val_accuracy: 0.9310\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 258s 914ms/step - loss: 0.0170 - accuracy: 0.9952 - val_loss: 0.5163 - val_accuracy: 0.8900\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 258s 916ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.3630 - val_accuracy: 0.9250\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 258s 915ms/step - loss: 0.0158 - accuracy: 0.9942 - val_loss: 0.3574 - val_accuracy: 0.9200\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 259s 918ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.4164 - val_accuracy: 0.9090\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 258s 914ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.4356 - val_accuracy: 0.9220\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 257s 912ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.3673 - val_accuracy: 0.9280\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 258s 915ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.4335 - val_accuracy: 0.9210\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 260s 920ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.3926 - val_accuracy: 0.9180\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 259s 919ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.4223 - val_accuracy: 0.9150\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 258s 915ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.4481 - val_accuracy: 0.9070\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 93.09999942779541\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 235s 741ms/step - loss: 0.4323 - accuracy: 0.7867 - val_loss: 0.3185 - val_accuracy: 0.8560\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 166s 589ms/step - loss: 0.2259 - accuracy: 0.9089 - val_loss: 0.2484 - val_accuracy: 0.8930\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 158s 561ms/step - loss: 0.1794 - accuracy: 0.9330 - val_loss: 0.2594 - val_accuracy: 0.9030\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 156s 552ms/step - loss: 0.1422 - accuracy: 0.9494 - val_loss: 0.2184 - val_accuracy: 0.9020\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 156s 553ms/step - loss: 0.1084 - accuracy: 0.9627 - val_loss: 0.2476 - val_accuracy: 0.8920\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 154s 547ms/step - loss: 0.0880 - accuracy: 0.9686 - val_loss: 0.2442 - val_accuracy: 0.9070\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.0640 - accuracy: 0.9780 - val_loss: 0.3201 - val_accuracy: 0.9050\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 156s 553ms/step - loss: 0.0487 - accuracy: 0.9821 - val_loss: 0.2977 - val_accuracy: 0.9070\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 156s 553ms/step - loss: 0.0368 - accuracy: 0.9878 - val_loss: 0.3365 - val_accuracy: 0.8990\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 156s 554ms/step - loss: 0.0290 - accuracy: 0.9898 - val_loss: 0.3215 - val_accuracy: 0.8930\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 157s 557ms/step - loss: 0.0215 - accuracy: 0.9925 - val_loss: 0.3582 - val_accuracy: 0.9170\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 156s 553ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.4751 - val_accuracy: 0.8990\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.0171 - accuracy: 0.9941 - val_loss: 0.4154 - val_accuracy: 0.9010\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 155s 551ms/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.4113 - val_accuracy: 0.9050\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 156s 552ms/step - loss: 0.0096 - accuracy: 0.9963 - val_loss: 0.4915 - val_accuracy: 0.8820\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0116 - accuracy: 0.9958 - val_loss: 0.4961 - val_accuracy: 0.8940\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 157s 558ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.5087 - val_accuracy: 0.8970\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 159s 563ms/step - loss: 0.0051 - accuracy: 0.9980 - val_loss: 0.5987 - val_accuracy: 0.8860\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 158s 559ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.6770 - val_accuracy: 0.9030\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 0.5950 - val_accuracy: 0.8860\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 156s 554ms/step - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.4949 - val_accuracy: 0.8980\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 91.69999957084656\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 201s 630ms/step - loss: 0.4239 - accuracy: 0.7777 - val_loss: 0.2573 - val_accuracy: 0.8960\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 249s 882ms/step - loss: 0.2147 - accuracy: 0.9162 - val_loss: 0.2482 - val_accuracy: 0.8940\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 261s 924ms/step - loss: 0.1730 - accuracy: 0.9400 - val_loss: 0.2400 - val_accuracy: 0.9050\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 246s 873ms/step - loss: 0.1422 - accuracy: 0.9473 - val_loss: 0.2505 - val_accuracy: 0.8970\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 248s 878ms/step - loss: 0.1243 - accuracy: 0.9545 - val_loss: 0.2824 - val_accuracy: 0.8990\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 251s 889ms/step - loss: 0.0900 - accuracy: 0.9671 - val_loss: 0.2901 - val_accuracy: 0.9180\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 248s 880ms/step - loss: 0.0730 - accuracy: 0.9732 - val_loss: 0.3467 - val_accuracy: 0.8970\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 249s 884ms/step - loss: 0.0542 - accuracy: 0.9837 - val_loss: 0.3146 - val_accuracy: 0.9100\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 252s 893ms/step - loss: 0.0368 - accuracy: 0.9883 - val_loss: 0.3736 - val_accuracy: 0.9010\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 246s 873ms/step - loss: 0.0332 - accuracy: 0.9907 - val_loss: 0.4655 - val_accuracy: 0.8830\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 248s 880ms/step - loss: 0.0373 - accuracy: 0.9863 - val_loss: 0.4140 - val_accuracy: 0.9130\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 253s 898ms/step - loss: 0.0299 - accuracy: 0.9915 - val_loss: 0.4855 - val_accuracy: 0.8990\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 251s 891ms/step - loss: 0.0123 - accuracy: 0.9953 - val_loss: 0.4655 - val_accuracy: 0.9070\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 248s 881ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.5468 - val_accuracy: 0.8940\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 252s 895ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.5642 - val_accuracy: 0.9020\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 252s 893ms/step - loss: 0.0191 - accuracy: 0.9935 - val_loss: 0.4877 - val_accuracy: 0.8990\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 91.79999828338623\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 285s 936ms/step - loss: 0.4144 - accuracy: 0.8023 - val_loss: 0.5068 - val_accuracy: 0.7900\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 208s 739ms/step - loss: 0.2159 - accuracy: 0.9163 - val_loss: 0.4583 - val_accuracy: 0.8520\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 200s 711ms/step - loss: 0.1938 - accuracy: 0.9254 - val_loss: 0.8239 - val_accuracy: 0.7980\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 190s 674ms/step - loss: 0.1470 - accuracy: 0.9435 - val_loss: 0.7024 - val_accuracy: 0.8140\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 190s 672ms/step - loss: 0.1101 - accuracy: 0.9586 - val_loss: 0.7767 - val_accuracy: 0.8170\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 189s 669ms/step - loss: 0.0871 - accuracy: 0.9697 - val_loss: 0.8352 - val_accuracy: 0.8060\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 187s 665ms/step - loss: 0.0655 - accuracy: 0.9746 - val_loss: 1.0969 - val_accuracy: 0.7930\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 188s 668ms/step - loss: 0.0413 - accuracy: 0.9837 - val_loss: 1.0804 - val_accuracy: 0.8110\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 188s 666ms/step - loss: 0.0359 - accuracy: 0.9875 - val_loss: 1.3672 - val_accuracy: 0.7970\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 189s 671ms/step - loss: 0.0375 - accuracy: 0.9873 - val_loss: 1.1632 - val_accuracy: 0.8370\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 192s 680ms/step - loss: 0.0256 - accuracy: 0.9918 - val_loss: 1.3951 - val_accuracy: 0.8010\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 201s 714ms/step - loss: 0.0149 - accuracy: 0.9963 - val_loss: 1.5952 - val_accuracy: 0.8110\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.19999980926514\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 271s 894ms/step - loss: 0.4333 - accuracy: 0.7831 - val_loss: 0.2458 - val_accuracy: 0.8960\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 299s 1s/step - loss: 0.2195 - accuracy: 0.9109 - val_loss: 0.3002 - val_accuracy: 0.8680\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 330s 1s/step - loss: 0.1835 - accuracy: 0.9260 - val_loss: 0.2431 - val_accuracy: 0.8920\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 335s 1s/step - loss: 0.1421 - accuracy: 0.9484 - val_loss: 0.3320 - val_accuracy: 0.8770\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 327s 1s/step - loss: 0.1166 - accuracy: 0.9599 - val_loss: 0.4545 - val_accuracy: 0.8600\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 323s 1s/step - loss: 0.0875 - accuracy: 0.9705 - val_loss: 0.4032 - val_accuracy: 0.8700\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 323s 1s/step - loss: 0.0753 - accuracy: 0.9725 - val_loss: 0.7814 - val_accuracy: 0.8410\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 331s 1s/step - loss: 0.0477 - accuracy: 0.9838 - val_loss: 0.5927 - val_accuracy: 0.8510\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 333s 1s/step - loss: 0.0327 - accuracy: 0.9892 - val_loss: 0.6928 - val_accuracy: 0.8650\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 333s 1s/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 1.0446 - val_accuracy: 0.8460\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.8918 - val_accuracy: 0.8640\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 89.60000276565552\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 273s 889ms/step - loss: 0.4036 - accuracy: 0.7989 - val_loss: 0.2549 - val_accuracy: 0.8910\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 307s 1s/step - loss: 0.2150 - accuracy: 0.9186 - val_loss: 0.2394 - val_accuracy: 0.9080\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 300s 1s/step - loss: 0.1867 - accuracy: 0.9325 - val_loss: 0.2426 - val_accuracy: 0.9110\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 299s 1s/step - loss: 0.1491 - accuracy: 0.9432 - val_loss: 0.2237 - val_accuracy: 0.9120\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 298s 1s/step - loss: 0.1068 - accuracy: 0.9627 - val_loss: 0.3223 - val_accuracy: 0.8840\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 298s 1s/step - loss: 0.0904 - accuracy: 0.9653 - val_loss: 0.2811 - val_accuracy: 0.9060\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 297s 1s/step - loss: 0.0665 - accuracy: 0.9764 - val_loss: 0.3210 - val_accuracy: 0.9010\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 297s 1s/step - loss: 0.0562 - accuracy: 0.9794 - val_loss: 0.3465 - val_accuracy: 0.9020\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 296s 1s/step - loss: 0.0393 - accuracy: 0.9873 - val_loss: 0.3975 - val_accuracy: 0.8860\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 296s 1s/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 0.4343 - val_accuracy: 0.9010\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 297s 1s/step - loss: 0.0206 - accuracy: 0.9932 - val_loss: 0.5045 - val_accuracy: 0.8760\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 297s 1s/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.5239 - val_accuracy: 0.8830\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 296s 1s/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.6404 - val_accuracy: 0.8840\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 296s 1s/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.6092 - val_accuracy: 0.8830\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 91.20000004768372\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 378s 1s/step - loss: 0.4273 - accuracy: 0.7834 - val_loss: 0.3958 - val_accuracy: 0.8500\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 250s 888ms/step - loss: 0.2127 - accuracy: 0.9200 - val_loss: 0.4414 - val_accuracy: 0.8490\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 230s 814ms/step - loss: 0.1624 - accuracy: 0.9383 - val_loss: 0.4342 - val_accuracy: 0.8460\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.1325 - accuracy: 0.9509 - val_loss: 0.5523 - val_accuracy: 0.8330\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.1015 - accuracy: 0.9620 - val_loss: 0.5599 - val_accuracy: 0.8220\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.0820 - accuracy: 0.9699 - val_loss: 0.6182 - val_accuracy: 0.8380\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 230s 814ms/step - loss: 0.0535 - accuracy: 0.9824 - val_loss: 0.5194 - val_accuracy: 0.8470\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 230s 814ms/step - loss: 0.0372 - accuracy: 0.9880 - val_loss: 0.6492 - val_accuracy: 0.8670\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.0431 - accuracy: 0.9849 - val_loss: 0.8663 - val_accuracy: 0.8290\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 228s 809ms/step - loss: 0.0517 - accuracy: 0.9851 - val_loss: 0.8264 - val_accuracy: 0.8460\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.6994 - val_accuracy: 0.8560\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 229s 812ms/step - loss: 0.0088 - accuracy: 0.9978 - val_loss: 1.0447 - val_accuracy: 0.8500\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.0182 - accuracy: 0.9949 - val_loss: 0.7588 - val_accuracy: 0.8580\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 229s 812ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.6222 - val_accuracy: 0.8750\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 0.0140 - accuracy: 0.9946 - val_loss: 0.7358 - val_accuracy: 0.8630\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 229s 811ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.9103 - val_accuracy: 0.8610\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 229s 814ms/step - loss: 7.3483e-04 - accuracy: 1.0000 - val_loss: 0.9394 - val_accuracy: 0.8570\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 229s 811ms/step - loss: 4.4403e-04 - accuracy: 1.0000 - val_loss: 1.0841 - val_accuracy: 0.8620\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 3.3843e-04 - accuracy: 1.0000 - val_loss: 1.2397 - val_accuracy: 0.8550\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 228s 810ms/step - loss: 1.0643e-04 - accuracy: 1.0000 - val_loss: 1.2147 - val_accuracy: 0.8630\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 229s 812ms/step - loss: 8.8826e-05 - accuracy: 1.0000 - val_loss: 1.2563 - val_accuracy: 0.8620\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 229s 812ms/step - loss: 6.8183e-05 - accuracy: 1.0000 - val_loss: 1.3061 - val_accuracy: 0.8610\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 230s 814ms/step - loss: 4.6288e-05 - accuracy: 1.0000 - val_loss: 1.3318 - val_accuracy: 0.8620\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 3.2791e-05 - accuracy: 1.0000 - val_loss: 1.4002 - val_accuracy: 0.8600\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Test Accuracy: 87.5\n",
      "\n",
      "   acc1       acc2       acc3       acc4  acc5       acc6  acc7       acc8  \\\n",
      "0  83.3  89.700001  88.999999  93.099999  91.7  91.799998  85.2  89.600003   \n",
      "\n",
      "   acc9  acc10    AVG  \n",
      "0  91.2   87.5  89.21  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.3</td>\n",
       "      <td>89.700001</td>\n",
       "      <td>88.999999</td>\n",
       "      <td>93.099999</td>\n",
       "      <td>91.7</td>\n",
       "      <td>91.799998</td>\n",
       "      <td>85.2</td>\n",
       "      <td>89.600003</td>\n",
       "      <td>91.2</td>\n",
       "      <td>87.5</td>\n",
       "      <td>89.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc1       acc2       acc3       acc4  acc5       acc6  acc7       acc8  \\\n",
       "0  83.3  89.700001  88.999999  93.099999  91.7  91.799998  85.2  89.600003   \n",
       "\n",
       "   acc9  acc10    AVG  \n",
       "0  91.2   87.5  89.21  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_SUBJ_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_45 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 461s 2s/step - loss: 0.3894 - accuracy: 0.8028 - val_loss: 0.2029 - val_accuracy: 0.9150\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 361s 1s/step - loss: 0.0968 - accuracy: 0.9659 - val_loss: 0.2166 - val_accuracy: 0.9090\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 421s 1s/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 0.3713 - val_accuracy: 0.8990\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 441s 2s/step - loss: 0.0158 - accuracy: 0.9960 - val_loss: 0.3699 - val_accuracy: 0.9080\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 440s 2s/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.4967 - val_accuracy: 0.8990\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 437s 2s/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.5952 - val_accuracy: 0.8970\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.50000214576721\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 482s 2s/step - loss: 0.3852 - accuracy: 0.8161 - val_loss: 0.2052 - val_accuracy: 0.9220\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 415s 1s/step - loss: 0.0926 - accuracy: 0.9676 - val_loss: 0.2557 - val_accuracy: 0.9010\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 427s 2s/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.3578 - val_accuracy: 0.9030\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 426s 2s/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.7998 - val_accuracy: 0.8570\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 429s 2s/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.8727 - val_accuracy: 0.8650\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 429s 2s/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.5065 - val_accuracy: 0.9060\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.1999990940094\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 475s 2s/step - loss: 0.3913 - accuracy: 0.8109 - val_loss: 0.2471 - val_accuracy: 0.8900\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 390s 1s/step - loss: 0.0878 - accuracy: 0.9701 - val_loss: 0.3402 - val_accuracy: 0.8770\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 356s 1s/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.3679 - val_accuracy: 0.9000\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 359s 1s/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.4867 - val_accuracy: 0.8900\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 358s 1s/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.6616 - val_accuracy: 0.8790\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 355s 1s/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.3726 - val_accuracy: 0.8880\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 354s 1s/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.5355 - val_accuracy: 0.9000\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 354s 1s/step - loss: 0.0010 - accuracy: 0.9994 - val_loss: 0.6333 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 89.99999761581421\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 343s 1s/step - loss: 0.3882 - accuracy: 0.8030 - val_loss: 0.1906 - val_accuracy: 0.9210\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 425s 2s/step - loss: 0.0894 - accuracy: 0.9699 - val_loss: 0.2000 - val_accuracy: 0.9160\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 423s 2s/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.2575 - val_accuracy: 0.9150\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 422s 1s/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.3232 - val_accuracy: 0.9180\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 421s 1s/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.4064 - val_accuracy: 0.9090\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 421s 1s/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3775 - val_accuracy: 0.9050\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.10000038146973\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 432s 1s/step - loss: 0.3868 - accuracy: 0.8190 - val_loss: 0.1921 - val_accuracy: 0.9270\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 531s 2s/step - loss: 0.0944 - accuracy: 0.9686 - val_loss: 0.2239 - val_accuracy: 0.9150\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 531s 2s/step - loss: 0.0261 - accuracy: 0.9932 - val_loss: 0.3334 - val_accuracy: 0.9090\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 530s 2s/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.3132 - val_accuracy: 0.9090\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 530s 2s/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.3279 - val_accuracy: 0.9160\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 504s 2s/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.4624 - val_accuracy: 0.9040\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.69999861717224\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 467s 2s/step - loss: 0.3969 - accuracy: 0.8027 - val_loss: 0.2037 - val_accuracy: 0.9220\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 442s 2s/step - loss: 0.0913 - accuracy: 0.9685 - val_loss: 0.1986 - val_accuracy: 0.9240\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 399s 1s/step - loss: 0.0296 - accuracy: 0.9918 - val_loss: 0.2719 - val_accuracy: 0.9030\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 398s 1s/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.3438 - val_accuracy: 0.9100\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 397s 1s/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.4813 - val_accuracy: 0.9040\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 398s 1s/step - loss: 5.5018e-04 - accuracy: 0.9999 - val_loss: 0.5178 - val_accuracy: 0.8910\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 398s 1s/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.3671 - val_accuracy: 0.9040\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 582s 2s/step - loss: 0.3862 - accuracy: 0.8132 - val_loss: 0.2929 - val_accuracy: 0.8830\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 511s 2s/step - loss: 0.0884 - accuracy: 0.9707 - val_loss: 0.4685 - val_accuracy: 0.8570\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 524s 2s/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 0.4680 - val_accuracy: 0.8870\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 530s 2s/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.5002 - val_accuracy: 0.8720\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 528s 2s/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.6564 - val_accuracy: 0.8750\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 540s 2s/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.7660 - val_accuracy: 0.8770\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 523s 2s/step - loss: 4.0501e-04 - accuracy: 1.0000 - val_loss: 0.9087 - val_accuracy: 0.8780\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 526s 2s/step - loss: 5.3876e-05 - accuracy: 1.0000 - val_loss: 0.9595 - val_accuracy: 0.8800\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 88.7000024318695\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 429s 1s/step - loss: 0.3971 - accuracy: 0.8092 - val_loss: 0.2290 - val_accuracy: 0.9100\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 458s 2s/step - loss: 0.0920 - accuracy: 0.9714 - val_loss: 0.2678 - val_accuracy: 0.8910\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 412s 1s/step - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.2990 - val_accuracy: 0.9020\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 396s 1s/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.3378 - val_accuracy: 0.8910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40\n",
      "282/282 [==============================] - 395s 1s/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.5661 - val_accuracy: 0.8850\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 395s 1s/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.7314 - val_accuracy: 0.8870\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.00000262260437\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 500s 2s/step - loss: 0.3828 - accuracy: 0.8127 - val_loss: 0.2447 - val_accuracy: 0.9020\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 442s 2s/step - loss: 0.0854 - accuracy: 0.9709 - val_loss: 0.2902 - val_accuracy: 0.8870\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 436s 2s/step - loss: 0.0271 - accuracy: 0.9926 - val_loss: 0.4460 - val_accuracy: 0.8890\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 435s 2s/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.4484 - val_accuracy: 0.8840\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 432s 2s/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.4577 - val_accuracy: 0.8850\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 431s 2s/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.4755 - val_accuracy: 0.8960\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 90.20000100135803\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 432s 1s/step - loss: 0.3921 - accuracy: 0.8105 - val_loss: 0.2252 - val_accuracy: 0.9070\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 509s 2s/step - loss: 0.0932 - accuracy: 0.9692 - val_loss: 0.3003 - val_accuracy: 0.8860\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 490s 2s/step - loss: 0.0315 - accuracy: 0.9926 - val_loss: 0.3656 - val_accuracy: 0.8930\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 492s 2s/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.3924 - val_accuracy: 0.9160\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 492s 2s/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 0.4679 - val_accuracy: 0.9020\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 492s 2s/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.6494 - val_accuracy: 0.8810\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 492s 2s/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.4350 - val_accuracy: 0.9090\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 492s 2s/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.5791 - val_accuracy: 0.8950\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 492s 2s/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.5832 - val_accuracy: 0.8890\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 91.60000085830688\n",
      "\n",
      "        acc1       acc2       acc3  acc4       acc5       acc6       acc7  \\\n",
      "0  91.500002  92.199999  89.999998  92.1  92.699999  92.400002  88.700002   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  91.000003  90.200001  91.600001  91.240001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.500002</td>\n",
       "      <td>92.199999</td>\n",
       "      <td>89.999998</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.699999</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>88.700002</td>\n",
       "      <td>91.000003</td>\n",
       "      <td>90.200001</td>\n",
       "      <td>91.600001</td>\n",
       "      <td>91.240001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3  acc4       acc5       acc6       acc7  \\\n",
       "0  91.500002  92.199999  89.999998  92.1  92.699999  92.400002  88.700002   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  91.000003  90.200001  91.600001  91.240001  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_SUBJ_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
