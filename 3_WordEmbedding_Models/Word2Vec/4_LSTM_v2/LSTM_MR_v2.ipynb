{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with MR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the MR Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10662, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it 's so laddish and juvenile , only teenage b...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garbus discards the potential for pathological...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>mazel tov to a film about a family 's joyous l...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>standing in the shadows of motown is the best ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>it 's nice to see piscopo again after all thes...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>provides a porthole into that noble , tremblin...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label  split\n",
       "0                       simplistic , silly and tedious .      0  train\n",
       "1      it 's so laddish and juvenile , only teenage b...      0  train\n",
       "2      exploitative and largely devoid of the depth o...      0  train\n",
       "3      garbus discards the potential for pathological...      0  train\n",
       "4      a visually flashy but narratively opaque and e...      0  train\n",
       "...                                                  ...    ...    ...\n",
       "10657  both exuberantly romantic and serenely melanch...      1  train\n",
       "10658  mazel tov to a film about a family 's joyous l...      1  train\n",
       "10659  standing in the shadows of motown is the best ...      1  train\n",
       "10660  it 's nice to see piscopo again after all thes...      1  train\n",
       "10661  provides a porthole into that noble , tremblin...      1  train\n",
       "\n",
       "[10662 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MR/MR.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10662 entries, 0 to 10661\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10662 non-null  object\n",
      " 1   label     10662 non-null  int32 \n",
      " 2   split     10662 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 208.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5331</td>\n",
       "      <td>5331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5331</td>\n",
       "      <td>5331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          5331   5331\n",
       "1          5331   5331"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simplistic , silly and tedious .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
      "Into a sequence of int: [3, 544, 1838, 13, 3909, 3366, 4, 658, 2629, 416, 10, 236, 4, 10112]\n",
      "Into a padded sequence: [    3   544  1838    13  3909  3366     4   658  2629   416    10   236\n",
      "     4 10112     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "a 3\n",
      "and 4\n",
      "of 5\n",
      "to 6\n",
      "is 7\n",
      "'s 8\n",
      "it 9\n",
      "in 10\n",
      "18760\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 89s 223ms/step - loss: 0.6163 - accuracy: 0.6415 - val_loss: 0.4619 - val_accuracy: 0.7873\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 57s 189ms/step - loss: 0.2410 - accuracy: 0.9080 - val_loss: 0.4809 - val_accuracy: 0.7835\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 60s 199ms/step - loss: 0.0838 - accuracy: 0.9738 - val_loss: 0.8936 - val_accuracy: 0.7723\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 60s 201ms/step - loss: 0.0359 - accuracy: 0.9884 - val_loss: 1.0687 - val_accuracy: 0.7610\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 63s 209ms/step - loss: 0.0181 - accuracy: 0.9941 - val_loss: 1.0059 - val_accuracy: 0.7666\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 60s 201ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 1.3534 - val_accuracy: 0.7460\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 60s 200ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 1.3551 - val_accuracy: 0.7619\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 58s 192ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 1.3780 - val_accuracy: 0.7591\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 58s 193ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 1.5912 - val_accuracy: 0.7535\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 58s 195ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 1.6829 - val_accuracy: 0.7535\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 59s 196ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.7011 - val_accuracy: 0.7413\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.72539758682251\n",
      "Training 2: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 118s 318ms/step - loss: 0.6152 - accuracy: 0.6361 - val_loss: 0.4630 - val_accuracy: 0.7938\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 62s 207ms/step - loss: 0.2397 - accuracy: 0.9073 - val_loss: 0.4972 - val_accuracy: 0.7676\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 62s 205ms/step - loss: 0.0860 - accuracy: 0.9717 - val_loss: 0.8153 - val_accuracy: 0.7666\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 61s 204ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.9457 - val_accuracy: 0.7591\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 61s 204ms/step - loss: 0.0139 - accuracy: 0.9948 - val_loss: 1.1122 - val_accuracy: 0.7554\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 60s 201ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 1.0272 - val_accuracy: 0.7526\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 61s 203ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 1.3724 - val_accuracy: 0.7648\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 61s 202ms/step - loss: 0.0089 - accuracy: 0.9968 - val_loss: 1.4249 - val_accuracy: 0.7545\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 60s 202ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 1.5103 - val_accuracy: 0.7657\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 60s 201ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 1.2798 - val_accuracy: 0.7488\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 59s 197ms/step - loss: 0.0104 - accuracy: 0.9971 - val_loss: 1.2610 - val_accuracy: 0.7685\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.38144207000732\n",
      "Training 3: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 98s 257ms/step - loss: 0.6156 - accuracy: 0.6446 - val_loss: 0.4884 - val_accuracy: 0.7786\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 61s 202ms/step - loss: 0.2263 - accuracy: 0.9184 - val_loss: 0.5748 - val_accuracy: 0.7589\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 61s 203ms/step - loss: 0.0855 - accuracy: 0.9706 - val_loss: 0.9211 - val_accuracy: 0.7542\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 59s 197ms/step - loss: 0.0333 - accuracy: 0.9903 - val_loss: 1.0110 - val_accuracy: 0.7495\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 59s 197ms/step - loss: 0.0139 - accuracy: 0.9968 - val_loss: 1.3935 - val_accuracy: 0.7439\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 59s 196ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 1.4038 - val_accuracy: 0.7486\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 58s 195ms/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 1.3696 - val_accuracy: 0.7355\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 58s 194ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 1.5199 - val_accuracy: 0.7411\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 58s 194ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 1.3418 - val_accuracy: 0.7345\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 58s 193ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 1.5688 - val_accuracy: 0.7420\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 58s 194ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 1.6057 - val_accuracy: 0.7448\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.86116600036621\n",
      "Training 4: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 132s 364ms/step - loss: 0.6129 - accuracy: 0.6464 - val_loss: 0.4897 - val_accuracy: 0.7580\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 98s 326ms/step - loss: 0.2313 - accuracy: 0.9120 - val_loss: 0.5398 - val_accuracy: 0.7608\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 102s 341ms/step - loss: 0.0878 - accuracy: 0.9707 - val_loss: 0.6626 - val_accuracy: 0.7523\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 102s 341ms/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 0.9263 - val_accuracy: 0.7608\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 101s 335ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 1.2530 - val_accuracy: 0.7448\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 98s 326ms/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 1.1869 - val_accuracy: 0.7364\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 98s 327ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 1.5908 - val_accuracy: 0.7411\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 102s 341ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 1.4224 - val_accuracy: 0.7617\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 100s 334ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 1.5891 - val_accuracy: 0.7430\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 101s 336ms/step - loss: 0.0072 - accuracy: 0.9974 - val_loss: 1.7488 - val_accuracy: 0.7430\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 102s 339ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 1.5451 - val_accuracy: 0.7542\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 98s 326ms/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 1.9033 - val_accuracy: 0.7430\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 99s 330ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 1.8613 - val_accuracy: 0.7533\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 100s 334ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4764 - val_accuracy: 0.7467\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 99s 332ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 1.9913 - val_accuracy: 0.7308\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 100s 332ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 2.0612 - val_accuracy: 0.7523\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 99s 329ms/step - loss: 1.5674e-04 - accuracy: 1.0000 - val_loss: 2.1487 - val_accuracy: 0.7495\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 101s 336ms/step - loss: 4.8509e-05 - accuracy: 1.0000 - val_loss: 2.2041 - val_accuracy: 0.7495\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 76.17260813713074\n",
      "Training 5: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 135s 365ms/step - loss: 0.6159 - accuracy: 0.6464 - val_loss: 0.4819 - val_accuracy: 0.7523\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 94s 315ms/step - loss: 0.2388 - accuracy: 0.9054 - val_loss: 0.5181 - val_accuracy: 0.7655\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.0723 - accuracy: 0.9757 - val_loss: 0.7113 - val_accuracy: 0.7589\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.8845 - val_accuracy: 0.7655\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 1.2295 - val_accuracy: 0.7758\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 1.2899 - val_accuracy: 0.7674\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 1.3138 - val_accuracy: 0.7636\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 1.4661 - val_accuracy: 0.7514\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 94s 315ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 1.3368 - val_accuracy: 0.7589\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 1.3798 - val_accuracy: 0.7598\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 95s 315ms/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 1.3736 - val_accuracy: 0.7373\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 1.2887 - val_accuracy: 0.7467\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.5043 - val_accuracy: 0.7420\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 95s 315ms/step - loss: 2.1403e-04 - accuracy: 1.0000 - val_loss: 1.6725 - val_accuracy: 0.7514\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 6.6886e-05 - accuracy: 1.0000 - val_loss: 1.7618 - val_accuracy: 0.7523\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 77.57973670959473\n",
      "Training 6: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 161s 463ms/step - loss: 0.6121 - accuracy: 0.6472 - val_loss: 0.4982 - val_accuracy: 0.7411\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 119s 396ms/step - loss: 0.2205 - accuracy: 0.9105 - val_loss: 0.6212 - val_accuracy: 0.7402\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 119s 396ms/step - loss: 0.0704 - accuracy: 0.9769 - val_loss: 0.9785 - val_accuracy: 0.7205\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 117s 391ms/step - loss: 0.0329 - accuracy: 0.9898 - val_loss: 1.3391 - val_accuracy: 0.7233\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 116s 387ms/step - loss: 0.0174 - accuracy: 0.9934 - val_loss: 1.1648 - val_accuracy: 0.7223\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 116s 386ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 1.4568 - val_accuracy: 0.7167\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 114s 382ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 1.8598 - val_accuracy: 0.7176\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 115s 383ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 1.9447 - val_accuracy: 0.7355\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 115s 382ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 1.4689 - val_accuracy: 0.7298\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 114s 381ms/step - loss: 0.0095 - accuracy: 0.9961 - val_loss: 1.7466 - val_accuracy: 0.7148\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 114s 382ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 1.8244 - val_accuracy: 0.7111\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 74.10881519317627\n",
      "Training 7: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 164s 475ms/step - loss: 0.6122 - accuracy: 0.6484 - val_loss: 0.4659 - val_accuracy: 0.7983\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 105s 350ms/step - loss: 0.2274 - accuracy: 0.9145 - val_loss: 0.4764 - val_accuracy: 0.7786\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0957 - accuracy: 0.9681 - val_loss: 0.7749 - val_accuracy: 0.7636\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 100s 333ms/step - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.9793 - val_accuracy: 0.7598\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 98s 326ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 1.1208 - val_accuracy: 0.7627\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 97s 324ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 1.1534 - val_accuracy: 0.7505\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 97s 323ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 1.4461 - val_accuracy: 0.7383\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 97s 324ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 1.6305 - val_accuracy: 0.7486\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 97s 325ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 1.3478 - val_accuracy: 0.7458\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 97s 323ms/step - loss: 0.0068 - accuracy: 0.9972 - val_loss: 1.4536 - val_accuracy: 0.7523\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 96s 321ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 1.5065 - val_accuracy: 0.7411\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.83114719390869\n",
      "Training 8: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 162s 462ms/step - loss: 0.6117 - accuracy: 0.6401 - val_loss: 0.4850 - val_accuracy: 0.7598\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 107s 357ms/step - loss: 0.2308 - accuracy: 0.9098 - val_loss: 0.5121 - val_accuracy: 0.7580\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 107s 358ms/step - loss: 0.0800 - accuracy: 0.9743 - val_loss: 0.6773 - val_accuracy: 0.7542\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 108s 360ms/step - loss: 0.0373 - accuracy: 0.9908 - val_loss: 1.0124 - val_accuracy: 0.7636\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 107s 355ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 1.1467 - val_accuracy: 0.7627\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0151 - accuracy: 0.9944 - val_loss: 1.2540 - val_accuracy: 0.7561\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 1.5997 - val_accuracy: 0.7448\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 105s 350ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 1.4299 - val_accuracy: 0.7608\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 1.4432 - val_accuracy: 0.7608\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 106s 355ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 1.5454 - val_accuracy: 0.7317\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 1.3563 - val_accuracy: 0.7514\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 105s 350ms/step - loss: 0.0050 - accuracy: 0.9980 - val_loss: 1.5222 - val_accuracy: 0.7683\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 106s 352ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 1.7980 - val_accuracy: 0.7505\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 105s 352ms/step - loss: 5.9609e-04 - accuracy: 0.9999 - val_loss: 1.9773 - val_accuracy: 0.7523\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 105s 350ms/step - loss: 6.1359e-04 - accuracy: 0.9999 - val_loss: 2.0328 - val_accuracy: 0.7514\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 105s 351ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 1.7846 - val_accuracy: 0.7495\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 105s 352ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 2.0226 - val_accuracy: 0.7533\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 1.6029 - val_accuracy: 0.7580\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 105s 351ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 1.8873 - val_accuracy: 0.7570\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 107s 357ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 1.6497 - val_accuracy: 0.7505\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 106s 353ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 1.7293 - val_accuracy: 0.7542\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 2.0478 - val_accuracy: 0.7552\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 76.82926654815674\n",
      "Training 9: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 150s 415ms/step - loss: 0.6121 - accuracy: 0.6527 - val_loss: 0.4588 - val_accuracy: 0.7852\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 104s 346ms/step - loss: 0.2336 - accuracy: 0.9073 - val_loss: 0.5194 - val_accuracy: 0.7627\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 101s 337ms/step - loss: 0.0863 - accuracy: 0.9743 - val_loss: 0.7427 - val_accuracy: 0.7645\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 101s 336ms/step - loss: 0.0324 - accuracy: 0.9905 - val_loss: 0.9361 - val_accuracy: 0.7749\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 1.1843 - val_accuracy: 0.7655\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 101s 337ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 1.3980 - val_accuracy: 0.7523\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 100s 334ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 1.4627 - val_accuracy: 0.7617\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 101s 335ms/step - loss: 0.0068 - accuracy: 0.9986 - val_loss: 1.4177 - val_accuracy: 0.7523\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 101s 337ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 1.4736 - val_accuracy: 0.7448\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 101s 335ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 1.8172 - val_accuracy: 0.7477\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 101s 337ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 1.8483 - val_accuracy: 0.7580\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.51782441139221\n",
      "Training 10: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 173s 495ms/step - loss: 0.6175 - accuracy: 0.6425 - val_loss: 0.4781 - val_accuracy: 0.7767\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 133s 442ms/step - loss: 0.2171 - accuracy: 0.9148 - val_loss: 0.5937 - val_accuracy: 0.7617\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 130s 433ms/step - loss: 0.0760 - accuracy: 0.9744 - val_loss: 0.8905 - val_accuracy: 0.7598\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 129s 431ms/step - loss: 0.0296 - accuracy: 0.9894 - val_loss: 0.9604 - val_accuracy: 0.7430\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 129s 429ms/step - loss: 0.0142 - accuracy: 0.9957 - val_loss: 1.1677 - val_accuracy: 0.7420\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 128s 427ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 1.3247 - val_accuracy: 0.7477\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 128s 427ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 1.5771 - val_accuracy: 0.7373\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 128s 425ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 1.5451 - val_accuracy: 0.7355\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 128s 425ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 1.2564 - val_accuracy: 0.7580\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 127s 425ms/step - loss: 0.0117 - accuracy: 0.9957 - val_loss: 1.3347 - val_accuracy: 0.7439\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 127s 425ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 2.3080 - val_accuracy: 0.7186\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.67354846000671\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  78.725398  79.381442  77.861166  76.172608  77.579737  74.108815   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.831147  76.829267  78.517824  77.673548  77.668095  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.725398</td>\n",
       "      <td>79.381442</td>\n",
       "      <td>77.861166</td>\n",
       "      <td>76.172608</td>\n",
       "      <td>77.579737</td>\n",
       "      <td>74.108815</td>\n",
       "      <td>79.831147</td>\n",
       "      <td>76.829267</td>\n",
       "      <td>78.517824</td>\n",
       "      <td>77.673548</td>\n",
       "      <td>77.668095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  78.725398  79.381442  77.861166  76.172608  77.579737  74.108815   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  79.831147  76.829267  78.517824  77.673548  77.668095  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_MR_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16448 words present from 18760 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.09184477, -0.97912855, -1.50660563, ...,  1.32925424,\n",
       "         0.56362925, -0.96337752],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 285,825\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 90s 231ms/step - loss: 0.6213 - accuracy: 0.6408 - val_loss: 0.5887 - val_accuracy: 0.6982\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 45s 152ms/step - loss: 0.4674 - accuracy: 0.7773 - val_loss: 0.5453 - val_accuracy: 0.7273\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.4074 - accuracy: 0.8112 - val_loss: 0.5106 - val_accuracy: 0.7516\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 45s 150ms/step - loss: 0.3682 - accuracy: 0.8365 - val_loss: 0.5143 - val_accuracy: 0.7516\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3166 - accuracy: 0.8583 - val_loss: 0.5383 - val_accuracy: 0.7573\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 45s 149ms/step - loss: 0.2588 - accuracy: 0.8910 - val_loss: 0.5686 - val_accuracy: 0.7629\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.2088 - accuracy: 0.9203 - val_loss: 0.8075 - val_accuracy: 0.7291\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 44s 145ms/step - loss: 0.1656 - accuracy: 0.9342 - val_loss: 0.7900 - val_accuracy: 0.7526\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 43s 145ms/step - loss: 0.1160 - accuracy: 0.9547 - val_loss: 0.8692 - val_accuracy: 0.7413\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 43s 144ms/step - loss: 0.0963 - accuracy: 0.9631 - val_loss: 0.8638 - val_accuracy: 0.7338\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 43s 143ms/step - loss: 0.0734 - accuracy: 0.9747 - val_loss: 0.9478 - val_accuracy: 0.7348\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 43s 145ms/step - loss: 0.0436 - accuracy: 0.9868 - val_loss: 1.2325 - val_accuracy: 0.7404\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 43s 143ms/step - loss: 0.0429 - accuracy: 0.9843 - val_loss: 1.2114 - val_accuracy: 0.7235\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 43s 144ms/step - loss: 0.0397 - accuracy: 0.9865 - val_loss: 1.3049 - val_accuracy: 0.7423\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 43s 143ms/step - loss: 0.0212 - accuracy: 0.9941 - val_loss: 1.3510 - val_accuracy: 0.7404\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 44s 146ms/step - loss: 0.0367 - accuracy: 0.9880 - val_loss: 1.4388 - val_accuracy: 0.7423\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 76.28865838050842\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 63s 141ms/step - loss: 0.6117 - accuracy: 0.6595 - val_loss: 0.4769 - val_accuracy: 0.7601\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 33s 108ms/step - loss: 0.4535 - accuracy: 0.7826 - val_loss: 0.4636 - val_accuracy: 0.7676\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.4069 - accuracy: 0.8129 - val_loss: 0.4548 - val_accuracy: 0.7835\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.3640 - accuracy: 0.8310 - val_loss: 0.5042 - val_accuracy: 0.7760\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 32s 105ms/step - loss: 0.2971 - accuracy: 0.8732 - val_loss: 0.5093 - val_accuracy: 0.7882\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.2665 - accuracy: 0.8892 - val_loss: 0.6102 - val_accuracy: 0.7704\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.2092 - accuracy: 0.9121 - val_loss: 0.6120 - val_accuracy: 0.7498\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1634 - accuracy: 0.9402 - val_loss: 0.7584 - val_accuracy: 0.7460\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1210 - accuracy: 0.9535 - val_loss: 0.7842 - val_accuracy: 0.7732\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0840 - accuracy: 0.9702 - val_loss: 0.8235 - val_accuracy: 0.7694\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 39s 129ms/step - loss: 0.0650 - accuracy: 0.9766 - val_loss: 0.9227 - val_accuracy: 0.7629\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 137s 457ms/step - loss: 0.0553 - accuracy: 0.9827 - val_loss: 1.4499 - val_accuracy: 0.7498\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 138s 460ms/step - loss: 0.0461 - accuracy: 0.9838 - val_loss: 1.1566 - val_accuracy: 0.7554\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 138s 459ms/step - loss: 0.0363 - accuracy: 0.9859 - val_loss: 1.2298 - val_accuracy: 0.7666\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 138s 459ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 1.2664 - val_accuracy: 0.7666\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 78.81911993026733\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 173s 505ms/step - loss: 0.6089 - accuracy: 0.6536 - val_loss: 0.5077 - val_accuracy: 0.7261\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 147s 489ms/step - loss: 0.4509 - accuracy: 0.7864 - val_loss: 0.4853 - val_accuracy: 0.7561\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 141s 470ms/step - loss: 0.4013 - accuracy: 0.8132 - val_loss: 0.4987 - val_accuracy: 0.7683\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 141s 470ms/step - loss: 0.3636 - accuracy: 0.8412 - val_loss: 0.4962 - val_accuracy: 0.7636\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 140s 465ms/step - loss: 0.3125 - accuracy: 0.8684 - val_loss: 0.4776 - val_accuracy: 0.7683\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 140s 465ms/step - loss: 0.2646 - accuracy: 0.8931 - val_loss: 0.5366 - val_accuracy: 0.7730\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 139s 465ms/step - loss: 0.2256 - accuracy: 0.9084 - val_loss: 0.6556 - val_accuracy: 0.7542\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 139s 465ms/step - loss: 0.1711 - accuracy: 0.9345 - val_loss: 0.7235 - val_accuracy: 0.7608\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 138s 462ms/step - loss: 0.1411 - accuracy: 0.9471 - val_loss: 0.7111 - val_accuracy: 0.7645\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 139s 462ms/step - loss: 0.1047 - accuracy: 0.9601 - val_loss: 0.8912 - val_accuracy: 0.7561\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 139s 462ms/step - loss: 0.0873 - accuracy: 0.9678 - val_loss: 1.0477 - val_accuracy: 0.7552\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 138s 461ms/step - loss: 0.0546 - accuracy: 0.9797 - val_loss: 1.0406 - val_accuracy: 0.7692\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 138s 461ms/step - loss: 0.0458 - accuracy: 0.9845 - val_loss: 0.9497 - val_accuracy: 0.7645\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 137s 458ms/step - loss: 0.0460 - accuracy: 0.9868 - val_loss: 1.2593 - val_accuracy: 0.7674\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 137s 458ms/step - loss: 0.0363 - accuracy: 0.9882 - val_loss: 1.1810 - val_accuracy: 0.7664\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 138s 461ms/step - loss: 0.0186 - accuracy: 0.9945 - val_loss: 1.3491 - val_accuracy: 0.7552\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.29831337928772\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 152s 438ms/step - loss: 0.6127 - accuracy: 0.6491 - val_loss: 0.4873 - val_accuracy: 0.7692\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 121s 403ms/step - loss: 0.4615 - accuracy: 0.7750 - val_loss: 0.4588 - val_accuracy: 0.7833\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 117s 391ms/step - loss: 0.4073 - accuracy: 0.8160 - val_loss: 0.4586 - val_accuracy: 0.7749\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 117s 389ms/step - loss: 0.3768 - accuracy: 0.8367 - val_loss: 0.5022 - val_accuracy: 0.7824\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 117s 390ms/step - loss: 0.3013 - accuracy: 0.8722 - val_loss: 0.5135 - val_accuracy: 0.7645\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 118s 394ms/step - loss: 0.2514 - accuracy: 0.8939 - val_loss: 0.5628 - val_accuracy: 0.7523\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 118s 393ms/step - loss: 0.2108 - accuracy: 0.9156 - val_loss: 0.6478 - val_accuracy: 0.7561\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 116s 388ms/step - loss: 0.1492 - accuracy: 0.9453 - val_loss: 0.7191 - val_accuracy: 0.7570\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 117s 390ms/step - loss: 0.1169 - accuracy: 0.9565 - val_loss: 0.8882 - val_accuracy: 0.7533\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 119s 397ms/step - loss: 0.0872 - accuracy: 0.9683 - val_loss: 1.0020 - val_accuracy: 0.7645\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 116s 388ms/step - loss: 0.0755 - accuracy: 0.9719 - val_loss: 1.1294 - val_accuracy: 0.7570\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 119s 398ms/step - loss: 0.0608 - accuracy: 0.9780 - val_loss: 1.1982 - val_accuracy: 0.7486\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.33020687103271\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 167s 467ms/step - loss: 0.6243 - accuracy: 0.6349 - val_loss: 0.5724 - val_accuracy: 0.6989\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 128s 427ms/step - loss: 0.4432 - accuracy: 0.7875 - val_loss: 0.5680 - val_accuracy: 0.7083\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 123s 411ms/step - loss: 0.4050 - accuracy: 0.8148 - val_loss: 0.6871 - val_accuracy: 0.7008\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 122s 408ms/step - loss: 0.3456 - accuracy: 0.8499 - val_loss: 0.7100 - val_accuracy: 0.7008\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 123s 410ms/step - loss: 0.3047 - accuracy: 0.8687 - val_loss: 0.6931 - val_accuracy: 0.7270\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 124s 412ms/step - loss: 0.2555 - accuracy: 0.8938 - val_loss: 0.7555 - val_accuracy: 0.7167\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 122s 407ms/step - loss: 0.2022 - accuracy: 0.9228 - val_loss: 0.7873 - val_accuracy: 0.7251\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 123s 410ms/step - loss: 0.1587 - accuracy: 0.9356 - val_loss: 1.1280 - val_accuracy: 0.7045\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 123s 410ms/step - loss: 0.1236 - accuracy: 0.9537 - val_loss: 1.0549 - val_accuracy: 0.7120\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 123s 409ms/step - loss: 0.1025 - accuracy: 0.9621 - val_loss: 1.0611 - val_accuracy: 0.7383\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 123s 410ms/step - loss: 0.0644 - accuracy: 0.9766 - val_loss: 1.6381 - val_accuracy: 0.7101\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 122s 408ms/step - loss: 0.0506 - accuracy: 0.9841 - val_loss: 1.1484 - val_accuracy: 0.7223\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 123s 412ms/step - loss: 0.0426 - accuracy: 0.9847 - val_loss: 1.6945 - val_accuracy: 0.7383\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 122s 408ms/step - loss: 0.0316 - accuracy: 0.9896 - val_loss: 1.6947 - val_accuracy: 0.7326\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 122s 408ms/step - loss: 0.0450 - accuracy: 0.9845 - val_loss: 1.5388 - val_accuracy: 0.7308\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 121s 403ms/step - loss: 0.0236 - accuracy: 0.9934 - val_loss: 1.6946 - val_accuracy: 0.7355\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 122s 406ms/step - loss: 0.0184 - accuracy: 0.9946 - val_loss: 1.4895 - val_accuracy: 0.7317\n",
      "Epoch 18/40\n",
      "300/300 [==============================] - 122s 405ms/step - loss: 0.0279 - accuracy: 0.9895 - val_loss: 2.0656 - val_accuracy: 0.7148\n",
      "Epoch 19/40\n",
      "300/300 [==============================] - 120s 400ms/step - loss: 0.0264 - accuracy: 0.9908 - val_loss: 1.9803 - val_accuracy: 0.7289\n",
      "Epoch 20/40\n",
      "300/300 [==============================] - 121s 404ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 2.4336 - val_accuracy: 0.7036\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 73.82739186286926\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 139s 392ms/step - loss: 0.6165 - accuracy: 0.6343 - val_loss: 0.4953 - val_accuracy: 0.7570\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 0.4558 - accuracy: 0.7815 - val_loss: 0.4704 - val_accuracy: 0.7767\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 97s 324ms/step - loss: 0.4052 - accuracy: 0.8113 - val_loss: 0.4791 - val_accuracy: 0.7655\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 97s 325ms/step - loss: 0.3676 - accuracy: 0.8334 - val_loss: 0.5787 - val_accuracy: 0.7645\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 0.3029 - accuracy: 0.8667 - val_loss: 0.5849 - val_accuracy: 0.7674\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 96s 319ms/step - loss: 0.2660 - accuracy: 0.8883 - val_loss: 0.6873 - val_accuracy: 0.7523\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 96s 320ms/step - loss: 0.2128 - accuracy: 0.9125 - val_loss: 0.6714 - val_accuracy: 0.7720\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 95s 317ms/step - loss: 0.1635 - accuracy: 0.9353 - val_loss: 0.7934 - val_accuracy: 0.7486\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.1315 - accuracy: 0.9491 - val_loss: 1.0686 - val_accuracy: 0.7467\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 95s 317ms/step - loss: 0.1010 - accuracy: 0.9628 - val_loss: 1.0269 - val_accuracy: 0.7608\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.0755 - accuracy: 0.9733 - val_loss: 1.3454 - val_accuracy: 0.7533\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.0709 - accuracy: 0.9750 - val_loss: 1.4588 - val_accuracy: 0.7383\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.67354846000671\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 165s 467ms/step - loss: 0.6115 - accuracy: 0.6378 - val_loss: 0.5404 - val_accuracy: 0.7326\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 142s 472ms/step - loss: 0.4509 - accuracy: 0.7859 - val_loss: 0.4578 - val_accuracy: 0.7795\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 137s 457ms/step - loss: 0.4138 - accuracy: 0.8110 - val_loss: 0.5003 - val_accuracy: 0.7692\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 137s 457ms/step - loss: 0.3611 - accuracy: 0.8400 - val_loss: 0.4898 - val_accuracy: 0.7645\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 136s 454ms/step - loss: 0.3083 - accuracy: 0.8718 - val_loss: 0.6093 - val_accuracy: 0.7505\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 136s 452ms/step - loss: 0.2654 - accuracy: 0.8819 - val_loss: 0.6367 - val_accuracy: 0.7523\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 135s 451ms/step - loss: 0.2181 - accuracy: 0.9109 - val_loss: 0.9280 - val_accuracy: 0.6979\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 136s 453ms/step - loss: 0.1856 - accuracy: 0.9264 - val_loss: 1.0616 - val_accuracy: 0.7195\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 136s 452ms/step - loss: 0.1263 - accuracy: 0.9516 - val_loss: 1.0810 - val_accuracy: 0.7214\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 136s 454ms/step - loss: 0.1060 - accuracy: 0.9593 - val_loss: 1.2924 - val_accuracy: 0.7195\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 136s 455ms/step - loss: 0.0761 - accuracy: 0.9725 - val_loss: 1.5280 - val_accuracy: 0.7073\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 137s 457ms/step - loss: 0.0589 - accuracy: 0.9773 - val_loss: 1.6581 - val_accuracy: 0.7158\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.95497179031372\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 186s 545ms/step - loss: 0.6027 - accuracy: 0.6641 - val_loss: 0.4924 - val_accuracy: 0.7589\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 122s 406ms/step - loss: 0.4361 - accuracy: 0.7931 - val_loss: 0.4758 - val_accuracy: 0.7674\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 120s 401ms/step - loss: 0.4006 - accuracy: 0.8199 - val_loss: 0.5013 - val_accuracy: 0.7674\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 120s 399ms/step - loss: 0.3501 - accuracy: 0.8470 - val_loss: 0.5156 - val_accuracy: 0.7552\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 119s 398ms/step - loss: 0.3008 - accuracy: 0.8692 - val_loss: 0.5512 - val_accuracy: 0.7439\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 119s 395ms/step - loss: 0.2700 - accuracy: 0.8861 - val_loss: 0.6278 - val_accuracy: 0.7542\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 119s 398ms/step - loss: 0.2042 - accuracy: 0.9196 - val_loss: 0.6577 - val_accuracy: 0.7336\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 118s 394ms/step - loss: 0.1527 - accuracy: 0.9379 - val_loss: 0.8177 - val_accuracy: 0.7392\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 119s 396ms/step - loss: 0.1210 - accuracy: 0.9541 - val_loss: 1.0029 - val_accuracy: 0.7317\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 118s 393ms/step - loss: 0.0833 - accuracy: 0.9705 - val_loss: 0.9783 - val_accuracy: 0.7495\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 119s 395ms/step - loss: 0.0586 - accuracy: 0.9792 - val_loss: 1.1641 - val_accuracy: 0.7439\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 118s 393ms/step - loss: 0.0533 - accuracy: 0.9800 - val_loss: 1.2921 - val_accuracy: 0.7486\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 76.73546075820923\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 146s 415ms/step - loss: 0.6152 - accuracy: 0.6500 - val_loss: 0.6030 - val_accuracy: 0.6876\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 116s 387ms/step - loss: 0.4559 - accuracy: 0.7836 - val_loss: 0.5065 - val_accuracy: 0.7411\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 115s 382ms/step - loss: 0.4237 - accuracy: 0.8061 - val_loss: 0.5434 - val_accuracy: 0.7298\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 114s 380ms/step - loss: 0.3668 - accuracy: 0.8370 - val_loss: 0.4937 - val_accuracy: 0.7767\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 114s 379ms/step - loss: 0.3145 - accuracy: 0.8667 - val_loss: 0.5135 - val_accuracy: 0.7702\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 113s 377ms/step - loss: 0.2773 - accuracy: 0.8858 - val_loss: 0.5034 - val_accuracy: 0.7880\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 110s 366ms/step - loss: 0.2270 - accuracy: 0.9071 - val_loss: 0.5345 - val_accuracy: 0.7767\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 108s 361ms/step - loss: 0.1803 - accuracy: 0.9268 - val_loss: 0.5710 - val_accuracy: 0.7758\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 109s 364ms/step - loss: 0.1551 - accuracy: 0.9357 - val_loss: 0.7482 - val_accuracy: 0.7674\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 109s 363ms/step - loss: 0.1056 - accuracy: 0.9564 - val_loss: 0.8522 - val_accuracy: 0.7505\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 108s 360ms/step - loss: 0.0932 - accuracy: 0.9658 - val_loss: 0.8721 - val_accuracy: 0.7542\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 109s 362ms/step - loss: 0.0687 - accuracy: 0.9732 - val_loss: 0.9587 - val_accuracy: 0.7655\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 108s 361ms/step - loss: 0.0512 - accuracy: 0.9824 - val_loss: 0.9956 - val_accuracy: 0.7580\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 108s 360ms/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 1.2124 - val_accuracy: 0.7477\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 108s 361ms/step - loss: 0.0354 - accuracy: 0.9882 - val_loss: 1.0843 - val_accuracy: 0.7608\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 107s 358ms/step - loss: 0.0430 - accuracy: 0.9865 - val_loss: 1.1548 - val_accuracy: 0.7505\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 78.79924774169922\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 144s 413ms/step - loss: 0.6077 - accuracy: 0.6446 - val_loss: 0.4898 - val_accuracy: 0.7711\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 108s 361ms/step - loss: 0.4475 - accuracy: 0.7847 - val_loss: 0.4940 - val_accuracy: 0.7495\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 106s 352ms/step - loss: 0.3929 - accuracy: 0.8199 - val_loss: 0.4759 - val_accuracy: 0.7702\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 106s 352ms/step - loss: 0.3540 - accuracy: 0.8366 - val_loss: 0.4755 - val_accuracy: 0.7664\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 107s 357ms/step - loss: 0.3124 - accuracy: 0.8631 - val_loss: 0.4988 - val_accuracy: 0.7711\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 104s 348ms/step - loss: 0.2555 - accuracy: 0.8874 - val_loss: 0.5350 - val_accuracy: 0.7758\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 101s 335ms/step - loss: 0.2013 - accuracy: 0.9146 - val_loss: 0.5815 - val_accuracy: 0.7608\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 101s 336ms/step - loss: 0.1635 - accuracy: 0.9340 - val_loss: 0.6478 - val_accuracy: 0.7636\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 102s 339ms/step - loss: 0.1283 - accuracy: 0.9507 - val_loss: 0.6898 - val_accuracy: 0.7570\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 102s 339ms/step - loss: 0.1005 - accuracy: 0.9643 - val_loss: 0.9044 - val_accuracy: 0.7402\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0840 - accuracy: 0.9708 - val_loss: 1.0571 - val_accuracy: 0.7514\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 102s 338ms/step - loss: 0.0568 - accuracy: 0.9812 - val_loss: 0.9945 - val_accuracy: 0.7636\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0479 - accuracy: 0.9837 - val_loss: 1.0368 - val_accuracy: 0.7345\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0360 - accuracy: 0.9881 - val_loss: 1.0988 - val_accuracy: 0.7261\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0275 - accuracy: 0.9916 - val_loss: 1.2581 - val_accuracy: 0.7542\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.0311 - accuracy: 0.9891 - val_loss: 1.4352 - val_accuracy: 0.7242\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.57973670959473\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  76.288658  78.81912  77.298313  78.330207  73.827392  77.673548  77.954972   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  76.735461  78.799248  77.579737  77.330666  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.288658</td>\n",
       "      <td>78.81912</td>\n",
       "      <td>77.298313</td>\n",
       "      <td>78.330207</td>\n",
       "      <td>73.827392</td>\n",
       "      <td>77.673548</td>\n",
       "      <td>77.954972</td>\n",
       "      <td>76.735461</td>\n",
       "      <td>78.799248</td>\n",
       "      <td>77.579737</td>\n",
       "      <td>77.330666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  76.288658  78.81912  77.298313  78.330207  73.827392  77.673548  77.954972   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  76.735461  78.799248  77.579737  77.330666  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_MR_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_45 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 181s 536ms/step - loss: 0.5886 - accuracy: 0.6659 - val_loss: 0.4602 - val_accuracy: 0.7619\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 158s 526ms/step - loss: 0.2730 - accuracy: 0.8901 - val_loss: 0.5827 - val_accuracy: 0.7788\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 177s 590ms/step - loss: 0.1297 - accuracy: 0.9514 - val_loss: 0.6569 - val_accuracy: 0.7657\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 177s 591ms/step - loss: 0.0453 - accuracy: 0.9843 - val_loss: 1.1875 - val_accuracy: 0.7666\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 177s 589ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 1.2101 - val_accuracy: 0.7694\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 175s 582ms/step - loss: 0.0132 - accuracy: 0.9964 - val_loss: 1.2877 - val_accuracy: 0.7545\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 176s 588ms/step - loss: 0.0162 - accuracy: 0.9964 - val_loss: 1.2178 - val_accuracy: 0.7619\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 176s 588ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 1.5813 - val_accuracy: 0.7648\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 177s 589ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 1.4405 - val_accuracy: 0.7591\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 175s 585ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.5390 - val_accuracy: 0.7582\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 175s 584ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 1.3583 - val_accuracy: 0.7676\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 175s 582ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 1.5616 - val_accuracy: 0.7507\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.88191437721252\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 232s 705ms/step - loss: 0.5998 - accuracy: 0.6523 - val_loss: 0.4287 - val_accuracy: 0.7957\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 175s 583ms/step - loss: 0.2713 - accuracy: 0.8879 - val_loss: 0.4394 - val_accuracy: 0.7919\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 169s 565ms/step - loss: 0.1198 - accuracy: 0.9621 - val_loss: 0.5010 - val_accuracy: 0.7816\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 170s 566ms/step - loss: 0.0524 - accuracy: 0.9840 - val_loss: 0.8264 - val_accuracy: 0.7704\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 169s 563ms/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.9404 - val_accuracy: 0.7526\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 167s 556ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 1.1092 - val_accuracy: 0.7610\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 166s 553ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 1.0611 - val_accuracy: 0.7741\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 166s 552ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 1.1690 - val_accuracy: 0.7713\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 167s 556ms/step - loss: 0.0096 - accuracy: 0.9962 - val_loss: 1.2013 - val_accuracy: 0.7873\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 166s 555ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 1.3837 - val_accuracy: 0.7826\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 166s 555ms/step - loss: 5.3841e-04 - accuracy: 1.0000 - val_loss: 1.6127 - val_accuracy: 0.7816\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.56888675689697\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 194s 579ms/step - loss: 0.5965 - accuracy: 0.6567 - val_loss: 0.5652 - val_accuracy: 0.7383\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 166s 554ms/step - loss: 0.2668 - accuracy: 0.8957 - val_loss: 0.6025 - val_accuracy: 0.7533\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 167s 557ms/step - loss: 0.1235 - accuracy: 0.9560 - val_loss: 0.8058 - val_accuracy: 0.7467\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 166s 554ms/step - loss: 0.0551 - accuracy: 0.9826 - val_loss: 0.8227 - val_accuracy: 0.7336\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 166s 552ms/step - loss: 0.0310 - accuracy: 0.9922 - val_loss: 1.4393 - val_accuracy: 0.7223\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 165s 551ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 1.4900 - val_accuracy: 0.7280\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 164s 545ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 1.8684 - val_accuracy: 0.7289\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 165s 552ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 1.8205 - val_accuracy: 0.7242\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 166s 552ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 1.8978 - val_accuracy: 0.7242\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 168s 559ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 1.5782 - val_accuracy: 0.7326\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 167s 557ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 1.9277 - val_accuracy: 0.7308\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 168s 559ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 2.0291 - val_accuracy: 0.7251\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 75.32833218574524\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 189s 543ms/step - loss: 0.5931 - accuracy: 0.6590 - val_loss: 0.4713 - val_accuracy: 0.7655\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 146s 487ms/step - loss: 0.2714 - accuracy: 0.8923 - val_loss: 0.5271 - val_accuracy: 0.7570\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 145s 483ms/step - loss: 0.1080 - accuracy: 0.9620 - val_loss: 0.7219 - val_accuracy: 0.7477\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 144s 480ms/step - loss: 0.0451 - accuracy: 0.9832 - val_loss: 1.0102 - val_accuracy: 0.7430\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 142s 472ms/step - loss: 0.0300 - accuracy: 0.9905 - val_loss: 1.0361 - val_accuracy: 0.7486\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 144s 479ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 1.4404 - val_accuracy: 0.7589\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 142s 473ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 1.3660 - val_accuracy: 0.7533\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 142s 474ms/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 1.4512 - val_accuracy: 0.7720\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 141s 471ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 1.5271 - val_accuracy: 0.7533\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 142s 473ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 1.4753 - val_accuracy: 0.7514\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 140s 468ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 1.5576 - val_accuracy: 0.7430\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 140s 468ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 1.7772 - val_accuracy: 0.7627\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 140s 468ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 1.4405 - val_accuracy: 0.7570\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 140s 466ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 1.5329 - val_accuracy: 0.7580\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 140s 466ms/step - loss: 0.0058 - accuracy: 0.9979 - val_loss: 1.8465 - val_accuracy: 0.7589\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 140s 467ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 1.7968 - val_accuracy: 0.7477\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 140s 466ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.5050 - val_accuracy: 0.7420\n",
      "Epoch 18/40\n",
      "300/300 [==============================] - 140s 465ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 1.7646 - val_accuracy: 0.7589\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.20450162887573\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 144s 412ms/step - loss: 0.5916 - accuracy: 0.6570 - val_loss: 0.4477 - val_accuracy: 0.7852\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 112s 372ms/step - loss: 0.2815 - accuracy: 0.8870 - val_loss: 0.4441 - val_accuracy: 0.7983\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 107s 358ms/step - loss: 0.1161 - accuracy: 0.9574 - val_loss: 0.5573 - val_accuracy: 0.7964\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 107s 357ms/step - loss: 0.0467 - accuracy: 0.9860 - val_loss: 0.7335 - val_accuracy: 0.7992\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 105s 351ms/step - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.8426 - val_accuracy: 0.8068\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.9681 - val_accuracy: 0.7739\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 1.1729 - val_accuracy: 0.8002\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 1.1775 - val_accuracy: 0.7899\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 106s 355ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 1.3285 - val_accuracy: 0.7908\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 1.3138 - val_accuracy: 0.7917\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 1.3943 - val_accuracy: 0.7936\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6260 - val_accuracy: 0.8049\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 107s 357ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 1.3463 - val_accuracy: 0.7833\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 107s 355ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.5163 - val_accuracy: 0.7889\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 106s 354ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 1.2834 - val_accuracy: 0.7992\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.67542314529419\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 194s 575ms/step - loss: 0.6077 - accuracy: 0.6357 - val_loss: 0.4396 - val_accuracy: 0.7880\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 182s 607ms/step - loss: 0.2774 - accuracy: 0.8823 - val_loss: 0.4650 - val_accuracy: 0.7842\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 175s 585ms/step - loss: 0.1253 - accuracy: 0.9528 - val_loss: 0.6101 - val_accuracy: 0.7805\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 173s 575ms/step - loss: 0.0432 - accuracy: 0.9868 - val_loss: 0.9093 - val_accuracy: 0.7730\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 171s 571ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 1.0593 - val_accuracy: 0.7805\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 171s 569ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 1.1049 - val_accuracy: 0.7805\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 171s 569ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 1.3015 - val_accuracy: 0.7814\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 171s 569ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 1.5232 - val_accuracy: 0.7805\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 170s 568ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 1.4986 - val_accuracy: 0.7824\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 170s 567ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 1.7368 - val_accuracy: 0.7936\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 172s 572ms/step - loss: 5.5879e-04 - accuracy: 1.0000 - val_loss: 1.8229 - val_accuracy: 0.7880\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 171s 571ms/step - loss: 1.0977e-04 - accuracy: 1.0000 - val_loss: 1.9215 - val_accuracy: 0.7908\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 172s 573ms/step - loss: 5.0682e-05 - accuracy: 1.0000 - val_loss: 1.9889 - val_accuracy: 0.7908\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 173s 575ms/step - loss: 3.9691e-05 - accuracy: 1.0000 - val_loss: 2.0503 - val_accuracy: 0.7880\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 172s 574ms/step - loss: 2.9328e-05 - accuracy: 1.0000 - val_loss: 2.0986 - val_accuracy: 0.7889\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 172s 572ms/step - loss: 2.7605e-05 - accuracy: 1.0000 - val_loss: 2.1446 - val_accuracy: 0.7899\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 173s 576ms/step - loss: 4.3494e-05 - accuracy: 1.0000 - val_loss: 2.1864 - val_accuracy: 0.7889\n",
      "Epoch 18/40\n",
      "300/300 [==============================] - 173s 578ms/step - loss: 1.5998e-05 - accuracy: 1.0000 - val_loss: 2.2291 - val_accuracy: 0.7889\n",
      "Epoch 19/40\n",
      "300/300 [==============================] - 172s 574ms/step - loss: 1.2960e-05 - accuracy: 1.0000 - val_loss: 2.2727 - val_accuracy: 0.7889\n",
      "Epoch 20/40\n",
      "300/300 [==============================] - 172s 572ms/step - loss: 9.9020e-06 - accuracy: 1.0000 - val_loss: 2.3091 - val_accuracy: 0.7899\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 79.36210036277771\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 180s 534ms/step - loss: 0.5983 - accuracy: 0.6573 - val_loss: 0.4515 - val_accuracy: 0.7777\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 135s 450ms/step - loss: 0.2644 - accuracy: 0.8952 - val_loss: 0.4766 - val_accuracy: 0.7777\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 133s 442ms/step - loss: 0.1139 - accuracy: 0.9621 - val_loss: 0.6145 - val_accuracy: 0.7655\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 133s 442ms/step - loss: 0.0468 - accuracy: 0.9843 - val_loss: 0.9215 - val_accuracy: 0.7542\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 132s 442ms/step - loss: 0.0285 - accuracy: 0.9906 - val_loss: 1.1039 - val_accuracy: 0.7598\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 133s 443ms/step - loss: 0.0149 - accuracy: 0.9952 - val_loss: 1.2759 - val_accuracy: 0.7439\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 133s 443ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 1.3097 - val_accuracy: 0.7636\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 132s 440ms/step - loss: 0.0110 - accuracy: 0.9960 - val_loss: 1.3261 - val_accuracy: 0.7608\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 133s 443ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 1.3534 - val_accuracy: 0.7589\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 132s 440ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 1.6518 - val_accuracy: 0.7505\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 132s 441ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.6461 - val_accuracy: 0.7514\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.76735424995422\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 177s 519ms/step - loss: 0.5848 - accuracy: 0.6734 - val_loss: 0.4580 - val_accuracy: 0.7842\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 161s 536ms/step - loss: 0.2691 - accuracy: 0.8969 - val_loss: 0.4982 - val_accuracy: 0.7655\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 158s 527ms/step - loss: 0.1185 - accuracy: 0.9629 - val_loss: 0.6352 - val_accuracy: 0.7674\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 156s 520ms/step - loss: 0.0494 - accuracy: 0.9841 - val_loss: 0.9670 - val_accuracy: 0.7486\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 155s 518ms/step - loss: 0.0219 - accuracy: 0.9927 - val_loss: 1.3096 - val_accuracy: 0.7542\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 155s 516ms/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 1.5040 - val_accuracy: 0.7373\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 154s 513ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 1.3134 - val_accuracy: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40\n",
      "300/300 [==============================] - 152s 507ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 1.1416 - val_accuracy: 0.7561\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 153s 511ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 1.4765 - val_accuracy: 0.7448\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 154s 514ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 1.5427 - val_accuracy: 0.7402\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 155s 517ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 1.9532 - val_accuracy: 0.7495\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.42401266098022\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 183s 545ms/step - loss: 0.5976 - accuracy: 0.6642 - val_loss: 0.4578 - val_accuracy: 0.7871\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 133s 443ms/step - loss: 0.2601 - accuracy: 0.8936 - val_loss: 0.4983 - val_accuracy: 0.7805\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 132s 439ms/step - loss: 0.1296 - accuracy: 0.9540 - val_loss: 0.7272 - val_accuracy: 0.7833\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 132s 441ms/step - loss: 0.0491 - accuracy: 0.9849 - val_loss: 0.8092 - val_accuracy: 0.7664\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 132s 441ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 1.0694 - val_accuracy: 0.7842\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 131s 438ms/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 1.1449 - val_accuracy: 0.7880\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 131s 438ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 1.3654 - val_accuracy: 0.7767\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 131s 438ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 1.3014 - val_accuracy: 0.7795\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 131s 437ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 1.3947 - val_accuracy: 0.7786\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 131s 437ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 1.4826 - val_accuracy: 0.7786\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 132s 439ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 1.6528 - val_accuracy: 0.7645\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 130s 435ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 1.3421 - val_accuracy: 0.7730\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 130s 434ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 1.1955 - val_accuracy: 0.7627\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 130s 434ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 1.5060 - val_accuracy: 0.7580\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 133s 443ms/step - loss: 5.6888e-04 - accuracy: 1.0000 - val_loss: 1.7681 - val_accuracy: 0.7561\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 129s 430ms/step - loss: 1.5537e-04 - accuracy: 1.0000 - val_loss: 1.8941 - val_accuracy: 0.7570\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 78.79924774169922\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 184s 551ms/step - loss: 0.5965 - accuracy: 0.6619 - val_loss: 0.4365 - val_accuracy: 0.7917\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 152s 507ms/step - loss: 0.2711 - accuracy: 0.8840 - val_loss: 0.4630 - val_accuracy: 0.7833\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 151s 502ms/step - loss: 0.1254 - accuracy: 0.9567 - val_loss: 0.6662 - val_accuracy: 0.7664\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 147s 491ms/step - loss: 0.0494 - accuracy: 0.9827 - val_loss: 0.9091 - val_accuracy: 0.7636\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 152s 508ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 0.9969 - val_accuracy: 0.7561\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 152s 508ms/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 1.2468 - val_accuracy: 0.7561\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 153s 510ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 1.2502 - val_accuracy: 0.7486\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 152s 508ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 1.4692 - val_accuracy: 0.7617\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 152s 508ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 1.3788 - val_accuracy: 0.7392\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 154s 513ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 1.4982 - val_accuracy: 0.7645\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 155s 518ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 1.5666 - val_accuracy: 0.7608\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.17448282241821\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  78.725398  79.381442  77.861166  76.172608  77.579737  74.108815   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.831147  76.829267  78.517824  77.673548  77.668095  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.881914</td>\n",
       "      <td>79.568887</td>\n",
       "      <td>75.328332</td>\n",
       "      <td>77.204502</td>\n",
       "      <td>80.675423</td>\n",
       "      <td>79.3621</td>\n",
       "      <td>77.767354</td>\n",
       "      <td>78.424013</td>\n",
       "      <td>78.799248</td>\n",
       "      <td>79.174483</td>\n",
       "      <td>78.418626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5     acc6       acc7  \\\n",
       "0  77.881914  79.568887  75.328332  77.204502  80.675423  79.3621  77.767354   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  78.424013  78.799248  79.174483  78.418626  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_MR_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
