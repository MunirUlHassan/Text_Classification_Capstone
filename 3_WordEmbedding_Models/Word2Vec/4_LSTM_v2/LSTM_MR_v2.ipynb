{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LSTM_MR_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yOFg_xHQIEIN",
        "30vZh2q2IEIa"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LugL67TZIEGn"
      },
      "source": [
        "# LSTM Classification with MR Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using LSTM model on the MR Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sY0Dl39ItRD",
        "outputId": "6ae4641c-58ba-4930-b754-e106a97b3601"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_7K1vgIEHB",
        "outputId": "55582aa9-e86e-4184-de54-b5ce32ef5535"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mlUenWpIEHI",
        "outputId": "5d9a28bf-ad24-43e3-9ee9-7fd60defb09c"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xUgSmsVIEHN"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "Re5BBlpNIEHP",
        "outputId": "d8d0498c-a493-47c7-d66a-2d251916ff94"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Disertasi/0_data/MR/MR.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10662, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simplistic , silly and tedious .</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it 's so laddish and juvenile , only teenage b...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>exploitative and largely devoid of the depth o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>garbus discards the potential for pathological...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a visually flashy but narratively opaque and e...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10657</th>\n",
              "      <td>both exuberantly romantic and serenely melanch...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10658</th>\n",
              "      <td>mazel tov to a film about a family 's joyous l...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10659</th>\n",
              "      <td>standing in the shadows of motown is the best ...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10660</th>\n",
              "      <td>it 's nice to see piscopo again after all thes...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10661</th>\n",
              "      <td>provides a porthole into that noble , tremblin...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10662 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label  split\n",
              "0                       simplistic , silly and tedious .      0  train\n",
              "1      it 's so laddish and juvenile , only teenage b...      0  train\n",
              "2      exploitative and largely devoid of the depth o...      0  train\n",
              "3      garbus discards the potential for pathological...      0  train\n",
              "4      a visually flashy but narratively opaque and e...      0  train\n",
              "...                                                  ...    ...    ...\n",
              "10657  both exuberantly romantic and serenely melanch...      1  train\n",
              "10658  mazel tov to a film about a family 's joyous l...      1  train\n",
              "10659  standing in the shadows of motown is the best ...      1  train\n",
              "10660  it 's nice to see piscopo again after all thes...      1  train\n",
              "10661  provides a porthole into that noble , tremblin...      1  train\n",
              "\n",
              "[10662 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCB2TQhOIEHR",
        "outputId": "fd84735e-1aff-47b3-cc7e-abe5c3c105ce"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10662 entries, 0 to 10661\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10662 non-null  object\n",
            " 1   label     10662 non-null  int64 \n",
            " 2   split     10662 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 250.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "2Hd8GXcgIEHT",
        "outputId": "f1ab217a-ba73-4b2f-85ab-be1986bfdb0c"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5331</td>\n",
              "      <td>5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5331</td>\n",
              "      <td>5331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          5331   5331\n",
              "1          5331   5331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI7NkSjiIEHs"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "doQFd0kXIEHw",
        "outputId": "da114ca9-3eab-4982-9a8d-f805949ca092"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'simplistic , silly and tedious .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn8fHDuqIEHz"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6T95NL5IEH5"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnyySFHAIEIE"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_mm-tv9IEIG",
        "outputId": "3375d7b7-0f54-4e19-bb08-e2fab0b66bc6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
            "Into a sequence of int: [3, 544, 1838, 13, 3909, 3366, 4, 658, 2629, 416, 10, 236, 4, 10112]\n",
            "Into a padded sequence: [    3   544  1838    13  3909  3366     4   658  2629   416    10   236\n",
            "     4 10112     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-4gENooIEII",
        "outputId": "4941de31-59d7-4ece-d4c6-f885aa972b97"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "a 3\n",
            "and 4\n",
            "of 5\n",
            "to 6\n",
            "is 7\n",
            "'s 8\n",
            "it 9\n",
            "in 10\n",
            "18760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6thFX8tIEIM"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOFg_xHQIEIN"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vhyoO_IEIO"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm4YhaJ_IEIQ",
        "outputId": "8bb3e4a2-9c5c-49fb-dcaa-77f83ec32113"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 585,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egU-ePC0IEIW"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30vZh2q2IEIa"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "M3Z5FD7zIEIc",
        "outputId": "33cdf8db-388f-464e-f420-308a231e7713"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 89s 223ms/step - loss: 0.6163 - accuracy: 0.6415 - val_loss: 0.4619 - val_accuracy: 0.7873\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.2410 - accuracy: 0.9080 - val_loss: 0.4809 - val_accuracy: 0.7835\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 60s 199ms/step - loss: 0.0838 - accuracy: 0.9738 - val_loss: 0.8936 - val_accuracy: 0.7723\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.0359 - accuracy: 0.9884 - val_loss: 1.0687 - val_accuracy: 0.7610\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.0181 - accuracy: 0.9941 - val_loss: 1.0059 - val_accuracy: 0.7666\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 1.3534 - val_accuracy: 0.7460\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 60s 200ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 1.3551 - val_accuracy: 0.7619\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 58s 192ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 1.3780 - val_accuracy: 0.7591\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 1.5912 - val_accuracy: 0.7535\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 58s 195ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 1.6829 - val_accuracy: 0.7535\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 59s 196ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.7011 - val_accuracy: 0.7413\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 78.72539758682251\n",
            "Training 2: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 118s 318ms/step - loss: 0.6152 - accuracy: 0.6361 - val_loss: 0.4630 - val_accuracy: 0.7938\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 62s 207ms/step - loss: 0.2397 - accuracy: 0.9073 - val_loss: 0.4972 - val_accuracy: 0.7676\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 62s 205ms/step - loss: 0.0860 - accuracy: 0.9717 - val_loss: 0.8153 - val_accuracy: 0.7666\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 61s 204ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.9457 - val_accuracy: 0.7591\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 61s 204ms/step - loss: 0.0139 - accuracy: 0.9948 - val_loss: 1.1122 - val_accuracy: 0.7554\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 1.0272 - val_accuracy: 0.7526\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 61s 203ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 1.3724 - val_accuracy: 0.7648\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 61s 202ms/step - loss: 0.0089 - accuracy: 0.9968 - val_loss: 1.4249 - val_accuracy: 0.7545\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 60s 202ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 1.5103 - val_accuracy: 0.7657\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 1.2798 - val_accuracy: 0.7488\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.0104 - accuracy: 0.9971 - val_loss: 1.2610 - val_accuracy: 0.7685\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 79.38144207000732\n",
            "Training 3: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 98s 257ms/step - loss: 0.6156 - accuracy: 0.6446 - val_loss: 0.4884 - val_accuracy: 0.7786\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 61s 202ms/step - loss: 0.2263 - accuracy: 0.9184 - val_loss: 0.5748 - val_accuracy: 0.7589\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 61s 203ms/step - loss: 0.0855 - accuracy: 0.9706 - val_loss: 0.9211 - val_accuracy: 0.7542\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.0333 - accuracy: 0.9903 - val_loss: 1.0110 - val_accuracy: 0.7495\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.0139 - accuracy: 0.9968 - val_loss: 1.3935 - val_accuracy: 0.7439\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 59s 196ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 1.4038 - val_accuracy: 0.7486\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 58s 195ms/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 1.3696 - val_accuracy: 0.7355\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 58s 194ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 1.5199 - val_accuracy: 0.7411\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 58s 194ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 1.3418 - val_accuracy: 0.7345\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 1.5688 - val_accuracy: 0.7420\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 58s 194ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 1.6057 - val_accuracy: 0.7448\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 77.86116600036621\n",
            "Training 4: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 132s 364ms/step - loss: 0.6129 - accuracy: 0.6464 - val_loss: 0.4897 - val_accuracy: 0.7580\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 98s 326ms/step - loss: 0.2313 - accuracy: 0.9120 - val_loss: 0.5398 - val_accuracy: 0.7608\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 102s 341ms/step - loss: 0.0878 - accuracy: 0.9707 - val_loss: 0.6626 - val_accuracy: 0.7523\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 102s 341ms/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 0.9263 - val_accuracy: 0.7608\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 101s 335ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 1.2530 - val_accuracy: 0.7448\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 98s 326ms/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 1.1869 - val_accuracy: 0.7364\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 98s 327ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 1.5908 - val_accuracy: 0.7411\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 102s 341ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 1.4224 - val_accuracy: 0.7617\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 100s 334ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 1.5891 - val_accuracy: 0.7430\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 101s 336ms/step - loss: 0.0072 - accuracy: 0.9974 - val_loss: 1.7488 - val_accuracy: 0.7430\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 102s 339ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 1.5451 - val_accuracy: 0.7542\n",
            "Epoch 12/30\n",
            "300/300 [==============================] - 98s 326ms/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 1.9033 - val_accuracy: 0.7430\n",
            "Epoch 13/30\n",
            "300/300 [==============================] - 99s 330ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 1.8613 - val_accuracy: 0.7533\n",
            "Epoch 14/30\n",
            "300/300 [==============================] - 100s 334ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4764 - val_accuracy: 0.7467\n",
            "Epoch 15/30\n",
            "300/300 [==============================] - 99s 332ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 1.9913 - val_accuracy: 0.7308\n",
            "Epoch 16/30\n",
            "300/300 [==============================] - 100s 332ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 2.0612 - val_accuracy: 0.7523\n",
            "Epoch 17/30\n",
            "300/300 [==============================] - 99s 329ms/step - loss: 1.5674e-04 - accuracy: 1.0000 - val_loss: 2.1487 - val_accuracy: 0.7495\n",
            "Epoch 18/30\n",
            "300/300 [==============================] - 101s 336ms/step - loss: 4.8509e-05 - accuracy: 1.0000 - val_loss: 2.2041 - val_accuracy: 0.7495\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00018: early stopping\n",
            "Test Accuracy: 76.17260813713074\n",
            "Training 5: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 135s 365ms/step - loss: 0.6159 - accuracy: 0.6464 - val_loss: 0.4819 - val_accuracy: 0.7523\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 94s 315ms/step - loss: 0.2388 - accuracy: 0.9054 - val_loss: 0.5181 - val_accuracy: 0.7655\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 95s 316ms/step - loss: 0.0723 - accuracy: 0.9757 - val_loss: 0.7113 - val_accuracy: 0.7589\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 94s 314ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.8845 - val_accuracy: 0.7655\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 94s 313ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 1.2295 - val_accuracy: 0.7758\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 96s 319ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 1.2899 - val_accuracy: 0.7674\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 95s 316ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 1.3138 - val_accuracy: 0.7636\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 95s 316ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 1.4661 - val_accuracy: 0.7514\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 94s 315ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 1.3368 - val_accuracy: 0.7589\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 92s 306ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 1.3798 - val_accuracy: 0.7598\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 95s 315ms/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 1.3736 - val_accuracy: 0.7373\n",
            "Epoch 12/30\n",
            "300/300 [==============================] - 94s 314ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 1.2887 - val_accuracy: 0.7467\n",
            "Epoch 13/30\n",
            "300/300 [==============================] - 95s 316ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.5043 - val_accuracy: 0.7420\n",
            "Epoch 14/30\n",
            "300/300 [==============================] - 95s 315ms/step - loss: 2.1403e-04 - accuracy: 1.0000 - val_loss: 1.6725 - val_accuracy: 0.7514\n",
            "Epoch 15/30\n",
            "300/300 [==============================] - 94s 314ms/step - loss: 6.6886e-05 - accuracy: 1.0000 - val_loss: 1.7618 - val_accuracy: 0.7523\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 77.57973670959473\n",
            "Training 6: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 161s 463ms/step - loss: 0.6121 - accuracy: 0.6472 - val_loss: 0.4982 - val_accuracy: 0.7411\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 119s 396ms/step - loss: 0.2205 - accuracy: 0.9105 - val_loss: 0.6212 - val_accuracy: 0.7402\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 119s 396ms/step - loss: 0.0704 - accuracy: 0.9769 - val_loss: 0.9785 - val_accuracy: 0.7205\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 117s 391ms/step - loss: 0.0329 - accuracy: 0.9898 - val_loss: 1.3391 - val_accuracy: 0.7233\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 116s 387ms/step - loss: 0.0174 - accuracy: 0.9934 - val_loss: 1.1648 - val_accuracy: 0.7223\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 116s 386ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 1.4568 - val_accuracy: 0.7167\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 114s 382ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 1.8598 - val_accuracy: 0.7176\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 115s 383ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 1.9447 - val_accuracy: 0.7355\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 115s 382ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 1.4689 - val_accuracy: 0.7298\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 114s 381ms/step - loss: 0.0095 - accuracy: 0.9961 - val_loss: 1.7466 - val_accuracy: 0.7148\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 114s 382ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 1.8244 - val_accuracy: 0.7111\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 74.10881519317627\n",
            "Training 7: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 164s 475ms/step - loss: 0.6122 - accuracy: 0.6484 - val_loss: 0.4659 - val_accuracy: 0.7983\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 105s 350ms/step - loss: 0.2274 - accuracy: 0.9145 - val_loss: 0.4764 - val_accuracy: 0.7786\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 101s 338ms/step - loss: 0.0957 - accuracy: 0.9681 - val_loss: 0.7749 - val_accuracy: 0.7636\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 100s 333ms/step - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.9793 - val_accuracy: 0.7598\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 98s 326ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 1.1208 - val_accuracy: 0.7627\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 97s 324ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 1.1534 - val_accuracy: 0.7505\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 97s 323ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 1.4461 - val_accuracy: 0.7383\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 97s 324ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 1.6305 - val_accuracy: 0.7486\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 97s 325ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 1.3478 - val_accuracy: 0.7458\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 97s 323ms/step - loss: 0.0068 - accuracy: 0.9972 - val_loss: 1.4536 - val_accuracy: 0.7523\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 96s 321ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 1.5065 - val_accuracy: 0.7411\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 79.83114719390869\n",
            "Training 8: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 162s 462ms/step - loss: 0.6117 - accuracy: 0.6401 - val_loss: 0.4850 - val_accuracy: 0.7598\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 107s 357ms/step - loss: 0.2308 - accuracy: 0.9098 - val_loss: 0.5121 - val_accuracy: 0.7580\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 107s 358ms/step - loss: 0.0800 - accuracy: 0.9743 - val_loss: 0.6773 - val_accuracy: 0.7542\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 108s 360ms/step - loss: 0.0373 - accuracy: 0.9908 - val_loss: 1.0124 - val_accuracy: 0.7636\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 107s 355ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 1.1467 - val_accuracy: 0.7627\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 106s 354ms/step - loss: 0.0151 - accuracy: 0.9944 - val_loss: 1.2540 - val_accuracy: 0.7561\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 106s 354ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 1.5997 - val_accuracy: 0.7448\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 105s 350ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 1.4299 - val_accuracy: 0.7608\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 106s 354ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 1.4432 - val_accuracy: 0.7608\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 106s 355ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 1.5454 - val_accuracy: 0.7317\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 106s 354ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 1.3563 - val_accuracy: 0.7514\n",
            "Epoch 12/30\n",
            "300/300 [==============================] - 105s 350ms/step - loss: 0.0050 - accuracy: 0.9980 - val_loss: 1.5222 - val_accuracy: 0.7683\n",
            "Epoch 13/30\n",
            "300/300 [==============================] - 106s 352ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 1.7980 - val_accuracy: 0.7505\n",
            "Epoch 14/30\n",
            "300/300 [==============================] - 105s 352ms/step - loss: 5.9609e-04 - accuracy: 0.9999 - val_loss: 1.9773 - val_accuracy: 0.7523\n",
            "Epoch 15/30\n",
            "300/300 [==============================] - 105s 350ms/step - loss: 6.1359e-04 - accuracy: 0.9999 - val_loss: 2.0328 - val_accuracy: 0.7514\n",
            "Epoch 16/30\n",
            "300/300 [==============================] - 105s 351ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 1.7846 - val_accuracy: 0.7495\n",
            "Epoch 17/30\n",
            "300/300 [==============================] - 105s 352ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 2.0226 - val_accuracy: 0.7533\n",
            "Epoch 18/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "300/300 [==============================] - 106s 354ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 1.6029 - val_accuracy: 0.7580\n",
            "Epoch 19/30\n",
            "300/300 [==============================] - 105s 351ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 1.8873 - val_accuracy: 0.7570\n",
            "Epoch 20/30\n",
            "300/300 [==============================] - 107s 357ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 1.6497 - val_accuracy: 0.7505\n",
            "Epoch 21/30\n",
            "300/300 [==============================] - 106s 353ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 1.7293 - val_accuracy: 0.7542\n",
            "Epoch 22/30\n",
            "300/300 [==============================] - 106s 354ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 2.0478 - val_accuracy: 0.7552\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00022: early stopping\n",
            "Test Accuracy: 76.82926654815674\n",
            "Training 9: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 150s 415ms/step - loss: 0.6121 - accuracy: 0.6527 - val_loss: 0.4588 - val_accuracy: 0.7852\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 104s 346ms/step - loss: 0.2336 - accuracy: 0.9073 - val_loss: 0.5194 - val_accuracy: 0.7627\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 101s 337ms/step - loss: 0.0863 - accuracy: 0.9743 - val_loss: 0.7427 - val_accuracy: 0.7645\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 101s 336ms/step - loss: 0.0324 - accuracy: 0.9905 - val_loss: 0.9361 - val_accuracy: 0.7749\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 101s 338ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 1.1843 - val_accuracy: 0.7655\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 101s 337ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 1.3980 - val_accuracy: 0.7523\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 100s 334ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 1.4627 - val_accuracy: 0.7617\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 101s 335ms/step - loss: 0.0068 - accuracy: 0.9986 - val_loss: 1.4177 - val_accuracy: 0.7523\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 101s 337ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 1.4736 - val_accuracy: 0.7448\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 101s 335ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 1.8172 - val_accuracy: 0.7477\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 101s 337ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 1.8483 - val_accuracy: 0.7580\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 78.51782441139221\n",
            "Training 10: \n",
            "Epoch 1/30\n",
            "300/300 [==============================] - 173s 495ms/step - loss: 0.6175 - accuracy: 0.6425 - val_loss: 0.4781 - val_accuracy: 0.7767\n",
            "Epoch 2/30\n",
            "300/300 [==============================] - 133s 442ms/step - loss: 0.2171 - accuracy: 0.9148 - val_loss: 0.5937 - val_accuracy: 0.7617\n",
            "Epoch 3/30\n",
            "300/300 [==============================] - 130s 433ms/step - loss: 0.0760 - accuracy: 0.9744 - val_loss: 0.8905 - val_accuracy: 0.7598\n",
            "Epoch 4/30\n",
            "300/300 [==============================] - 129s 431ms/step - loss: 0.0296 - accuracy: 0.9894 - val_loss: 0.9604 - val_accuracy: 0.7430\n",
            "Epoch 5/30\n",
            "300/300 [==============================] - 129s 429ms/step - loss: 0.0142 - accuracy: 0.9957 - val_loss: 1.1677 - val_accuracy: 0.7420\n",
            "Epoch 6/30\n",
            "300/300 [==============================] - 128s 427ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 1.3247 - val_accuracy: 0.7477\n",
            "Epoch 7/30\n",
            "300/300 [==============================] - 128s 427ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 1.5771 - val_accuracy: 0.7373\n",
            "Epoch 8/30\n",
            "300/300 [==============================] - 128s 425ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 1.5451 - val_accuracy: 0.7355\n",
            "Epoch 9/30\n",
            "300/300 [==============================] - 128s 425ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 1.2564 - val_accuracy: 0.7580\n",
            "Epoch 10/30\n",
            "300/300 [==============================] - 127s 425ms/step - loss: 0.0117 - accuracy: 0.9957 - val_loss: 1.3347 - val_accuracy: 0.7439\n",
            "Epoch 11/30\n",
            "300/300 [==============================] - 127s 425ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 2.3080 - val_accuracy: 0.7186\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 77.67354846000671\n",
            "\n",
            "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
            "0  78.725398  79.381442  77.861166  76.172608  77.579737  74.108815   \n",
            "\n",
            "        acc7       acc8       acc9      acc10        AVG  \n",
            "0  79.831147  76.829267  78.517824  77.673548  77.668095  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJcmWJcZIEIh"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gatChLUuIEI1",
        "outputId": "f9de628b-4911-4278-f74c-5e94270b3482"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78.725398</td>\n",
              "      <td>79.381442</td>\n",
              "      <td>77.861166</td>\n",
              "      <td>76.172608</td>\n",
              "      <td>77.579737</td>\n",
              "      <td>74.108815</td>\n",
              "      <td>79.831147</td>\n",
              "      <td>76.829267</td>\n",
              "      <td>78.517824</td>\n",
              "      <td>77.673548</td>\n",
              "      <td>77.668095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
              "0  78.725398  79.381442  77.861166  76.172608  77.579737  74.108815   \n",
              "\n",
              "        acc7       acc8       acc9      acc10        AVG  \n",
              "0  79.831147  76.829267  78.517824  77.673548  77.668095  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Oy7S7GIEI2"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('LSTM_MR_v2.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsbYBDyJIEI3"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6Fa-zLIEI4"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc0kpZ9tIEI6"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7lzCMxSIEI7"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Disertasi/WordEmbedding_Models/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GidN7L8sIEI9",
        "outputId": "0b7a0a50-563e-4cda-d03d-12b6ed5a2266"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8eRZH_fIEI_"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGmCJ9AZIEJA"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y10Wp4vIEJB",
        "outputId": "97fd0125-806c-4e36-eb68-ba9a155aeea3"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16448 words present from 18760 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEYd4Dh5IEJD"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SNu4VD4IEJE",
        "outputId": "abb682c9-1291-4439-9f9d-3264b1c06f04"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah23BETVIEJF"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgzI3Tp-IEJG",
        "outputId": "e0b04c9e-7b7f-4494-9688-32423be10813"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZTTBJD-IEJH"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmihisuhIEJI"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkIxKBTQIEJK",
        "outputId": "9940b495-3702-485c-de9c-88f4c97ec7cd"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 285,825\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gybW9N1sIEJL"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mjxTzgHIEJM"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G99LfRQIEJN",
        "outputId": "4e85ccce-3a3f-4b06-be1c-781276197fec"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 73s 200ms/step - loss: 0.5787 - accuracy: 0.6827 - val_loss: 0.4736 - val_accuracy: 0.7638\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 0.4485 - accuracy: 0.7843 - val_loss: 0.4468 - val_accuracy: 0.7826\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 53s 176ms/step - loss: 0.4138 - accuracy: 0.8032 - val_loss: 0.4094 - val_accuracy: 0.8116\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 0.3766 - accuracy: 0.8274 - val_loss: 0.4212 - val_accuracy: 0.7966\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 55s 183ms/step - loss: 0.3605 - accuracy: 0.8375 - val_loss: 0.4420 - val_accuracy: 0.8022\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 53s 178ms/step - loss: 0.3025 - accuracy: 0.8643 - val_loss: 0.4380 - val_accuracy: 0.8013\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 53s 178ms/step - loss: 0.2566 - accuracy: 0.8911 - val_loss: 0.4719 - val_accuracy: 0.7901\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 0.2127 - accuracy: 0.9127 - val_loss: 0.5347 - val_accuracy: 0.7891\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 55s 182ms/step - loss: 0.1625 - accuracy: 0.9329 - val_loss: 0.6245 - val_accuracy: 0.7882\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 54s 180ms/step - loss: 0.1242 - accuracy: 0.9524 - val_loss: 0.6875 - val_accuracy: 0.7891\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 54s 181ms/step - loss: 0.0930 - accuracy: 0.9677 - val_loss: 0.7135 - val_accuracy: 0.8051\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 81.1621367931366\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 84s 237ms/step - loss: 0.5576 - accuracy: 0.7051 - val_loss: 0.5263 - val_accuracy: 0.7507\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 69s 231ms/step - loss: 0.4494 - accuracy: 0.7920 - val_loss: 0.4765 - val_accuracy: 0.7619\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 62s 208ms/step - loss: 0.4137 - accuracy: 0.8059 - val_loss: 0.4771 - val_accuracy: 0.7676\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 65s 218ms/step - loss: 0.3857 - accuracy: 0.8220 - val_loss: 0.4794 - val_accuracy: 0.7751\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 62s 205ms/step - loss: 0.3452 - accuracy: 0.8480 - val_loss: 0.4734 - val_accuracy: 0.7835\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 61s 203ms/step - loss: 0.3041 - accuracy: 0.8696 - val_loss: 0.5499 - val_accuracy: 0.7619\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 60s 200ms/step - loss: 0.2769 - accuracy: 0.8820 - val_loss: 0.5032 - val_accuracy: 0.7732\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.2399 - accuracy: 0.9057 - val_loss: 0.5620 - val_accuracy: 0.7816\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 60s 200ms/step - loss: 0.1908 - accuracy: 0.9246 - val_loss: 0.6321 - val_accuracy: 0.7863\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 61s 202ms/step - loss: 0.1368 - accuracy: 0.9449 - val_loss: 0.6508 - val_accuracy: 0.7835\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.1132 - accuracy: 0.9582 - val_loss: 0.7782 - val_accuracy: 0.7779\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 62s 205ms/step - loss: 0.0831 - accuracy: 0.9705 - val_loss: 0.7835 - val_accuracy: 0.7891\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.0715 - accuracy: 0.9752 - val_loss: 0.9935 - val_accuracy: 0.7723\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 62s 206ms/step - loss: 0.0519 - accuracy: 0.9820 - val_loss: 0.9423 - val_accuracy: 0.7676\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 62s 207ms/step - loss: 0.0451 - accuracy: 0.9844 - val_loss: 1.0699 - val_accuracy: 0.7779\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 61s 203ms/step - loss: 0.0345 - accuracy: 0.9889 - val_loss: 1.2427 - val_accuracy: 0.7816\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.0353 - accuracy: 0.9872 - val_loss: 1.1104 - val_accuracy: 0.7798\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 62s 206ms/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 1.3594 - val_accuracy: 0.7535\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.0335 - accuracy: 0.9896 - val_loss: 1.3193 - val_accuracy: 0.7676\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 1.4464 - val_accuracy: 0.7685\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00020: early stopping\n",
            "Test Accuracy: 78.91284227371216\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 86s 223ms/step - loss: 0.5657 - accuracy: 0.7019 - val_loss: 0.4779 - val_accuracy: 0.7495\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.4418 - accuracy: 0.7890 - val_loss: 0.4873 - val_accuracy: 0.7683\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 59s 196ms/step - loss: 0.4114 - accuracy: 0.8051 - val_loss: 0.4605 - val_accuracy: 0.7674\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 62s 207ms/step - loss: 0.3820 - accuracy: 0.8183 - val_loss: 0.4579 - val_accuracy: 0.7842\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 62s 206ms/step - loss: 0.3540 - accuracy: 0.8407 - val_loss: 0.4695 - val_accuracy: 0.7795\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 60s 199ms/step - loss: 0.3159 - accuracy: 0.8578 - val_loss: 0.4904 - val_accuracy: 0.7711\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 58s 192ms/step - loss: 0.2703 - accuracy: 0.8855 - val_loss: 0.5339 - val_accuracy: 0.7664\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 57s 190ms/step - loss: 0.2216 - accuracy: 0.9048 - val_loss: 0.6045 - val_accuracy: 0.7402\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.1966 - accuracy: 0.9161 - val_loss: 0.6162 - val_accuracy: 0.7627\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 0.1365 - accuracy: 0.9460 - val_loss: 0.7543 - val_accuracy: 0.7514\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.1020 - accuracy: 0.9624 - val_loss: 0.9135 - val_accuracy: 0.7467\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 62s 206ms/step - loss: 0.0864 - accuracy: 0.9661 - val_loss: 1.0194 - val_accuracy: 0.7486\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 78.42401266098022\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 87s 226ms/step - loss: 0.5715 - accuracy: 0.6923 - val_loss: 0.4943 - val_accuracy: 0.7589\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 60s 199ms/step - loss: 0.4475 - accuracy: 0.7858 - val_loss: 0.4462 - val_accuracy: 0.7720\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 61s 202ms/step - loss: 0.4128 - accuracy: 0.8058 - val_loss: 0.4835 - val_accuracy: 0.7458\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.3811 - accuracy: 0.8249 - val_loss: 0.4491 - val_accuracy: 0.7758\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.3495 - accuracy: 0.8445 - val_loss: 0.4626 - val_accuracy: 0.7674\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.3170 - accuracy: 0.8573 - val_loss: 0.4931 - val_accuracy: 0.7777\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 55s 185ms/step - loss: 0.2719 - accuracy: 0.8878 - val_loss: 0.5100 - val_accuracy: 0.7692\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.2315 - accuracy: 0.9056 - val_loss: 0.5454 - val_accuracy: 0.7617\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.1864 - accuracy: 0.9259 - val_loss: 0.6662 - val_accuracy: 0.7702\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.1449 - accuracy: 0.9403 - val_loss: 0.7046 - val_accuracy: 0.7711\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 56s 186ms/step - loss: 0.1027 - accuracy: 0.9638 - val_loss: 0.7889 - val_accuracy: 0.7683\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.0771 - accuracy: 0.9739 - val_loss: 0.8420 - val_accuracy: 0.7580\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.0537 - accuracy: 0.9827 - val_loss: 0.9240 - val_accuracy: 0.7777\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.0533 - accuracy: 0.9819 - val_loss: 1.1780 - val_accuracy: 0.7598\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 77.76735424995422\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 74s 202ms/step - loss: 0.5749 - accuracy: 0.6861 - val_loss: 0.4529 - val_accuracy: 0.7805\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 59s 198ms/step - loss: 0.4631 - accuracy: 0.7771 - val_loss: 0.4422 - val_accuracy: 0.7992\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.4127 - accuracy: 0.8021 - val_loss: 0.4402 - val_accuracy: 0.8021\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.3847 - accuracy: 0.8201 - val_loss: 0.4301 - val_accuracy: 0.7992\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.3529 - accuracy: 0.8390 - val_loss: 0.4424 - val_accuracy: 0.7936\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.3068 - accuracy: 0.8650 - val_loss: 0.4485 - val_accuracy: 0.7927\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.2719 - accuracy: 0.8816 - val_loss: 0.4882 - val_accuracy: 0.7824\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.2270 - accuracy: 0.9051 - val_loss: 0.5221 - val_accuracy: 0.7833\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.1752 - accuracy: 0.9285 - val_loss: 0.5963 - val_accuracy: 0.7927\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 0.1350 - accuracy: 0.9459 - val_loss: 0.7623 - val_accuracy: 0.7702\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.1161 - accuracy: 0.9571 - val_loss: 0.7857 - val_accuracy: 0.7777\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 80.20637631416321\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 85s 231ms/step - loss: 0.5700 - accuracy: 0.6959 - val_loss: 0.4913 - val_accuracy: 0.7655\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.4370 - accuracy: 0.7900 - val_loss: 0.4717 - val_accuracy: 0.7814\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 62s 207ms/step - loss: 0.3991 - accuracy: 0.8125 - val_loss: 0.4776 - val_accuracy: 0.7852\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 61s 203ms/step - loss: 0.3695 - accuracy: 0.8291 - val_loss: 0.4818 - val_accuracy: 0.7786\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 61s 204ms/step - loss: 0.3400 - accuracy: 0.8446 - val_loss: 0.5004 - val_accuracy: 0.7889\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.2883 - accuracy: 0.8751 - val_loss: 0.5286 - val_accuracy: 0.7833\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 59s 197ms/step - loss: 0.2505 - accuracy: 0.8935 - val_loss: 0.5837 - val_accuracy: 0.7683\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 65s 218ms/step - loss: 0.2061 - accuracy: 0.9158 - val_loss: 0.6496 - val_accuracy: 0.7598\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1660 - accuracy: 0.9350 - val_loss: 0.6574 - val_accuracy: 0.7692\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1207 - accuracy: 0.9585 - val_loss: 0.8197 - val_accuracy: 0.7674\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.0913 - accuracy: 0.9648 - val_loss: 0.9961 - val_accuracy: 0.7617\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.0710 - accuracy: 0.9717 - val_loss: 1.0183 - val_accuracy: 0.7617\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.0508 - accuracy: 0.9846 - val_loss: 1.1683 - val_accuracy: 0.7795\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 78.8930594921112\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 89s 232ms/step - loss: 0.5718 - accuracy: 0.7004 - val_loss: 0.4908 - val_accuracy: 0.7561\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.4527 - accuracy: 0.7861 - val_loss: 0.4630 - val_accuracy: 0.7683\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.4104 - accuracy: 0.8039 - val_loss: 0.4253 - val_accuracy: 0.8049\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.3734 - accuracy: 0.8313 - val_loss: 0.4273 - val_accuracy: 0.8011\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.3394 - accuracy: 0.8507 - val_loss: 0.4354 - val_accuracy: 0.7871\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 65s 218ms/step - loss: 0.3106 - accuracy: 0.8601 - val_loss: 0.4720 - val_accuracy: 0.7899\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 65s 217ms/step - loss: 0.2654 - accuracy: 0.8876 - val_loss: 0.4928 - val_accuracy: 0.7955\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 65s 217ms/step - loss: 0.2134 - accuracy: 0.9128 - val_loss: 0.5067 - val_accuracy: 0.7824\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.1715 - accuracy: 0.9270 - val_loss: 0.5381 - val_accuracy: 0.7889\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 66s 219ms/step - loss: 0.1305 - accuracy: 0.9517 - val_loss: 0.6643 - val_accuracy: 0.7842\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.1065 - accuracy: 0.9585 - val_loss: 0.7670 - val_accuracy: 0.7767\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 80.48780560493469\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 82s 222ms/step - loss: 0.5683 - accuracy: 0.7011 - val_loss: 0.4518 - val_accuracy: 0.7852\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.4421 - accuracy: 0.7900 - val_loss: 0.4753 - val_accuracy: 0.7711\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 65s 217ms/step - loss: 0.4138 - accuracy: 0.8002 - val_loss: 0.4390 - val_accuracy: 0.7946\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 65s 218ms/step - loss: 0.3869 - accuracy: 0.8262 - val_loss: 0.4886 - val_accuracy: 0.7730\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.3541 - accuracy: 0.8394 - val_loss: 0.4578 - val_accuracy: 0.7749\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 66s 218ms/step - loss: 0.3110 - accuracy: 0.8604 - val_loss: 0.5130 - val_accuracy: 0.7711\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.2732 - accuracy: 0.8896 - val_loss: 0.5660 - val_accuracy: 0.7749\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.2237 - accuracy: 0.9066 - val_loss: 0.5969 - val_accuracy: 0.7767\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1859 - accuracy: 0.9205 - val_loss: 0.6573 - val_accuracy: 0.7936\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 62s 208ms/step - loss: 0.1419 - accuracy: 0.9440 - val_loss: 0.7449 - val_accuracy: 0.7711\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 58s 192ms/step - loss: 0.1022 - accuracy: 0.9609 - val_loss: 0.7985 - val_accuracy: 0.7730\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 79.4559121131897\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 83s 217ms/step - loss: 0.5750 - accuracy: 0.6845 - val_loss: 0.4724 - val_accuracy: 0.7589\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 57s 188ms/step - loss: 0.4374 - accuracy: 0.7979 - val_loss: 0.4499 - val_accuracy: 0.7720\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 62s 205ms/step - loss: 0.4038 - accuracy: 0.8053 - val_loss: 0.4678 - val_accuracy: 0.7842\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 66s 219ms/step - loss: 0.3743 - accuracy: 0.8279 - val_loss: 0.4453 - val_accuracy: 0.7758\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 62s 205ms/step - loss: 0.3490 - accuracy: 0.8442 - val_loss: 0.4776 - val_accuracy: 0.7636\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 57s 188ms/step - loss: 0.3176 - accuracy: 0.8638 - val_loss: 0.5041 - val_accuracy: 0.7711\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 57s 190ms/step - loss: 0.2688 - accuracy: 0.8854 - val_loss: 0.5298 - val_accuracy: 0.7645\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.2163 - accuracy: 0.9119 - val_loss: 0.6027 - val_accuracy: 0.7645\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.1803 - accuracy: 0.9293 - val_loss: 0.6850 - val_accuracy: 0.7674\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 58s 192ms/step - loss: 0.1274 - accuracy: 0.9509 - val_loss: 0.8003 - val_accuracy: 0.7570\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.0837 - accuracy: 0.9697 - val_loss: 0.8393 - val_accuracy: 0.7617\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 78.42401266098022\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 74s 202ms/step - loss: 0.5801 - accuracy: 0.6753 - val_loss: 0.4554 - val_accuracy: 0.7824\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.4543 - accuracy: 0.7786 - val_loss: 0.4814 - val_accuracy: 0.7730\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.4304 - accuracy: 0.7947 - val_loss: 0.4544 - val_accuracy: 0.7842\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 61s 204ms/step - loss: 0.3820 - accuracy: 0.8242 - val_loss: 0.4434 - val_accuracy: 0.7983\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.3450 - accuracy: 0.8479 - val_loss: 0.4821 - val_accuracy: 0.7917\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 56s 186ms/step - loss: 0.3099 - accuracy: 0.8636 - val_loss: 0.5007 - val_accuracy: 0.7786\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.2567 - accuracy: 0.8920 - val_loss: 0.5129 - val_accuracy: 0.7889\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 57s 190ms/step - loss: 0.2021 - accuracy: 0.9223 - val_loss: 0.5682 - val_accuracy: 0.7749\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.1594 - accuracy: 0.9350 - val_loss: 0.6094 - val_accuracy: 0.7739\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 57s 190ms/step - loss: 0.1225 - accuracy: 0.9503 - val_loss: 0.7694 - val_accuracy: 0.7777\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 0.0894 - accuracy: 0.9646 - val_loss: 0.8837 - val_accuracy: 0.7805\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 57s 190ms/step - loss: 0.0647 - accuracy: 0.9753 - val_loss: 0.9631 - val_accuracy: 0.7514\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 79.83114719390869\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
            "0  81.162137  78.912842  78.424013  ...  78.424013  79.831147  79.356466\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbvBxwYfIEJP"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "QwNHacp2IEJQ",
        "outputId": "53bf48ef-35ca-4400-f9c0-af8ab1810225"
      },
      "source": [
        "record2"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.162137</td>\n",
              "      <td>78.912842</td>\n",
              "      <td>78.424013</td>\n",
              "      <td>77.767354</td>\n",
              "      <td>80.206376</td>\n",
              "      <td>78.893059</td>\n",
              "      <td>80.487806</td>\n",
              "      <td>79.455912</td>\n",
              "      <td>78.424013</td>\n",
              "      <td>79.831147</td>\n",
              "      <td>79.356466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
              "0  81.162137  78.912842  78.424013  ...  78.424013  79.831147  79.356466\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOTdxJfuIEJR"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('LSTM_MR_v2_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfmr0v7FIEJS"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62m4y2xzIEJT"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKSum4DhIEJU"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klAsL8AYIEJV"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oux7l2PzIEJW",
        "outputId": "5018a852-aac2-4193-c081-4166c7823d62"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_22 (Bidirectio (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_23 (Bidirectio (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 585,825\n",
            "Trainable params: 585,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBtoJhAIEJX"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3WqIoYgIEJY"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9YKuYA1IEJZ",
        "outputId": "7805afef-7f2b-4b5d-c082-a4c90779093a"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record3)\n",
        "print()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 102s 281ms/step - loss: 0.5647 - accuracy: 0.6916 - val_loss: 0.4353 - val_accuracy: 0.7985\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 0.2590 - accuracy: 0.8996 - val_loss: 0.4730 - val_accuracy: 0.7891\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 0.0969 - accuracy: 0.9663 - val_loss: 0.7006 - val_accuracy: 0.7798\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 82s 272ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.9412 - val_accuracy: 0.7844\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 82s 275ms/step - loss: 0.0211 - accuracy: 0.9938 - val_loss: 1.1069 - val_accuracy: 0.7648\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 83s 277ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 1.0875 - val_accuracy: 0.7732\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 85s 283ms/step - loss: 0.0091 - accuracy: 0.9963 - val_loss: 1.4839 - val_accuracy: 0.7629\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 98s 326ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 1.8082 - val_accuracy: 0.7694\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 82s 273ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 1.7069 - val_accuracy: 0.7657\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.85004782676697\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 100s 288ms/step - loss: 0.5613 - accuracy: 0.6980 - val_loss: 0.4160 - val_accuracy: 0.8107\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 82s 272ms/step - loss: 0.2513 - accuracy: 0.9046 - val_loss: 0.5309 - val_accuracy: 0.7657\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 86s 286ms/step - loss: 0.1086 - accuracy: 0.9616 - val_loss: 0.6182 - val_accuracy: 0.7723\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 91s 303ms/step - loss: 0.0414 - accuracy: 0.9878 - val_loss: 1.2821 - val_accuracy: 0.7601\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 87s 290ms/step - loss: 0.0165 - accuracy: 0.9942 - val_loss: 1.0957 - val_accuracy: 0.7591\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 85s 284ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 1.2557 - val_accuracy: 0.7619\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 82s 275ms/step - loss: 0.0109 - accuracy: 0.9964 - val_loss: 1.7396 - val_accuracy: 0.7498\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 82s 273ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 1.5430 - val_accuracy: 0.7507\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 81s 269ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 1.4709 - val_accuracy: 0.7591\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 81.06841444969177\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 99s 284ms/step - loss: 0.5588 - accuracy: 0.7143 - val_loss: 0.4328 - val_accuracy: 0.7936\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 80s 266ms/step - loss: 0.2562 - accuracy: 0.9005 - val_loss: 0.4761 - val_accuracy: 0.7720\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 80s 267ms/step - loss: 0.1078 - accuracy: 0.9624 - val_loss: 0.8333 - val_accuracy: 0.7570\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 81s 271ms/step - loss: 0.0420 - accuracy: 0.9859 - val_loss: 0.8874 - val_accuracy: 0.7889\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 1.0291 - val_accuracy: 0.7683\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 79s 264ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 1.1566 - val_accuracy: 0.7702\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 81s 269ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 1.3413 - val_accuracy: 0.7711\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 80s 265ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 1.2866 - val_accuracy: 0.7655\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 82s 273ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 1.3370 - val_accuracy: 0.7683\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.36210036277771\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 103s 288ms/step - loss: 0.5545 - accuracy: 0.7060 - val_loss: 0.4354 - val_accuracy: 0.7795\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 83s 277ms/step - loss: 0.2530 - accuracy: 0.8960 - val_loss: 0.4943 - val_accuracy: 0.7899\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 83s 276ms/step - loss: 0.0995 - accuracy: 0.9639 - val_loss: 0.6044 - val_accuracy: 0.7767\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 81s 271ms/step - loss: 0.0462 - accuracy: 0.9846 - val_loss: 0.8747 - val_accuracy: 0.7702\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 81s 270ms/step - loss: 0.0208 - accuracy: 0.9937 - val_loss: 1.1979 - val_accuracy: 0.7664\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 82s 272ms/step - loss: 0.0133 - accuracy: 0.9953 - val_loss: 1.1409 - val_accuracy: 0.7655\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 81s 269ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 1.4944 - val_accuracy: 0.7608\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 80s 267ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 1.3506 - val_accuracy: 0.7523\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 82s 272ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 1.6314 - val_accuracy: 0.7692\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 81s 270ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 1.5479 - val_accuracy: 0.7467\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 78.98686528205872\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 98s 282ms/step - loss: 0.5530 - accuracy: 0.7033 - val_loss: 0.4116 - val_accuracy: 0.8068\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 83s 277ms/step - loss: 0.2549 - accuracy: 0.9033 - val_loss: 0.4792 - val_accuracy: 0.7917\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 82s 272ms/step - loss: 0.0993 - accuracy: 0.9656 - val_loss: 0.6241 - val_accuracy: 0.7852\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 82s 274ms/step - loss: 0.0378 - accuracy: 0.9882 - val_loss: 0.6931 - val_accuracy: 0.7899\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 83s 275ms/step - loss: 0.0196 - accuracy: 0.9950 - val_loss: 1.1652 - val_accuracy: 0.7814\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 81s 269ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 1.1453 - val_accuracy: 0.7852\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 82s 273ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 1.1007 - val_accuracy: 0.7927\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 1.1931 - val_accuracy: 0.7842\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0092 - accuracy: 0.9966 - val_loss: 1.2915 - val_accuracy: 0.7842\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.67542314529419\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 110s 319ms/step - loss: 0.5534 - accuracy: 0.7027 - val_loss: 0.4411 - val_accuracy: 0.7974\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 83s 275ms/step - loss: 0.2553 - accuracy: 0.8983 - val_loss: 0.5118 - val_accuracy: 0.7702\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 81s 271ms/step - loss: 0.1016 - accuracy: 0.9630 - val_loss: 0.7476 - val_accuracy: 0.7458\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 85s 285ms/step - loss: 0.0418 - accuracy: 0.9871 - val_loss: 0.9604 - val_accuracy: 0.7655\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 86s 287ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 1.1340 - val_accuracy: 0.7514\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 83s 275ms/step - loss: 0.0162 - accuracy: 0.9947 - val_loss: 1.0021 - val_accuracy: 0.7674\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 83s 277ms/step - loss: 0.0128 - accuracy: 0.9967 - val_loss: 1.3351 - val_accuracy: 0.7561\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 81s 270ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 1.5301 - val_accuracy: 0.7636\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 80s 265ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 1.8915 - val_accuracy: 0.7645\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.7373354434967\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 100s 289ms/step - loss: 0.5755 - accuracy: 0.6750 - val_loss: 0.4063 - val_accuracy: 0.8152\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.2650 - accuracy: 0.8958 - val_loss: 0.4306 - val_accuracy: 0.8021\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 82s 274ms/step - loss: 0.1057 - accuracy: 0.9640 - val_loss: 0.6172 - val_accuracy: 0.7833\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0494 - accuracy: 0.9839 - val_loss: 1.0014 - val_accuracy: 0.7824\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 83s 277ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 1.1529 - val_accuracy: 0.7749\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 84s 280ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 1.1959 - val_accuracy: 0.7786\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 87s 289ms/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 1.3339 - val_accuracy: 0.7880\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 86s 288ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 1.1943 - val_accuracy: 0.7767\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 85s 282ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 1.2723 - val_accuracy: 0.7814\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 81.51969909667969\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 105s 300ms/step - loss: 0.5595 - accuracy: 0.6936 - val_loss: 0.4295 - val_accuracy: 0.8030\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 0.2545 - accuracy: 0.8965 - val_loss: 0.5069 - val_accuracy: 0.7917\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 83s 277ms/step - loss: 0.1072 - accuracy: 0.9632 - val_loss: 0.6434 - val_accuracy: 0.7814\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 84s 279ms/step - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.9383 - val_accuracy: 0.7767\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 82s 273ms/step - loss: 0.0267 - accuracy: 0.9921 - val_loss: 1.0224 - val_accuracy: 0.7786\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 86s 285ms/step - loss: 0.0134 - accuracy: 0.9951 - val_loss: 1.3018 - val_accuracy: 0.7795\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 86s 286ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 1.3995 - val_accuracy: 0.7720\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 84s 278ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 1.7066 - val_accuracy: 0.7702\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 89s 298ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 1.3116 - val_accuracy: 0.7683\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.3001880645752\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 115s 316ms/step - loss: 0.5392 - accuracy: 0.7124 - val_loss: 0.4552 - val_accuracy: 0.7777\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 86s 288ms/step - loss: 0.2503 - accuracy: 0.9021 - val_loss: 0.5451 - val_accuracy: 0.7664\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 85s 284ms/step - loss: 0.1003 - accuracy: 0.9659 - val_loss: 0.7650 - val_accuracy: 0.7655\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 0.0397 - accuracy: 0.9879 - val_loss: 0.9834 - val_accuracy: 0.7598\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 86s 286ms/step - loss: 0.0209 - accuracy: 0.9933 - val_loss: 1.2322 - val_accuracy: 0.7627\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 86s 286ms/step - loss: 0.0174 - accuracy: 0.9933 - val_loss: 1.4983 - val_accuracy: 0.7608\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 91s 302ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 1.6482 - val_accuracy: 0.7533\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 88s 293ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 1.4802 - val_accuracy: 0.7598\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 89s 297ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 1.6610 - val_accuracy: 0.7533\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 77.76735424995422\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 108s 310ms/step - loss: 0.5412 - accuracy: 0.7168 - val_loss: 0.4325 - val_accuracy: 0.8030\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 86s 287ms/step - loss: 0.2489 - accuracy: 0.9005 - val_loss: 0.5646 - val_accuracy: 0.7871\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 90s 299ms/step - loss: 0.0941 - accuracy: 0.9689 - val_loss: 0.7224 - val_accuracy: 0.7824\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 91s 302ms/step - loss: 0.0464 - accuracy: 0.9844 - val_loss: 1.0247 - val_accuracy: 0.7786\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 90s 300ms/step - loss: 0.0267 - accuracy: 0.9916 - val_loss: 1.0497 - val_accuracy: 0.7702\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 91s 302ms/step - loss: 0.0111 - accuracy: 0.9953 - val_loss: 1.2887 - val_accuracy: 0.7758\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 90s 301ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 1.3312 - val_accuracy: 0.7702\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 89s 297ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 1.4969 - val_accuracy: 0.7739\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 92s 305ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 1.7143 - val_accuracy: 0.7664\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.3001880645752\n",
            "\n",
            "        acc1       acc2     acc3  ...       acc9      acc10        AVG\n",
            "0  79.850048  81.068414  79.3621  ...  77.767354  80.300188  79.956762\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap-hplfDIEJb"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "LAhObttdIEJc",
        "outputId": "757723c6-9838-4b0b-8334-182748aea342"
      },
      "source": [
        "record3"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>79.850048</td>\n",
              "      <td>81.068414</td>\n",
              "      <td>79.3621</td>\n",
              "      <td>78.986865</td>\n",
              "      <td>80.675423</td>\n",
              "      <td>79.737335</td>\n",
              "      <td>81.519699</td>\n",
              "      <td>80.300188</td>\n",
              "      <td>77.767354</td>\n",
              "      <td>80.300188</td>\n",
              "      <td>79.956762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2     acc3  ...       acc9      acc10        AVG\n",
              "0  79.850048  81.068414  79.3621  ...  77.767354  80.300188  79.956762\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVdKPwzhIEJd"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('LSTM_MR_v2_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}