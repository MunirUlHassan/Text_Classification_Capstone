{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with CR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the Customer Reviews Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3775, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weaknesses are minor the feel and layout of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many of our disney movies do n 't play on this...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player has a problem with dual layer dvd 's su...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know the saying is you get what you pay for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will never purchase apex again .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>so far , the anti spam feature seems to be ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>i did not have any of the installation problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>their products have been great and have saved ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     weaknesses are minor the feel and layout of th...      0  train\n",
       "1     many of our disney movies do n 't play on this...      0  train\n",
       "2     player has a problem with dual layer dvd 's su...      0  train\n",
       "3     i know the saying is you get what you pay for ...      0  train\n",
       "4                      will never purchase apex again .      0  train\n",
       "...                                                 ...    ...    ...\n",
       "3770  so far , the anti spam feature seems to be ver...      1  train\n",
       "3771  i downloaded a trial version of computer assoc...      1  train\n",
       "3772  i did not have any of the installation problem...      1  train\n",
       "3773  their products have been great and have saved ...      1  train\n",
       "3774                                                         1  train\n",
       "\n",
       "[3775 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/CR/CR.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3775 entries, 0 to 3774\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3775 non-null   object\n",
      " 1   label     3775 non-null   int32 \n",
      " 2   split     3775 non-null   object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 73.9+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2407</td>\n",
       "      <td>2407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          1368   1368\n",
       "1          2407   2407"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weaknesses are minor the feel and layout of the remote control are only so so . it does n 't show the complete file names of mp3s with really long names . you must cycle through every zoom setting ( 2x , 3x , 4x , 1 2x , etc . ) before getting back to normal size sorry if i 'm just ignorant of a way to get back to 1x quickly .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  will never purchase apex again .\n",
      "Into a sequence of int: [72, 194, 285, 207, 286]\n",
      "Into a padded sequence: [ 72 194 285 207 286   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "and 3\n",
      "i 4\n",
      "it 5\n",
      "to 6\n",
      "a 7\n",
      "is 8\n",
      "of 9\n",
      "this 10\n",
      "5336\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 78s 470ms/step - loss: 0.6128 - accuracy: 0.6679 - val_loss: 0.4231 - val_accuracy: 0.7884\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 44s 413ms/step - loss: 0.2795 - accuracy: 0.8922 - val_loss: 0.4191 - val_accuracy: 0.7910\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 48s 445ms/step - loss: 0.1241 - accuracy: 0.9616 - val_loss: 0.5523 - val_accuracy: 0.7937\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 42s 390ms/step - loss: 0.0839 - accuracy: 0.9714 - val_loss: 0.6551 - val_accuracy: 0.7857\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.0420 - accuracy: 0.9861 - val_loss: 0.8798 - val_accuracy: 0.7857\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 40s 373ms/step - loss: 0.0245 - accuracy: 0.9918 - val_loss: 1.0067 - val_accuracy: 0.7804\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 1.2655 - val_accuracy: 0.7672\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 1.2974 - val_accuracy: 0.7672\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 100s 668ms/step - loss: 0.6113 - accuracy: 0.6602 - val_loss: 0.4669 - val_accuracy: 0.7460\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 44s 414ms/step - loss: 0.2786 - accuracy: 0.8907 - val_loss: 0.5015 - val_accuracy: 0.7963\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 53s 496ms/step - loss: 0.1183 - accuracy: 0.9593 - val_loss: 0.6743 - val_accuracy: 0.7778\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 54s 506ms/step - loss: 0.0607 - accuracy: 0.9797 - val_loss: 0.7511 - val_accuracy: 0.7751\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 55s 514ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 1.0973 - val_accuracy: 0.7698\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 54s 509ms/step - loss: 0.0309 - accuracy: 0.9911 - val_loss: 1.0613 - val_accuracy: 0.7460\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 0.0252 - accuracy: 0.9903 - val_loss: 1.1595 - val_accuracy: 0.7698\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 91s 588ms/step - loss: 0.6021 - accuracy: 0.6604 - val_loss: 0.4229 - val_accuracy: 0.8254\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 39s 368ms/step - loss: 0.2401 - accuracy: 0.9050 - val_loss: 0.4453 - val_accuracy: 0.8095\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 35s 329ms/step - loss: 0.1177 - accuracy: 0.9594 - val_loss: 0.7022 - val_accuracy: 0.7831\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 34s 322ms/step - loss: 0.0612 - accuracy: 0.9790 - val_loss: 0.8245 - val_accuracy: 0.8122\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.0503 - accuracy: 0.9826 - val_loss: 1.0977 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 1.0010 - val_accuracy: 0.7725\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 142s 1s/step - loss: 0.5984 - accuracy: 0.6810 - val_loss: 0.4452 - val_accuracy: 0.7963\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 90s 843ms/step - loss: 0.2601 - accuracy: 0.8946 - val_loss: 0.4446 - val_accuracy: 0.7963\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 67s 624ms/step - loss: 0.1257 - accuracy: 0.9581 - val_loss: 0.5813 - val_accuracy: 0.8016\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 65s 605ms/step - loss: 0.0632 - accuracy: 0.9785 - val_loss: 0.6529 - val_accuracy: 0.7751\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 65s 604ms/step - loss: 0.0439 - accuracy: 0.9869 - val_loss: 0.9480 - val_accuracy: 0.7778\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 66s 614ms/step - loss: 0.0281 - accuracy: 0.9892 - val_loss: 0.9433 - val_accuracy: 0.7778\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 64s 594ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 1.0975 - val_accuracy: 0.7566\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 65s 608ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 1.4414 - val_accuracy: 0.7619\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 130s 984ms/step - loss: 0.6111 - accuracy: 0.6539 - val_loss: 0.4232 - val_accuracy: 0.8016\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 70s 653ms/step - loss: 0.2644 - accuracy: 0.8985 - val_loss: 0.4524 - val_accuracy: 0.8042\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 58s 545ms/step - loss: 0.1262 - accuracy: 0.9623 - val_loss: 0.6921 - val_accuracy: 0.8148\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 60s 559ms/step - loss: 0.0965 - accuracy: 0.9703 - val_loss: 0.6949 - val_accuracy: 0.7672\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 61s 572ms/step - loss: 0.0439 - accuracy: 0.9876 - val_loss: 0.8454 - val_accuracy: 0.7831\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 63s 590ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.7862 - val_accuracy: 0.7884\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 1.0347 - val_accuracy: 0.7857\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 1.1984 - val_accuracy: 0.7619\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 134s 997ms/step - loss: 0.6134 - accuracy: 0.6694 - val_loss: 0.4140 - val_accuracy: 0.8011\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 75s 698ms/step - loss: 0.2754 - accuracy: 0.8963 - val_loss: 0.4835 - val_accuracy: 0.7825\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 59s 553ms/step - loss: 0.1388 - accuracy: 0.9506 - val_loss: 0.6949 - val_accuracy: 0.7719\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 59s 552ms/step - loss: 0.0703 - accuracy: 0.9759 - val_loss: 0.8925 - val_accuracy: 0.7533\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.0473 - accuracy: 0.9854 - val_loss: 0.9819 - val_accuracy: 0.7692\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 57s 530ms/step - loss: 0.0213 - accuracy: 0.9955 - val_loss: 1.1917 - val_accuracy: 0.7666\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 139s 1s/step - loss: 0.6051 - accuracy: 0.6926 - val_loss: 0.4703 - val_accuracy: 0.7692\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 85s 793ms/step - loss: 0.2750 - accuracy: 0.8890 - val_loss: 0.5288 - val_accuracy: 0.7639\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 59s 552ms/step - loss: 0.1221 - accuracy: 0.9618 - val_loss: 0.6318 - val_accuracy: 0.7825\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 63s 589ms/step - loss: 0.0624 - accuracy: 0.9789 - val_loss: 0.7267 - val_accuracy: 0.7851\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 63s 587ms/step - loss: 0.0424 - accuracy: 0.9880 - val_loss: 0.7702 - val_accuracy: 0.7798\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 61s 575ms/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 1.2065 - val_accuracy: 0.7719\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 64s 600ms/step - loss: 0.0189 - accuracy: 0.9925 - val_loss: 1.0294 - val_accuracy: 0.7533\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 65s 611ms/step - loss: 0.0167 - accuracy: 0.9927 - val_loss: 1.2215 - val_accuracy: 0.7745\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 66s 615ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 1.3347 - val_accuracy: 0.7745\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.6098 - accuracy: 0.6739 - val_loss: 0.4519 - val_accuracy: 0.7745\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 80s 749ms/step - loss: 0.2559 - accuracy: 0.8902 - val_loss: 0.4349 - val_accuracy: 0.7692\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 54s 505ms/step - loss: 0.1190 - accuracy: 0.9588 - val_loss: 0.5987 - val_accuracy: 0.7745\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.0554 - accuracy: 0.9811 - val_loss: 0.7114 - val_accuracy: 0.7745\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.0458 - accuracy: 0.9874 - val_loss: 0.6752 - val_accuracy: 0.7692\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 63s 593ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.9830 - val_accuracy: 0.7745\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 77.45358347892761\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 118s 861ms/step - loss: 0.6090 - accuracy: 0.6554 - val_loss: 0.4313 - val_accuracy: 0.7772\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 79s 735ms/step - loss: 0.2557 - accuracy: 0.9079 - val_loss: 0.4416 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 68s 640ms/step - loss: 0.1255 - accuracy: 0.9607 - val_loss: 0.5349 - val_accuracy: 0.7719\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 70s 657ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 0.8698 - val_accuracy: 0.7507\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 70s 658ms/step - loss: 0.0527 - accuracy: 0.9822 - val_loss: 0.9495 - val_accuracy: 0.7639\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 70s 658ms/step - loss: 0.0312 - accuracy: 0.9886 - val_loss: 0.8520 - val_accuracy: 0.7719\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 70s 655ms/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 1.0528 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 131s 982ms/step - loss: 0.6077 - accuracy: 0.6515 - val_loss: 0.4709 - val_accuracy: 0.7825\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.2616 - accuracy: 0.8926 - val_loss: 0.4821 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 60s 559ms/step - loss: 0.1282 - accuracy: 0.9570 - val_loss: 0.6109 - val_accuracy: 0.7719\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 59s 556ms/step - loss: 0.0606 - accuracy: 0.9800 - val_loss: 0.7832 - val_accuracy: 0.7931\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.0345 - accuracy: 0.9907 - val_loss: 0.9332 - val_accuracy: 0.7639\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 60s 560ms/step - loss: 0.0138 - accuracy: 0.9967 - val_loss: 1.1276 - val_accuracy: 0.7745\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 60s 558ms/step - loss: 0.0234 - accuracy: 0.9912 - val_loss: 1.1211 - val_accuracy: 0.7772\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 60s 558ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 1.1737 - val_accuracy: 0.7586\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 58s 543ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 1.2749 - val_accuracy: 0.7639\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  79.365081  79.62963  82.539684  80.158728  81.481481  80.106103  78.514588   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  77.453583  78.514588  79.310346  79.707381  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.365081</td>\n",
       "      <td>79.62963</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>77.453583</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.707381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  79.365081  79.62963  82.539684  80.158728  81.481481  80.106103  78.514588   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  77.453583  78.514588  79.310346  79.707381  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_CR_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5046 words present from 5336 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 285,825\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 45s 282ms/step - loss: 0.5923 - accuracy: 0.6707 - val_loss: 0.4642 - val_accuracy: 0.7566\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 0.4101 - accuracy: 0.8113 - val_loss: 0.4512 - val_accuracy: 0.7831\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 0.3829 - accuracy: 0.8204 - val_loss: 0.4361 - val_accuracy: 0.8122\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 20s 192ms/step - loss: 0.3351 - accuracy: 0.8545 - val_loss: 0.4934 - val_accuracy: 0.8042\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 21s 196ms/step - loss: 0.2966 - accuracy: 0.8638 - val_loss: 0.4507 - val_accuracy: 0.8042\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 20s 189ms/step - loss: 0.2394 - accuracy: 0.8961 - val_loss: 0.4945 - val_accuracy: 0.7698\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 20s 185ms/step - loss: 0.2070 - accuracy: 0.9090 - val_loss: 0.5252 - val_accuracy: 0.7937\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 19s 179ms/step - loss: 0.1781 - accuracy: 0.9207 - val_loss: 0.5784 - val_accuracy: 0.7725\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 19s 182ms/step - loss: 0.1568 - accuracy: 0.9319 - val_loss: 0.6859 - val_accuracy: 0.7302\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 20s 187ms/step - loss: 0.1625 - accuracy: 0.9312 - val_loss: 0.7354 - val_accuracy: 0.7513\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 20s 186ms/step - loss: 0.1006 - accuracy: 0.9605 - val_loss: 0.7278 - val_accuracy: 0.7646\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 65s 449ms/step - loss: 0.6065 - accuracy: 0.6776 - val_loss: 0.4704 - val_accuracy: 0.7698\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 29s 272ms/step - loss: 0.4516 - accuracy: 0.7916 - val_loss: 0.4229 - val_accuracy: 0.7857\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 29s 275ms/step - loss: 0.3860 - accuracy: 0.8211 - val_loss: 0.4184 - val_accuracy: 0.7989\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 30s 280ms/step - loss: 0.3920 - accuracy: 0.8213 - val_loss: 0.4220 - val_accuracy: 0.8069\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 30s 285ms/step - loss: 0.3061 - accuracy: 0.8755 - val_loss: 0.4050 - val_accuracy: 0.8069\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 30s 285ms/step - loss: 0.2516 - accuracy: 0.9003 - val_loss: 0.4367 - val_accuracy: 0.7778\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 31s 288ms/step - loss: 0.2329 - accuracy: 0.9053 - val_loss: 0.4271 - val_accuracy: 0.7989\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 31s 292ms/step - loss: 0.1755 - accuracy: 0.9295 - val_loss: 0.5536 - val_accuracy: 0.7884\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 32s 298ms/step - loss: 0.1532 - accuracy: 0.9396 - val_loss: 0.5888 - val_accuracy: 0.8016\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 32s 303ms/step - loss: 0.1233 - accuracy: 0.9490 - val_loss: 0.5489 - val_accuracy: 0.7910\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 32s 299ms/step - loss: 0.1296 - accuracy: 0.9428 - val_loss: 0.6870 - val_accuracy: 0.7937\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 33s 305ms/step - loss: 0.0707 - accuracy: 0.9697 - val_loss: 0.7017 - val_accuracy: 0.7857\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 47s 294ms/step - loss: 0.6011 - accuracy: 0.6664 - val_loss: 0.4618 - val_accuracy: 0.7646\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 21s 193ms/step - loss: 0.4441 - accuracy: 0.7935 - val_loss: 0.4241 - val_accuracy: 0.7910\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 20s 187ms/step - loss: 0.3939 - accuracy: 0.8194 - val_loss: 0.4240 - val_accuracy: 0.7910\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 20s 187ms/step - loss: 0.3486 - accuracy: 0.8401 - val_loss: 0.4367 - val_accuracy: 0.8016\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 20s 187ms/step - loss: 0.3073 - accuracy: 0.8639 - val_loss: 0.4461 - val_accuracy: 0.7857\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 20s 188ms/step - loss: 0.2790 - accuracy: 0.8739 - val_loss: 0.4425 - val_accuracy: 0.7672\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 20s 184ms/step - loss: 0.2442 - accuracy: 0.8952 - val_loss: 0.5954 - val_accuracy: 0.7751\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 19s 179ms/step - loss: 0.2235 - accuracy: 0.9078 - val_loss: 0.6071 - val_accuracy: 0.7804\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 19s 180ms/step - loss: 0.1659 - accuracy: 0.9318 - val_loss: 0.6754 - val_accuracy: 0.7831\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 19s 180ms/step - loss: 0.1518 - accuracy: 0.9395 - val_loss: 0.6707 - val_accuracy: 0.7619\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 19s 181ms/step - loss: 0.1082 - accuracy: 0.9564 - val_loss: 0.8315 - val_accuracy: 0.7937\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 19s 178ms/step - loss: 0.0696 - accuracy: 0.9770 - val_loss: 0.8504 - val_accuracy: 0.7593\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 92s 632ms/step - loss: 0.5978 - accuracy: 0.6719 - val_loss: 0.5394 - val_accuracy: 0.7196\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 50s 465ms/step - loss: 0.4534 - accuracy: 0.7868 - val_loss: 0.4364 - val_accuracy: 0.8122\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 36s 335ms/step - loss: 0.3790 - accuracy: 0.8335 - val_loss: 0.4218 - val_accuracy: 0.8122\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 34s 318ms/step - loss: 0.3535 - accuracy: 0.8512 - val_loss: 0.4438 - val_accuracy: 0.7698\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 36s 338ms/step - loss: 0.3063 - accuracy: 0.8734 - val_loss: 0.4421 - val_accuracy: 0.7857\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 37s 348ms/step - loss: 0.2674 - accuracy: 0.8878 - val_loss: 0.4248 - val_accuracy: 0.8228\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 35s 331ms/step - loss: 0.2450 - accuracy: 0.8948 - val_loss: 0.5388 - val_accuracy: 0.7831\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 33s 306ms/step - loss: 0.1929 - accuracy: 0.9173 - val_loss: 0.4942 - val_accuracy: 0.7963\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 33s 307ms/step - loss: 0.1695 - accuracy: 0.9282 - val_loss: 0.5460 - val_accuracy: 0.8095\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 36s 336ms/step - loss: 0.1382 - accuracy: 0.9456 - val_loss: 0.5707 - val_accuracy: 0.7989\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 37s 344ms/step - loss: 0.0882 - accuracy: 0.9671 - val_loss: 0.7871 - val_accuracy: 0.7910\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 37s 346ms/step - loss: 0.0653 - accuracy: 0.9767 - val_loss: 0.7513 - val_accuracy: 0.8016\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 34s 314ms/step - loss: 0.0539 - accuracy: 0.9807 - val_loss: 0.8293 - val_accuracy: 0.7831\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 34s 313ms/step - loss: 0.0588 - accuracy: 0.9797 - val_loss: 0.8216 - val_accuracy: 0.8016\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 82.27513432502747\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 56s 362ms/step - loss: 0.6058 - accuracy: 0.6534 - val_loss: 0.4625 - val_accuracy: 0.7566\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 29s 267ms/step - loss: 0.4391 - accuracy: 0.7987 - val_loss: 0.4661 - val_accuracy: 0.7725\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 28s 261ms/step - loss: 0.3833 - accuracy: 0.8287 - val_loss: 0.4758 - val_accuracy: 0.7884\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 27s 253ms/step - loss: 0.3663 - accuracy: 0.8440 - val_loss: 0.4793 - val_accuracy: 0.7857\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 28s 258ms/step - loss: 0.3378 - accuracy: 0.8516 - val_loss: 0.5135 - val_accuracy: 0.7937\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 26s 246ms/step - loss: 0.2798 - accuracy: 0.8748 - val_loss: 0.4869 - val_accuracy: 0.7937\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 26s 244ms/step - loss: 0.2670 - accuracy: 0.8850 - val_loss: 0.5009 - val_accuracy: 0.7725\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 26s 247ms/step - loss: 0.2216 - accuracy: 0.9108 - val_loss: 0.5930 - val_accuracy: 0.7725\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 26s 242ms/step - loss: 0.1636 - accuracy: 0.9306 - val_loss: 0.6664 - val_accuracy: 0.7884\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.1161 - accuracy: 0.9533 - val_loss: 0.6302 - val_accuracy: 0.7937\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 26s 239ms/step - loss: 0.0994 - accuracy: 0.9636 - val_loss: 0.7244 - val_accuracy: 0.7804\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.1140 - accuracy: 0.9565 - val_loss: 0.8735 - val_accuracy: 0.7566\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 26s 241ms/step - loss: 0.0982 - accuracy: 0.9667 - val_loss: 0.8882 - val_accuracy: 0.8042\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 26s 241ms/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.8912 - val_accuracy: 0.8016\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.0299 - accuracy: 0.9881 - val_loss: 1.0884 - val_accuracy: 0.7831\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.0406 - accuracy: 0.9829 - val_loss: 1.1190 - val_accuracy: 0.7460\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 26s 247ms/step - loss: 0.0525 - accuracy: 0.9829 - val_loss: 1.0148 - val_accuracy: 0.7778\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 28s 264ms/step - loss: 0.0135 - accuracy: 0.9972 - val_loss: 1.0666 - val_accuracy: 0.7672\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 30s 276ms/step - loss: 0.0117 - accuracy: 0.9976 - val_loss: 1.3575 - val_accuracy: 0.7804\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 29s 273ms/step - loss: 0.0115 - accuracy: 0.9959 - val_loss: 1.2887 - val_accuracy: 0.7937\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 30s 278ms/step - loss: 0.1051 - accuracy: 0.9655 - val_loss: 0.9281 - val_accuracy: 0.7831\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 80.42327761650085\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 55s 343ms/step - loss: 0.5912 - accuracy: 0.6842 - val_loss: 0.4496 - val_accuracy: 0.7851\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 63s 589ms/step - loss: 0.4141 - accuracy: 0.8190 - val_loss: 0.4172 - val_accuracy: 0.8037\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 62s 581ms/step - loss: 0.3758 - accuracy: 0.8340 - val_loss: 0.3986 - val_accuracy: 0.8064\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 61s 571ms/step - loss: 0.3370 - accuracy: 0.8520 - val_loss: 0.3835 - val_accuracy: 0.8090\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 60s 564ms/step - loss: 0.2992 - accuracy: 0.8824 - val_loss: 0.4349 - val_accuracy: 0.8117\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 60s 561ms/step - loss: 0.2596 - accuracy: 0.8889 - val_loss: 0.4158 - val_accuracy: 0.7825\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 61s 566ms/step - loss: 0.2164 - accuracy: 0.9186 - val_loss: 0.4842 - val_accuracy: 0.7878\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 60s 563ms/step - loss: 0.2079 - accuracy: 0.9197 - val_loss: 0.4556 - val_accuracy: 0.7905\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 60s 561ms/step - loss: 0.1558 - accuracy: 0.9389 - val_loss: 0.4406 - val_accuracy: 0.8037\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 61s 568ms/step - loss: 0.1411 - accuracy: 0.9470 - val_loss: 0.6218 - val_accuracy: 0.8011\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 61s 570ms/step - loss: 0.1072 - accuracy: 0.9583 - val_loss: 0.5600 - val_accuracy: 0.8090\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 61s 568ms/step - loss: 0.0661 - accuracy: 0.9784 - val_loss: 0.7283 - val_accuracy: 0.7825\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 61s 566ms/step - loss: 0.0636 - accuracy: 0.9736 - val_loss: 0.7312 - val_accuracy: 0.8064\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 81.16710782051086\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 112s 861ms/step - loss: 0.6083 - accuracy: 0.6687 - val_loss: 0.5014 - val_accuracy: 0.7454\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 64s 600ms/step - loss: 0.4394 - accuracy: 0.7984 - val_loss: 0.4429 - val_accuracy: 0.7878\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 51s 479ms/step - loss: 0.3763 - accuracy: 0.8323 - val_loss: 0.4418 - val_accuracy: 0.7798\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 52s 482ms/step - loss: 0.3242 - accuracy: 0.8593 - val_loss: 0.4142 - val_accuracy: 0.8170\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 52s 485ms/step - loss: 0.3003 - accuracy: 0.8618 - val_loss: 0.3964 - val_accuracy: 0.8223\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 51s 480ms/step - loss: 0.2773 - accuracy: 0.8788 - val_loss: 0.3925 - val_accuracy: 0.8276\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 51s 475ms/step - loss: 0.2325 - accuracy: 0.9040 - val_loss: 0.4316 - val_accuracy: 0.8170\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 51s 477ms/step - loss: 0.2142 - accuracy: 0.9030 - val_loss: 0.4841 - val_accuracy: 0.8382\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 51s 481ms/step - loss: 0.1871 - accuracy: 0.9282 - val_loss: 0.5338 - val_accuracy: 0.8117\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 51s 478ms/step - loss: 0.1268 - accuracy: 0.9529 - val_loss: 0.6023 - val_accuracy: 0.7692\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 51s 478ms/step - loss: 0.0930 - accuracy: 0.9657 - val_loss: 0.6095 - val_accuracy: 0.8170\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 52s 488ms/step - loss: 0.0807 - accuracy: 0.9759 - val_loss: 0.8169 - val_accuracy: 0.7586\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 52s 487ms/step - loss: 0.0963 - accuracy: 0.9583 - val_loss: 0.6771 - val_accuracy: 0.8382\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 52s 489ms/step - loss: 0.0626 - accuracy: 0.9783 - val_loss: 0.7843 - val_accuracy: 0.7745\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 52s 484ms/step - loss: 0.0492 - accuracy: 0.9838 - val_loss: 0.7840 - val_accuracy: 0.8090\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 52s 481ms/step - loss: 0.0314 - accuracy: 0.9884 - val_loss: 0.8169 - val_accuracy: 0.8117\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 83.81962776184082\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 82s 578ms/step - loss: 0.5805 - accuracy: 0.6865 - val_loss: 0.4732 - val_accuracy: 0.7798\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 51s 478ms/step - loss: 0.4298 - accuracy: 0.8000 - val_loss: 0.4460 - val_accuracy: 0.7798\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 53s 497ms/step - loss: 0.3852 - accuracy: 0.8310 - val_loss: 0.4289 - val_accuracy: 0.7984\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 53s 498ms/step - loss: 0.3326 - accuracy: 0.8460 - val_loss: 0.4331 - val_accuracy: 0.8011\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 53s 499ms/step - loss: 0.2904 - accuracy: 0.8769 - val_loss: 0.4496 - val_accuracy: 0.7851\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 54s 507ms/step - loss: 0.2675 - accuracy: 0.8843 - val_loss: 0.4723 - val_accuracy: 0.7958\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 54s 504ms/step - loss: 0.2083 - accuracy: 0.9138 - val_loss: 0.6313 - val_accuracy: 0.7560\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 54s 503ms/step - loss: 0.1870 - accuracy: 0.9234 - val_loss: 0.6165 - val_accuracy: 0.7851\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 54s 500ms/step - loss: 0.1342 - accuracy: 0.9433 - val_loss: 0.7230 - val_accuracy: 0.7851\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 54s 503ms/step - loss: 0.1098 - accuracy: 0.9524 - val_loss: 0.6153 - val_accuracy: 0.7772\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 54s 503ms/step - loss: 0.0918 - accuracy: 0.9618 - val_loss: 0.7187 - val_accuracy: 0.7851\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 54s 506ms/step - loss: 0.0730 - accuracy: 0.9743 - val_loss: 0.8214 - val_accuracy: 0.7825\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 111s 856ms/step - loss: 0.5972 - accuracy: 0.6620 - val_loss: 0.4667 - val_accuracy: 0.7798\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 95s 891ms/step - loss: 0.4230 - accuracy: 0.8031 - val_loss: 0.4453 - val_accuracy: 0.7719\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 88s 825ms/step - loss: 0.3731 - accuracy: 0.8397 - val_loss: 0.4615 - val_accuracy: 0.7825\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 86s 808ms/step - loss: 0.3282 - accuracy: 0.8620 - val_loss: 0.4563 - val_accuracy: 0.7772\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 87s 810ms/step - loss: 0.3305 - accuracy: 0.8577 - val_loss: 0.4799 - val_accuracy: 0.7772\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 87s 813ms/step - loss: 0.2701 - accuracy: 0.8892 - val_loss: 0.4490 - val_accuracy: 0.7931\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 86s 804ms/step - loss: 0.2212 - accuracy: 0.9034 - val_loss: 0.5744 - val_accuracy: 0.7905\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 86s 803ms/step - loss: 0.1787 - accuracy: 0.9253 - val_loss: 0.5778 - val_accuracy: 0.7798\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 86s 799ms/step - loss: 0.1509 - accuracy: 0.9399 - val_loss: 0.5754 - val_accuracy: 0.7825\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 87s 812ms/step - loss: 0.1240 - accuracy: 0.9521 - val_loss: 0.7689 - val_accuracy: 0.7613\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 87s 814ms/step - loss: 0.1337 - accuracy: 0.9455 - val_loss: 0.6855 - val_accuracy: 0.7931\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 86s 803ms/step - loss: 0.0578 - accuracy: 0.9804 - val_loss: 0.7451 - val_accuracy: 0.7905\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 85s 794ms/step - loss: 0.0784 - accuracy: 0.9728 - val_loss: 1.0218 - val_accuracy: 0.7825\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 86s 800ms/step - loss: 0.0684 - accuracy: 0.9732 - val_loss: 0.8362 - val_accuracy: 0.7666\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 111s 848ms/step - loss: 0.5957 - accuracy: 0.6703 - val_loss: 0.4727 - val_accuracy: 0.7639\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 76s 707ms/step - loss: 0.4273 - accuracy: 0.8056 - val_loss: 0.4419 - val_accuracy: 0.7798\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 80s 745ms/step - loss: 0.4008 - accuracy: 0.8136 - val_loss: 0.4356 - val_accuracy: 0.7878\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 82s 768ms/step - loss: 0.3641 - accuracy: 0.8340 - val_loss: 0.4393 - val_accuracy: 0.7692\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 84s 783ms/step - loss: 0.3153 - accuracy: 0.8543 - val_loss: 0.4979 - val_accuracy: 0.7745\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.2669 - accuracy: 0.8843 - val_loss: 0.4370 - val_accuracy: 0.7825\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 86s 807ms/step - loss: 0.2358 - accuracy: 0.8944 - val_loss: 0.4997 - val_accuracy: 0.7825\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 87s 812ms/step - loss: 0.2035 - accuracy: 0.9095 - val_loss: 0.5042 - val_accuracy: 0.7878\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 87s 811ms/step - loss: 0.1525 - accuracy: 0.9380 - val_loss: 0.6669 - val_accuracy: 0.7798\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 87s 815ms/step - loss: 0.1212 - accuracy: 0.9543 - val_loss: 0.6489 - val_accuracy: 0.7958\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 87s 814ms/step - loss: 0.0901 - accuracy: 0.9676 - val_loss: 0.7696 - val_accuracy: 0.7984\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 87s 817ms/step - loss: 0.0845 - accuracy: 0.9686 - val_loss: 0.7450 - val_accuracy: 0.8011\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 87s 817ms/step - loss: 0.0554 - accuracy: 0.9809 - val_loss: 0.8492 - val_accuracy: 0.7851\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 87s 817ms/step - loss: 0.0587 - accuracy: 0.9822 - val_loss: 0.9208 - val_accuracy: 0.7825\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 89s 828ms/step - loss: 0.0488 - accuracy: 0.9862 - val_loss: 0.8869 - val_accuracy: 0.7666\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 89s 828ms/step - loss: 0.0470 - accuracy: 0.9872 - val_loss: 0.9833 - val_accuracy: 0.7878\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 89s 831ms/step - loss: 0.0201 - accuracy: 0.9952 - val_loss: 1.1951 - val_accuracy: 0.7878\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 89s 834ms/step - loss: 0.0309 - accuracy: 0.9865 - val_loss: 1.0375 - val_accuracy: 0.7905\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 91s 853ms/step - loss: 0.0130 - accuracy: 0.9970 - val_loss: 1.2183 - val_accuracy: 0.7878\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 88s 826ms/step - loss: 0.0130 - accuracy: 0.9942 - val_loss: 1.2412 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  81.216931  80.687833  80.158728  82.275134  80.423278  81.167108   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  83.819628  80.106103  79.310346  80.106103  80.927119  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.216931</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>82.275134</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>83.819628</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>80.927119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  81.216931  80.687833  80.158728  82.275134  80.423278  81.167108   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  83.819628  80.106103  79.310346  80.106103  80.927119  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_CR_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 138s 1s/step - loss: 0.5824 - accuracy: 0.6799 - val_loss: 0.4133 - val_accuracy: 0.8280\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 89s 835ms/step - loss: 0.2794 - accuracy: 0.8921 - val_loss: 0.4156 - val_accuracy: 0.7963\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 66s 616ms/step - loss: 0.1668 - accuracy: 0.9479 - val_loss: 0.5249 - val_accuracy: 0.7937\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 65s 609ms/step - loss: 0.0980 - accuracy: 0.9672 - val_loss: 0.6735 - val_accuracy: 0.7884\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 68s 639ms/step - loss: 0.0502 - accuracy: 0.9813 - val_loss: 0.7325 - val_accuracy: 0.7910\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 69s 644ms/step - loss: 0.0336 - accuracy: 0.9896 - val_loss: 0.7625 - val_accuracy: 0.7989\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 70s 654ms/step - loss: 0.0219 - accuracy: 0.9930 - val_loss: 1.0778 - val_accuracy: 0.7804\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 70s 650ms/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 1.0587 - val_accuracy: 0.7751\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 71s 661ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 1.2377 - val_accuracy: 0.7646\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 82.80423283576965\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 103s 779ms/step - loss: 0.5846 - accuracy: 0.6984 - val_loss: 0.4211 - val_accuracy: 0.8042\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 72s 676ms/step - loss: 0.2977 - accuracy: 0.8836 - val_loss: 0.4206 - val_accuracy: 0.8201\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 66s 616ms/step - loss: 0.1693 - accuracy: 0.9438 - val_loss: 0.5240 - val_accuracy: 0.7884\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 66s 615ms/step - loss: 0.0744 - accuracy: 0.9720 - val_loss: 0.5941 - val_accuracy: 0.7989\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 65s 612ms/step - loss: 0.0555 - accuracy: 0.9810 - val_loss: 0.8790 - val_accuracy: 0.7910\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 66s 613ms/step - loss: 0.0286 - accuracy: 0.9913 - val_loss: 1.0201 - val_accuracy: 0.7804\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 66s 617ms/step - loss: 0.0189 - accuracy: 0.9946 - val_loss: 1.0652 - val_accuracy: 0.7698\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 66s 618ms/step - loss: 0.0274 - accuracy: 0.9897 - val_loss: 0.9685 - val_accuracy: 0.7804\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 66s 619ms/step - loss: 0.0145 - accuracy: 0.9968 - val_loss: 1.3024 - val_accuracy: 0.7831\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 66s 621ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 1.0093 - val_accuracy: 0.7857\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 82.0105791091919\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 131s 1s/step - loss: 0.5853 - accuracy: 0.6702 - val_loss: 0.4626 - val_accuracy: 0.7487\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 88s 820ms/step - loss: 0.3196 - accuracy: 0.8536 - val_loss: 0.3843 - val_accuracy: 0.8360\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 62s 577ms/step - loss: 0.1524 - accuracy: 0.9436 - val_loss: 0.4433 - val_accuracy: 0.8069\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 64s 596ms/step - loss: 0.0898 - accuracy: 0.9730 - val_loss: 0.5600 - val_accuracy: 0.8095\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 62s 578ms/step - loss: 0.0648 - accuracy: 0.9822 - val_loss: 0.7397 - val_accuracy: 0.8069\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 65s 604ms/step - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.7673 - val_accuracy: 0.7989\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 64s 595ms/step - loss: 0.0504 - accuracy: 0.9828 - val_loss: 0.8845 - val_accuracy: 0.7857\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 64s 599ms/step - loss: 0.0316 - accuracy: 0.9878 - val_loss: 0.7717 - val_accuracy: 0.8122\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 64s 595ms/step - loss: 0.0304 - accuracy: 0.9883 - val_loss: 0.9713 - val_accuracy: 0.7831\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 63s 588ms/step - loss: 0.0139 - accuracy: 0.9963 - val_loss: 0.9772 - val_accuracy: 0.8122\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 83.59788656234741\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 106s 800ms/step - loss: 0.5894 - accuracy: 0.6679 - val_loss: 0.3808 - val_accuracy: 0.8280\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 70s 652ms/step - loss: 0.2957 - accuracy: 0.8801 - val_loss: 0.4291 - val_accuracy: 0.8280\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 69s 643ms/step - loss: 0.1467 - accuracy: 0.9458 - val_loss: 0.4083 - val_accuracy: 0.8042\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 70s 650ms/step - loss: 0.0877 - accuracy: 0.9738 - val_loss: 0.5554 - val_accuracy: 0.7989\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 70s 652ms/step - loss: 0.0532 - accuracy: 0.9853 - val_loss: 0.8452 - val_accuracy: 0.8069\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 69s 650ms/step - loss: 0.0330 - accuracy: 0.9921 - val_loss: 0.6539 - val_accuracy: 0.7619\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 70s 653ms/step - loss: 0.0379 - accuracy: 0.9870 - val_loss: 0.8040 - val_accuracy: 0.7910\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 70s 654ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 1.0719 - val_accuracy: 0.7857\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 71s 660ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 1.2671 - val_accuracy: 0.7989\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 82.80423283576965\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 102s 758ms/step - loss: 0.5960 - accuracy: 0.6711 - val_loss: 0.4394 - val_accuracy: 0.7831\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 73s 680ms/step - loss: 0.3128 - accuracy: 0.8661 - val_loss: 0.4396 - val_accuracy: 0.8122\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 70s 650ms/step - loss: 0.1595 - accuracy: 0.9452 - val_loss: 0.4579 - val_accuracy: 0.8016\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 70s 654ms/step - loss: 0.1049 - accuracy: 0.9677 - val_loss: 0.5183 - val_accuracy: 0.8069\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 69s 648ms/step - loss: 0.0500 - accuracy: 0.9848 - val_loss: 0.7337 - val_accuracy: 0.8095\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 69s 649ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.8042 - val_accuracy: 0.8042\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 70s 656ms/step - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.9078 - val_accuracy: 0.7989\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 71s 661ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.9924 - val_accuracy: 0.8016\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 69s 647ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 1.0441 - val_accuracy: 0.7989\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 75s 707ms/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 1.2949 - val_accuracy: 0.8016\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 187s 2s/step - loss: 0.5734 - accuracy: 0.6840 - val_loss: 0.4100 - val_accuracy: 0.8170\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 158s 1s/step - loss: 0.3019 - accuracy: 0.8826 - val_loss: 0.4555 - val_accuracy: 0.8143\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 130s 1s/step - loss: 0.1370 - accuracy: 0.9511 - val_loss: 0.6353 - val_accuracy: 0.7931\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 129s 1s/step - loss: 0.0777 - accuracy: 0.9724 - val_loss: 0.5427 - val_accuracy: 0.8037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "107/107 [==============================] - 127s 1s/step - loss: 0.0565 - accuracy: 0.9821 - val_loss: 0.7874 - val_accuracy: 0.7984\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 128s 1s/step - loss: 0.0407 - accuracy: 0.9890 - val_loss: 1.2362 - val_accuracy: 0.7268\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 127s 1s/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.8524 - val_accuracy: 0.7958\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 128s 1s/step - loss: 0.0242 - accuracy: 0.9909 - val_loss: 1.0229 - val_accuracy: 0.7878\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 130s 1s/step - loss: 0.0139 - accuracy: 0.9962 - val_loss: 1.3621 - val_accuracy: 0.7984\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 81.69761300086975\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 167s 1s/step - loss: 0.5875 - accuracy: 0.6781 - val_loss: 0.4152 - val_accuracy: 0.8064\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 116s 1s/step - loss: 0.2928 - accuracy: 0.8870 - val_loss: 0.4196 - val_accuracy: 0.7798\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 103s 965ms/step - loss: 0.1606 - accuracy: 0.9453 - val_loss: 0.5633 - val_accuracy: 0.7905\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 104s 969ms/step - loss: 0.0824 - accuracy: 0.9742 - val_loss: 0.6488 - val_accuracy: 0.7745\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 103s 960ms/step - loss: 0.0547 - accuracy: 0.9837 - val_loss: 0.7961 - val_accuracy: 0.7719\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 103s 968ms/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 1.0063 - val_accuracy: 0.7613\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 104s 970ms/step - loss: 0.0243 - accuracy: 0.9898 - val_loss: 0.9574 - val_accuracy: 0.7507\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 103s 966ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 1.1473 - val_accuracy: 0.7772\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 103s 967ms/step - loss: 0.0089 - accuracy: 0.9966 - val_loss: 1.6839 - val_accuracy: 0.7427\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 136s 1s/step - loss: 0.5972 - accuracy: 0.6745 - val_loss: 0.4249 - val_accuracy: 0.7825\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 108s 1s/step - loss: 0.2958 - accuracy: 0.8818 - val_loss: 0.4131 - val_accuracy: 0.7905\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 83s 776ms/step - loss: 0.1795 - accuracy: 0.9346 - val_loss: 0.5454 - val_accuracy: 0.7798\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 83s 773ms/step - loss: 0.0952 - accuracy: 0.9639 - val_loss: 0.6181 - val_accuracy: 0.7958\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 83s 774ms/step - loss: 0.0590 - accuracy: 0.9804 - val_loss: 0.8835 - val_accuracy: 0.7825\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 83s 773ms/step - loss: 0.0285 - accuracy: 0.9902 - val_loss: 0.7777 - val_accuracy: 0.7825\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 82s 768ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.9004 - val_accuracy: 0.7825\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 82s 771ms/step - loss: 0.0152 - accuracy: 0.9946 - val_loss: 1.2961 - val_accuracy: 0.7666\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 83s 771ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 1.0047 - val_accuracy: 0.7905\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 82s 770ms/step - loss: 0.0179 - accuracy: 0.9958 - val_loss: 1.0882 - val_accuracy: 0.7798\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 82s 771ms/step - loss: 0.0100 - accuracy: 0.9965 - val_loss: 1.4460 - val_accuracy: 0.7798\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 82s 767ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 1.3791 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 86s 618ms/step - loss: 0.5925 - accuracy: 0.6747 - val_loss: 0.4499 - val_accuracy: 0.7772\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 52s 483ms/step - loss: 0.2834 - accuracy: 0.8819 - val_loss: 0.4217 - val_accuracy: 0.8011\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 48s 449ms/step - loss: 0.1655 - accuracy: 0.9397 - val_loss: 0.4644 - val_accuracy: 0.7958\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 48s 446ms/step - loss: 0.0991 - accuracy: 0.9660 - val_loss: 0.6802 - val_accuracy: 0.7958\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 48s 445ms/step - loss: 0.0682 - accuracy: 0.9764 - val_loss: 0.8157 - val_accuracy: 0.7745\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 48s 446ms/step - loss: 0.0262 - accuracy: 0.9914 - val_loss: 0.9680 - val_accuracy: 0.7825\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 48s 449ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.9432 - val_accuracy: 0.7745\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 48s 446ms/step - loss: 0.0135 - accuracy: 0.9960 - val_loss: 1.3432 - val_accuracy: 0.7851\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 48s 449ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 1.1932 - val_accuracy: 0.7772\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 48s 450ms/step - loss: 0.0164 - accuracy: 0.9959 - val_loss: 1.4320 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 97s 711ms/step - loss: 0.5898 - accuracy: 0.6724 - val_loss: 0.4065 - val_accuracy: 0.8223\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 137s 1s/step - loss: 0.2899 - accuracy: 0.8844 - val_loss: 0.4266 - val_accuracy: 0.7878\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 138s 1s/step - loss: 0.1545 - accuracy: 0.9494 - val_loss: 0.4482 - val_accuracy: 0.8064\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 139s 1s/step - loss: 0.0902 - accuracy: 0.9760 - val_loss: 0.5518 - val_accuracy: 0.7825\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 138s 1s/step - loss: 0.0425 - accuracy: 0.9865 - val_loss: 0.7403 - val_accuracy: 0.7878\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 139s 1s/step - loss: 0.0218 - accuracy: 0.9944 - val_loss: 1.1345 - val_accuracy: 0.7560\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 138s 1s/step - loss: 0.0246 - accuracy: 0.9914 - val_loss: 1.1454 - val_accuracy: 0.7984\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 138s 1s/step - loss: 0.0179 - accuracy: 0.9927 - val_loss: 1.0915 - val_accuracy: 0.7851\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 139s 1s/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 1.0813 - val_accuracy: 0.7745\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 82.22811818122864\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7924bfeccf6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mrecord3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'record' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.804233</td>\n",
       "      <td>82.010579</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>82.804233</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>82.228118</td>\n",
       "      <td>81.66779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  82.804233  82.010579  83.597887  82.804233  81.216931  81.697613   \n",
       "\n",
       "        acc7       acc8       acc9      acc10       AVG  \n",
       "0  80.636603  79.575598  80.106103  82.228118  81.66779  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_CR_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
