{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with CR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the Customer Reviews Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3775, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weaknesses are minor the feel and layout of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many of our disney movies do n 't play on this...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player has a problem with dual layer dvd 's su...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know the saying is you get what you pay for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will never purchase apex again .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>so far , the anti spam feature seems to be ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>i did not have any of the installation problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>their products have been great and have saved ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     weaknesses are minor the feel and layout of th...      0  train\n",
       "1     many of our disney movies do n 't play on this...      0  train\n",
       "2     player has a problem with dual layer dvd 's su...      0  train\n",
       "3     i know the saying is you get what you pay for ...      0  train\n",
       "4                      will never purchase apex again .      0  train\n",
       "...                                                 ...    ...    ...\n",
       "3770  so far , the anti spam feature seems to be ver...      1  train\n",
       "3771  i downloaded a trial version of computer assoc...      1  train\n",
       "3772  i did not have any of the installation problem...      1  train\n",
       "3773  their products have been great and have saved ...      1  train\n",
       "3774                                                         1  train\n",
       "\n",
       "[3775 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/CR/CR.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3775 entries, 0 to 3774\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3775 non-null   object\n",
      " 1   label     3775 non-null   int32 \n",
      " 2   split     3775 non-null   object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 73.9+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2407</td>\n",
       "      <td>2407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          1368   1368\n",
       "1          2407   2407"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weaknesses are minor the feel and layout of the remote control are only so so . it does n 't show the complete file names of mp3s with really long names . you must cycle through every zoom setting ( 2x , 3x , 4x , 1 2x , etc . ) before getting back to normal size sorry if i 'm just ignorant of a way to get back to 1x quickly .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  will never purchase apex again .\n",
      "Into a sequence of int: [72, 194, 285, 207, 286]\n",
      "Into a padded sequence: [ 72 194 285 207 286   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "and 3\n",
      "i 4\n",
      "it 5\n",
      "to 6\n",
      "a 7\n",
      "is 8\n",
      "of 9\n",
      "this 10\n",
      "5336\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 78s 470ms/step - loss: 0.6128 - accuracy: 0.6679 - val_loss: 0.4231 - val_accuracy: 0.7884\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 44s 413ms/step - loss: 0.2795 - accuracy: 0.8922 - val_loss: 0.4191 - val_accuracy: 0.7910\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 48s 445ms/step - loss: 0.1241 - accuracy: 0.9616 - val_loss: 0.5523 - val_accuracy: 0.7937\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 42s 390ms/step - loss: 0.0839 - accuracy: 0.9714 - val_loss: 0.6551 - val_accuracy: 0.7857\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.0420 - accuracy: 0.9861 - val_loss: 0.8798 - val_accuracy: 0.7857\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 40s 373ms/step - loss: 0.0245 - accuracy: 0.9918 - val_loss: 1.0067 - val_accuracy: 0.7804\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 1.2655 - val_accuracy: 0.7672\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 1.2974 - val_accuracy: 0.7672\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 100s 668ms/step - loss: 0.6113 - accuracy: 0.6602 - val_loss: 0.4669 - val_accuracy: 0.7460\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 44s 414ms/step - loss: 0.2786 - accuracy: 0.8907 - val_loss: 0.5015 - val_accuracy: 0.7963\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 53s 496ms/step - loss: 0.1183 - accuracy: 0.9593 - val_loss: 0.6743 - val_accuracy: 0.7778\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 54s 506ms/step - loss: 0.0607 - accuracy: 0.9797 - val_loss: 0.7511 - val_accuracy: 0.7751\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 55s 514ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 1.0973 - val_accuracy: 0.7698\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 54s 509ms/step - loss: 0.0309 - accuracy: 0.9911 - val_loss: 1.0613 - val_accuracy: 0.7460\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 0.0252 - accuracy: 0.9903 - val_loss: 1.1595 - val_accuracy: 0.7698\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 91s 588ms/step - loss: 0.6021 - accuracy: 0.6604 - val_loss: 0.4229 - val_accuracy: 0.8254\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 39s 368ms/step - loss: 0.2401 - accuracy: 0.9050 - val_loss: 0.4453 - val_accuracy: 0.8095\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 35s 329ms/step - loss: 0.1177 - accuracy: 0.9594 - val_loss: 0.7022 - val_accuracy: 0.7831\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 34s 322ms/step - loss: 0.0612 - accuracy: 0.9790 - val_loss: 0.8245 - val_accuracy: 0.8122\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.0503 - accuracy: 0.9826 - val_loss: 1.0977 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 1.0010 - val_accuracy: 0.7725\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 142s 1s/step - loss: 0.5984 - accuracy: 0.6810 - val_loss: 0.4452 - val_accuracy: 0.7963\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 90s 843ms/step - loss: 0.2601 - accuracy: 0.8946 - val_loss: 0.4446 - val_accuracy: 0.7963\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 67s 624ms/step - loss: 0.1257 - accuracy: 0.9581 - val_loss: 0.5813 - val_accuracy: 0.8016\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 65s 605ms/step - loss: 0.0632 - accuracy: 0.9785 - val_loss: 0.6529 - val_accuracy: 0.7751\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 65s 604ms/step - loss: 0.0439 - accuracy: 0.9869 - val_loss: 0.9480 - val_accuracy: 0.7778\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 66s 614ms/step - loss: 0.0281 - accuracy: 0.9892 - val_loss: 0.9433 - val_accuracy: 0.7778\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 64s 594ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 1.0975 - val_accuracy: 0.7566\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 65s 608ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 1.4414 - val_accuracy: 0.7619\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 130s 984ms/step - loss: 0.6111 - accuracy: 0.6539 - val_loss: 0.4232 - val_accuracy: 0.8016\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 70s 653ms/step - loss: 0.2644 - accuracy: 0.8985 - val_loss: 0.4524 - val_accuracy: 0.8042\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 58s 545ms/step - loss: 0.1262 - accuracy: 0.9623 - val_loss: 0.6921 - val_accuracy: 0.8148\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 60s 559ms/step - loss: 0.0965 - accuracy: 0.9703 - val_loss: 0.6949 - val_accuracy: 0.7672\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 61s 572ms/step - loss: 0.0439 - accuracy: 0.9876 - val_loss: 0.8454 - val_accuracy: 0.7831\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 63s 590ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.7862 - val_accuracy: 0.7884\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 1.0347 - val_accuracy: 0.7857\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 1.1984 - val_accuracy: 0.7619\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 134s 997ms/step - loss: 0.6134 - accuracy: 0.6694 - val_loss: 0.4140 - val_accuracy: 0.8011\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 75s 698ms/step - loss: 0.2754 - accuracy: 0.8963 - val_loss: 0.4835 - val_accuracy: 0.7825\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 59s 553ms/step - loss: 0.1388 - accuracy: 0.9506 - val_loss: 0.6949 - val_accuracy: 0.7719\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 59s 552ms/step - loss: 0.0703 - accuracy: 0.9759 - val_loss: 0.8925 - val_accuracy: 0.7533\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.0473 - accuracy: 0.9854 - val_loss: 0.9819 - val_accuracy: 0.7692\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 57s 530ms/step - loss: 0.0213 - accuracy: 0.9955 - val_loss: 1.1917 - val_accuracy: 0.7666\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 139s 1s/step - loss: 0.6051 - accuracy: 0.6926 - val_loss: 0.4703 - val_accuracy: 0.7692\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 85s 793ms/step - loss: 0.2750 - accuracy: 0.8890 - val_loss: 0.5288 - val_accuracy: 0.7639\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 59s 552ms/step - loss: 0.1221 - accuracy: 0.9618 - val_loss: 0.6318 - val_accuracy: 0.7825\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 63s 589ms/step - loss: 0.0624 - accuracy: 0.9789 - val_loss: 0.7267 - val_accuracy: 0.7851\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 63s 587ms/step - loss: 0.0424 - accuracy: 0.9880 - val_loss: 0.7702 - val_accuracy: 0.7798\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 61s 575ms/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 1.2065 - val_accuracy: 0.7719\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 64s 600ms/step - loss: 0.0189 - accuracy: 0.9925 - val_loss: 1.0294 - val_accuracy: 0.7533\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 65s 611ms/step - loss: 0.0167 - accuracy: 0.9927 - val_loss: 1.2215 - val_accuracy: 0.7745\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 66s 615ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 1.3347 - val_accuracy: 0.7745\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.6098 - accuracy: 0.6739 - val_loss: 0.4519 - val_accuracy: 0.7745\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 80s 749ms/step - loss: 0.2559 - accuracy: 0.8902 - val_loss: 0.4349 - val_accuracy: 0.7692\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 54s 505ms/step - loss: 0.1190 - accuracy: 0.9588 - val_loss: 0.5987 - val_accuracy: 0.7745\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.0554 - accuracy: 0.9811 - val_loss: 0.7114 - val_accuracy: 0.7745\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.0458 - accuracy: 0.9874 - val_loss: 0.6752 - val_accuracy: 0.7692\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 63s 593ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.9830 - val_accuracy: 0.7745\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 77.45358347892761\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 118s 861ms/step - loss: 0.6090 - accuracy: 0.6554 - val_loss: 0.4313 - val_accuracy: 0.7772\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 79s 735ms/step - loss: 0.2557 - accuracy: 0.9079 - val_loss: 0.4416 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 68s 640ms/step - loss: 0.1255 - accuracy: 0.9607 - val_loss: 0.5349 - val_accuracy: 0.7719\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 70s 657ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 0.8698 - val_accuracy: 0.7507\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 70s 658ms/step - loss: 0.0527 - accuracy: 0.9822 - val_loss: 0.9495 - val_accuracy: 0.7639\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 70s 658ms/step - loss: 0.0312 - accuracy: 0.9886 - val_loss: 0.8520 - val_accuracy: 0.7719\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 70s 655ms/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 1.0528 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 131s 982ms/step - loss: 0.6077 - accuracy: 0.6515 - val_loss: 0.4709 - val_accuracy: 0.7825\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 62s 580ms/step - loss: 0.2616 - accuracy: 0.8926 - val_loss: 0.4821 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 60s 559ms/step - loss: 0.1282 - accuracy: 0.9570 - val_loss: 0.6109 - val_accuracy: 0.7719\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 59s 556ms/step - loss: 0.0606 - accuracy: 0.9800 - val_loss: 0.7832 - val_accuracy: 0.7931\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.0345 - accuracy: 0.9907 - val_loss: 0.9332 - val_accuracy: 0.7639\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 60s 560ms/step - loss: 0.0138 - accuracy: 0.9967 - val_loss: 1.1276 - val_accuracy: 0.7745\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 60s 558ms/step - loss: 0.0234 - accuracy: 0.9912 - val_loss: 1.1211 - val_accuracy: 0.7772\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 60s 558ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 1.1737 - val_accuracy: 0.7586\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 58s 543ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 1.2749 - val_accuracy: 0.7639\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  79.365081  79.62963  82.539684  80.158728  81.481481  80.106103  78.514588   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  77.453583  78.514588  79.310346  79.707381  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.365081</td>\n",
       "      <td>79.62963</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>77.453583</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.707381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  79.365081  79.62963  82.539684  80.158728  81.481481  80.106103  78.514588   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  77.453583  78.514588  79.310346  79.707381  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_CR_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5046 words present from 5336 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.78736068,  0.6381698 ,  0.56861984, ..., -0.53932057,\n",
       "        -0.47350975, -0.08038928],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 285,825\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 75s 465ms/step - loss: 0.6287 - accuracy: 0.6368 - val_loss: 0.5047 - val_accuracy: 0.7302\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 34s 314ms/step - loss: 0.4712 - accuracy: 0.7644 - val_loss: 0.4437 - val_accuracy: 0.7751\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 30s 276ms/step - loss: 0.4083 - accuracy: 0.8156 - val_loss: 0.4907 - val_accuracy: 0.7407\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 27s 253ms/step - loss: 0.3529 - accuracy: 0.8481 - val_loss: 0.4232 - val_accuracy: 0.7937\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 28s 260ms/step - loss: 0.3378 - accuracy: 0.8437 - val_loss: 0.4363 - val_accuracy: 0.7646\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 24s 227ms/step - loss: 0.3072 - accuracy: 0.8638 - val_loss: 0.4630 - val_accuracy: 0.7963\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 25s 234ms/step - loss: 0.2218 - accuracy: 0.9086 - val_loss: 0.4841 - val_accuracy: 0.7857\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 25s 234ms/step - loss: 0.1908 - accuracy: 0.9179 - val_loss: 0.5325 - val_accuracy: 0.7884\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 25s 235ms/step - loss: 0.1335 - accuracy: 0.9463 - val_loss: 0.6054 - val_accuracy: 0.7778\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 25s 234ms/step - loss: 0.1577 - accuracy: 0.9392 - val_loss: 0.6658 - val_accuracy: 0.7725\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 25s 234ms/step - loss: 0.0841 - accuracy: 0.9686 - val_loss: 0.7770 - val_accuracy: 0.7698\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 50s 260ms/step - loss: 0.6161 - accuracy: 0.6601 - val_loss: 0.5595 - val_accuracy: 0.7143\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 19s 178ms/step - loss: 0.4543 - accuracy: 0.7949 - val_loss: 0.5595 - val_accuracy: 0.7407\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 19s 173ms/step - loss: 0.4192 - accuracy: 0.8144 - val_loss: 0.5949 - val_accuracy: 0.7593\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 18s 171ms/step - loss: 0.3562 - accuracy: 0.8396 - val_loss: 0.6332 - val_accuracy: 0.7646\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 18s 170ms/step - loss: 0.2900 - accuracy: 0.8787 - val_loss: 0.6014 - val_accuracy: 0.7884\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.2619 - accuracy: 0.8956 - val_loss: 0.6974 - val_accuracy: 0.7910\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 18s 165ms/step - loss: 0.2126 - accuracy: 0.9022 - val_loss: 0.7051 - val_accuracy: 0.7804\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 18s 167ms/step - loss: 0.1751 - accuracy: 0.9242 - val_loss: 0.8528 - val_accuracy: 0.7751\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 18s 165ms/step - loss: 0.1503 - accuracy: 0.9437 - val_loss: 0.9299 - val_accuracy: 0.7751\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 18s 164ms/step - loss: 0.1101 - accuracy: 0.9570 - val_loss: 0.9772 - val_accuracy: 0.7831\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 18s 164ms/step - loss: 0.0949 - accuracy: 0.9624 - val_loss: 0.8099 - val_accuracy: 0.7910\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.10053133964539\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 150s 1s/step - loss: 0.6238 - accuracy: 0.6519 - val_loss: 0.4992 - val_accuracy: 0.7566\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 101s 944ms/step - loss: 0.4704 - accuracy: 0.7771 - val_loss: 0.4637 - val_accuracy: 0.7646\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 72s 671ms/step - loss: 0.3901 - accuracy: 0.8133 - val_loss: 0.4668 - val_accuracy: 0.7646\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 69s 648ms/step - loss: 0.3424 - accuracy: 0.8486 - val_loss: 0.5356 - val_accuracy: 0.7487\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 69s 649ms/step - loss: 0.2991 - accuracy: 0.8720 - val_loss: 0.6266 - val_accuracy: 0.7275\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 69s 647ms/step - loss: 0.2730 - accuracy: 0.8856 - val_loss: 0.7055 - val_accuracy: 0.7328\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 70s 657ms/step - loss: 0.2096 - accuracy: 0.9055 - val_loss: 0.8189 - val_accuracy: 0.7354\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 76.4550268650055\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 119s 885ms/step - loss: 0.6281 - accuracy: 0.6600 - val_loss: 0.4973 - val_accuracy: 0.7593\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 71s 666ms/step - loss: 0.4582 - accuracy: 0.7820 - val_loss: 0.4857 - val_accuracy: 0.7751\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 58s 541ms/step - loss: 0.3986 - accuracy: 0.8257 - val_loss: 0.4338 - val_accuracy: 0.8175\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 58s 542ms/step - loss: 0.3500 - accuracy: 0.8481 - val_loss: 0.4454 - val_accuracy: 0.7937\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 58s 542ms/step - loss: 0.3067 - accuracy: 0.8723 - val_loss: 0.4972 - val_accuracy: 0.7831\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 57s 537ms/step - loss: 0.2446 - accuracy: 0.8925 - val_loss: 0.5258 - val_accuracy: 0.7540\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 58s 542ms/step - loss: 0.2419 - accuracy: 0.8960 - val_loss: 0.5133 - val_accuracy: 0.8122\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 58s 541ms/step - loss: 0.1719 - accuracy: 0.9286 - val_loss: 0.6094 - val_accuracy: 0.8069\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 81.7460298538208\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 131s 942ms/step - loss: 0.6218 - accuracy: 0.6568 - val_loss: 0.5896 - val_accuracy: 0.7196\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 83s 778ms/step - loss: 0.4378 - accuracy: 0.7935 - val_loss: 0.5579 - val_accuracy: 0.7222\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 54s 502ms/step - loss: 0.3800 - accuracy: 0.8241 - val_loss: 0.4988 - val_accuracy: 0.7566\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 54s 508ms/step - loss: 0.3537 - accuracy: 0.8470 - val_loss: 0.5071 - val_accuracy: 0.7698\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 54s 505ms/step - loss: 0.3153 - accuracy: 0.8708 - val_loss: 0.5193 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 56s 521ms/step - loss: 0.2760 - accuracy: 0.8854 - val_loss: 0.6772 - val_accuracy: 0.7354\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 56s 521ms/step - loss: 0.2246 - accuracy: 0.9018 - val_loss: 0.6510 - val_accuracy: 0.7698\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 55s 517ms/step - loss: 0.1881 - accuracy: 0.9232 - val_loss: 0.6574 - val_accuracy: 0.7354\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 56s 519ms/step - loss: 0.1612 - accuracy: 0.9324 - val_loss: 0.6534 - val_accuracy: 0.7725\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 57s 528ms/step - loss: 0.1363 - accuracy: 0.9454 - val_loss: 0.8117 - val_accuracy: 0.7725\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 57s 535ms/step - loss: 0.0882 - accuracy: 0.9686 - val_loss: 0.9359 - val_accuracy: 0.7354\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 57s 535ms/step - loss: 0.0633 - accuracy: 0.9739 - val_loss: 0.9501 - val_accuracy: 0.7328\n",
      "Epoch 13/15\n",
      "107/107 [==============================] - 56s 528ms/step - loss: 0.0650 - accuracy: 0.9786 - val_loss: 1.1979 - val_accuracy: 0.7487\n",
      "Epoch 14/15\n",
      "107/107 [==============================] - 56s 527ms/step - loss: 0.0379 - accuracy: 0.9865 - val_loss: 1.4134 - val_accuracy: 0.7249\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 113s 832ms/step - loss: 0.6258 - accuracy: 0.6370 - val_loss: 0.4823 - val_accuracy: 0.7507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "107/107 [==============================] - 69s 644ms/step - loss: 0.4826 - accuracy: 0.7628 - val_loss: 0.4335 - val_accuracy: 0.8037\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 61s 571ms/step - loss: 0.4091 - accuracy: 0.8145 - val_loss: 0.4195 - val_accuracy: 0.8090\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 61s 570ms/step - loss: 0.3551 - accuracy: 0.8455 - val_loss: 0.4173 - val_accuracy: 0.8064\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 61s 568ms/step - loss: 0.3220 - accuracy: 0.8607 - val_loss: 0.4444 - val_accuracy: 0.7905\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 60s 565ms/step - loss: 0.2442 - accuracy: 0.8947 - val_loss: 0.4714 - val_accuracy: 0.7878\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 60s 561ms/step - loss: 0.2019 - accuracy: 0.9163 - val_loss: 0.5042 - val_accuracy: 0.8037\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 60s 560ms/step - loss: 0.1791 - accuracy: 0.9330 - val_loss: 0.7453 - val_accuracy: 0.7560\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 102s 742ms/step - loss: 0.6156 - accuracy: 0.6477 - val_loss: 0.4724 - val_accuracy: 0.7772\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 62s 585ms/step - loss: 0.4486 - accuracy: 0.7899 - val_loss: 0.4565 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 66s 617ms/step - loss: 0.3991 - accuracy: 0.8225 - val_loss: 0.4425 - val_accuracy: 0.7878\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 67s 622ms/step - loss: 0.3705 - accuracy: 0.8259 - val_loss: 0.4720 - val_accuracy: 0.8064\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 67s 622ms/step - loss: 0.3123 - accuracy: 0.8646 - val_loss: 0.4421 - val_accuracy: 0.7931\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 66s 621ms/step - loss: 0.2639 - accuracy: 0.8799 - val_loss: 0.5601 - val_accuracy: 0.7533\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 66s 613ms/step - loss: 0.2336 - accuracy: 0.8944 - val_loss: 0.5660 - val_accuracy: 0.7984\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 65s 612ms/step - loss: 0.2041 - accuracy: 0.9080 - val_loss: 0.6274 - val_accuracy: 0.7798\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 66s 614ms/step - loss: 0.1284 - accuracy: 0.9465 - val_loss: 0.7597 - val_accuracy: 0.7878\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 149s 1s/step - loss: 0.6300 - accuracy: 0.6416 - val_loss: 0.5328 - val_accuracy: 0.7056\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 126s 1s/step - loss: 0.4480 - accuracy: 0.7910 - val_loss: 0.4771 - val_accuracy: 0.7480\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 92s 860ms/step - loss: 0.3686 - accuracy: 0.8317 - val_loss: 0.4849 - val_accuracy: 0.7692\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 93s 865ms/step - loss: 0.3247 - accuracy: 0.8551 - val_loss: 0.5021 - val_accuracy: 0.7692\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 92s 861ms/step - loss: 0.3004 - accuracy: 0.8686 - val_loss: 0.5447 - val_accuracy: 0.7878\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 92s 856ms/step - loss: 0.2328 - accuracy: 0.9060 - val_loss: 0.5325 - val_accuracy: 0.7666\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 92s 859ms/step - loss: 0.2223 - accuracy: 0.9034 - val_loss: 0.5858 - val_accuracy: 0.7931\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 92s 858ms/step - loss: 0.1795 - accuracy: 0.9233 - val_loss: 0.6285 - val_accuracy: 0.7905\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 91s 854ms/step - loss: 0.1329 - accuracy: 0.9479 - val_loss: 0.8899 - val_accuracy: 0.7878\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 91s 855ms/step - loss: 0.1237 - accuracy: 0.9473 - val_loss: 0.9713 - val_accuracy: 0.7851\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 91s 855ms/step - loss: 0.0918 - accuracy: 0.9703 - val_loss: 1.0401 - val_accuracy: 0.7719\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 94s 876ms/step - loss: 0.0617 - accuracy: 0.9754 - val_loss: 1.0196 - val_accuracy: 0.7825\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 140s 1s/step - loss: 0.6318 - accuracy: 0.6395 - val_loss: 0.6074 - val_accuracy: 0.6684\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 93s 870ms/step - loss: 0.4730 - accuracy: 0.7713 - val_loss: 0.4275 - val_accuracy: 0.8170\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 88s 823ms/step - loss: 0.4067 - accuracy: 0.8126 - val_loss: 0.3890 - val_accuracy: 0.8064\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 89s 833ms/step - loss: 0.3509 - accuracy: 0.8521 - val_loss: 0.4357 - val_accuracy: 0.7878\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 89s 831ms/step - loss: 0.3042 - accuracy: 0.8672 - val_loss: 0.4327 - val_accuracy: 0.8064\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 87s 817ms/step - loss: 0.2664 - accuracy: 0.8842 - val_loss: 0.4554 - val_accuracy: 0.7931\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 88s 821ms/step - loss: 0.2151 - accuracy: 0.9058 - val_loss: 0.5252 - val_accuracy: 0.8011\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 81.69761300086975\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 154s 1s/step - loss: 0.6164 - accuracy: 0.6559 - val_loss: 0.5022 - val_accuracy: 0.7374\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 100s 938ms/step - loss: 0.4383 - accuracy: 0.7942 - val_loss: 0.4394 - val_accuracy: 0.7639\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 94s 882ms/step - loss: 0.3947 - accuracy: 0.8225 - val_loss: 0.4935 - val_accuracy: 0.7454\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 91s 849ms/step - loss: 0.3627 - accuracy: 0.8343 - val_loss: 0.4397 - val_accuracy: 0.7798\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 88s 824ms/step - loss: 0.2965 - accuracy: 0.8744 - val_loss: 0.4563 - val_accuracy: 0.7639\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 88s 822ms/step - loss: 0.2746 - accuracy: 0.8825 - val_loss: 0.4987 - val_accuracy: 0.7772\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 88s 821ms/step - loss: 0.2178 - accuracy: 0.9031 - val_loss: 0.5521 - val_accuracy: 0.7427\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 89s 832ms/step - loss: 0.1782 - accuracy: 0.9267 - val_loss: 0.5601 - val_accuracy: 0.7613\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 88s 827ms/step - loss: 0.1503 - accuracy: 0.9421 - val_loss: 0.6711 - val_accuracy: 0.7745\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "\n",
      "       acc1       acc2       acc3      acc4       acc5       acc6       acc7  \\\n",
      "0  79.62963  79.100531  76.455027  81.74603  77.248675  80.901855  80.636603   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  79.310346  81.697613  77.984083  79.471039  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.62963</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>76.455027</td>\n",
       "      <td>81.74603</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>79.471039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc1       acc2       acc3      acc4       acc5       acc6       acc7  \\\n",
       "0  79.62963  79.100531  76.455027  81.74603  77.248675  80.901855  80.636603   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  79.310346  81.697613  77.984083  79.471039  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_CR_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_48 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 585,825\n",
      "Trainable params: 585,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.6074 - accuracy: 0.6530 - val_loss: 0.5082 - val_accuracy: 0.7725\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 94s 884ms/step - loss: 0.3121 - accuracy: 0.8698 - val_loss: 0.4584 - val_accuracy: 0.7937\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 85s 797ms/step - loss: 0.1915 - accuracy: 0.9274 - val_loss: 0.6119 - val_accuracy: 0.7593\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.0979 - accuracy: 0.9680 - val_loss: 0.7193 - val_accuracy: 0.7487\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 84s 788ms/step - loss: 0.0774 - accuracy: 0.9725 - val_loss: 0.8153 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 84s 784ms/step - loss: 0.0410 - accuracy: 0.9851 - val_loss: 0.9553 - val_accuracy: 0.7698\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 84s 786ms/step - loss: 0.0226 - accuracy: 0.9934 - val_loss: 1.2181 - val_accuracy: 0.7593\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 127s 949ms/step - loss: 0.6138 - accuracy: 0.6431 - val_loss: 0.4442 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 90s 839ms/step - loss: 0.3292 - accuracy: 0.8622 - val_loss: 0.4539 - val_accuracy: 0.7937\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 96s 895ms/step - loss: 0.1999 - accuracy: 0.9270 - val_loss: 0.6271 - val_accuracy: 0.7698\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 98s 911ms/step - loss: 0.0824 - accuracy: 0.9717 - val_loss: 0.8991 - val_accuracy: 0.7540\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 106s 992ms/step - loss: 0.0435 - accuracy: 0.9865 - val_loss: 1.0379 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 175s 2s/step - loss: 0.0315 - accuracy: 0.9892 - val_loss: 1.0566 - val_accuracy: 0.7487\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 175s 2s/step - loss: 0.0386 - accuracy: 0.9884 - val_loss: 1.3519 - val_accuracy: 0.7407\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 229s 2s/step - loss: 0.6054 - accuracy: 0.6627 - val_loss: 0.5285 - val_accuracy: 0.7804\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 154s 1s/step - loss: 0.3188 - accuracy: 0.8693 - val_loss: 0.7387 - val_accuracy: 0.7593\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 123s 1s/step - loss: 0.1683 - accuracy: 0.9366 - val_loss: 0.8509 - val_accuracy: 0.7407\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 122s 1s/step - loss: 0.0970 - accuracy: 0.9654 - val_loss: 1.2095 - val_accuracy: 0.7513\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 121s 1s/step - loss: 0.0507 - accuracy: 0.9841 - val_loss: 1.2599 - val_accuracy: 0.7698\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 119s 1s/step - loss: 0.0360 - accuracy: 0.9910 - val_loss: 1.5087 - val_accuracy: 0.7487\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 173s 1s/step - loss: 0.6029 - accuracy: 0.6600 - val_loss: 0.3998 - val_accuracy: 0.8069\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 122s 1s/step - loss: 0.3374 - accuracy: 0.8504 - val_loss: 0.4038 - val_accuracy: 0.8254\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 112s 1s/step - loss: 0.1775 - accuracy: 0.9337 - val_loss: 0.4133 - val_accuracy: 0.8201\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 115s 1s/step - loss: 0.1042 - accuracy: 0.9641 - val_loss: 0.6352 - val_accuracy: 0.8201\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 117s 1s/step - loss: 0.0590 - accuracy: 0.9804 - val_loss: 0.5685 - val_accuracy: 0.8333\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 116s 1s/step - loss: 0.0349 - accuracy: 0.9897 - val_loss: 0.7637 - val_accuracy: 0.8228\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 115s 1s/step - loss: 0.0351 - accuracy: 0.9876 - val_loss: 0.6974 - val_accuracy: 0.8201\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 117s 1s/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.9127 - val_accuracy: 0.8122\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 114s 1s/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.8062 - val_accuracy: 0.8228\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 115s 1s/step - loss: 0.0190 - accuracy: 0.9951 - val_loss: 0.8287 - val_accuracy: 0.8307\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 83.33333134651184\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 188s 1s/step - loss: 0.6158 - accuracy: 0.6517 - val_loss: 0.3901 - val_accuracy: 0.8095\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 144s 1s/step - loss: 0.3142 - accuracy: 0.8666 - val_loss: 0.3850 - val_accuracy: 0.8360\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 112s 1s/step - loss: 0.1772 - accuracy: 0.9390 - val_loss: 0.4169 - val_accuracy: 0.8175\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 108s 1s/step - loss: 0.0909 - accuracy: 0.9698 - val_loss: 0.5597 - val_accuracy: 0.7884\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 108s 1s/step - loss: 0.0555 - accuracy: 0.9803 - val_loss: 0.6691 - val_accuracy: 0.8042\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 107s 1s/step - loss: 0.0324 - accuracy: 0.9895 - val_loss: 0.7725 - val_accuracy: 0.7989\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 107s 1000ms/step - loss: 0.0410 - accuracy: 0.9875 - val_loss: 1.1165 - val_accuracy: 0.7831\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 83.59788656234741\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 152s 1s/step - loss: 0.6092 - accuracy: 0.6529 - val_loss: 0.4275 - val_accuracy: 0.7958\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 98s 920ms/step - loss: 0.3222 - accuracy: 0.8681 - val_loss: 0.4057 - val_accuracy: 0.8223\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 82s 766ms/step - loss: 0.1968 - accuracy: 0.9294 - val_loss: 0.4729 - val_accuracy: 0.8011\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 81s 759ms/step - loss: 0.0874 - accuracy: 0.9704 - val_loss: 0.5483 - val_accuracy: 0.7798\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 79s 740ms/step - loss: 0.0768 - accuracy: 0.9740 - val_loss: 0.7032 - val_accuracy: 0.7905\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 77s 719ms/step - loss: 0.0330 - accuracy: 0.9885 - val_loss: 0.8902 - val_accuracy: 0.7798\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 77s 715ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.8562 - val_accuracy: 0.7984\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 82.22811818122864\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 118s 887ms/step - loss: 0.6089 - accuracy: 0.6563 - val_loss: 0.4306 - val_accuracy: 0.7984\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 91s 853ms/step - loss: 0.3351 - accuracy: 0.8645 - val_loss: 0.4147 - val_accuracy: 0.8170\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 100s 931ms/step - loss: 0.1798 - accuracy: 0.9350 - val_loss: 0.5395 - val_accuracy: 0.8090\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 99s 929ms/step - loss: 0.0906 - accuracy: 0.9657 - val_loss: 0.6641 - val_accuracy: 0.7905\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 100s 932ms/step - loss: 0.0576 - accuracy: 0.9814 - val_loss: 0.7671 - val_accuracy: 0.8011\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 99s 926ms/step - loss: 0.0314 - accuracy: 0.9910 - val_loss: 0.9362 - val_accuracy: 0.7905\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 99s 923ms/step - loss: 0.0476 - accuracy: 0.9813 - val_loss: 1.1575 - val_accuracy: 0.7878\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.69761300086975\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 213s 2s/step - loss: 0.6031 - accuracy: 0.6610 - val_loss: 0.4561 - val_accuracy: 0.7666\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 180s 2s/step - loss: 0.3103 - accuracy: 0.8733 - val_loss: 0.4520 - val_accuracy: 0.7798\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 148s 1s/step - loss: 0.1730 - accuracy: 0.9385 - val_loss: 0.4837 - val_accuracy: 0.8037\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 149s 1s/step - loss: 0.1093 - accuracy: 0.9641 - val_loss: 0.5766 - val_accuracy: 0.7798\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 141s 1s/step - loss: 0.0644 - accuracy: 0.9783 - val_loss: 1.0456 - val_accuracy: 0.7692\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 133s 1s/step - loss: 0.0282 - accuracy: 0.9896 - val_loss: 0.8313 - val_accuracy: 0.7905\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 132s 1s/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 1.1976 - val_accuracy: 0.7401\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 132s 1s/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 1.1881 - val_accuracy: 0.7798\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 80.37135004997253\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 192s 2s/step - loss: 0.6070 - accuracy: 0.6578 - val_loss: 0.4979 - val_accuracy: 0.7613\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.3151 - accuracy: 0.8670 - val_loss: 0.4879 - val_accuracy: 0.7745\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.1668 - accuracy: 0.9397 - val_loss: 0.5738 - val_accuracy: 0.7851\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.0871 - accuracy: 0.9715 - val_loss: 0.8775 - val_accuracy: 0.7401\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.0521 - accuracy: 0.9858 - val_loss: 1.0473 - val_accuracy: 0.7454\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 146s 1s/step - loss: 0.0287 - accuracy: 0.9915 - val_loss: 1.0611 - val_accuracy: 0.7639\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 146s 1s/step - loss: 0.0185 - accuracy: 0.9938 - val_loss: 1.3425 - val_accuracy: 0.7401\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.0251 - accuracy: 0.9935 - val_loss: 1.1186 - val_accuracy: 0.7666\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 145s 1s/step - loss: 0.5980 - accuracy: 0.6671 - val_loss: 0.4098 - val_accuracy: 0.8064\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 98s 913ms/step - loss: 0.3390 - accuracy: 0.8559 - val_loss: 0.3922 - val_accuracy: 0.8249\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 91s 851ms/step - loss: 0.1908 - accuracy: 0.9230 - val_loss: 0.4817 - val_accuracy: 0.8011\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 91s 848ms/step - loss: 0.0904 - accuracy: 0.9688 - val_loss: 0.5187 - val_accuracy: 0.8117\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 90s 845ms/step - loss: 0.0578 - accuracy: 0.9800 - val_loss: 0.5895 - val_accuracy: 0.8196\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 90s 845ms/step - loss: 0.0319 - accuracy: 0.9911 - val_loss: 0.7169 - val_accuracy: 0.7958\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 90s 846ms/step - loss: 0.0241 - accuracy: 0.9910 - val_loss: 0.7692 - val_accuracy: 0.8143\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 82.49337077140808\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  79.365081  79.62963  82.539684  80.158728  81.481481  80.106103  78.514588   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  77.453583  78.514588  79.310346  79.707381  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.365081</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>83.333331</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>82.228118</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>80.37135</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>82.493371</td>\n",
       "      <td>80.900875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  79.365081  79.365081  78.042328  83.333331  83.597887  82.228118   \n",
       "\n",
       "        acc7      acc8       acc9      acc10        AVG  \n",
       "0  81.697613  80.37135  78.514588  82.493371  80.900875  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_CR_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
