{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LSTM_MR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t_uapOVOj3Wd",
        "aPuyh9oij3Wg"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuYFRWuAj3V-"
      },
      "source": [
        "# LSTM Classification with MR Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using LSTM model on the MR Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyW5FqogkHyZ",
        "outputId": "0c78a4fc-392e-446f-b784-a30ea4500626"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7DImIH8j3WM",
        "outputId": "dc3ffc36-b124-44d5-a883-f9373bc87496"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npI8lkGrj3WN",
        "outputId": "7af91454-a9c3-43a6-883a-c0afe6ca5e8d"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZmZUF7Nj3WS"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "z1rr1p_kj3WT",
        "outputId": "67fc8080-0db7-43c7-dcd3-b64d8d58c4b5"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Google Colab/0_data/MR/MR.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10662, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simplistic , silly and tedious .</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it 's so laddish and juvenile , only teenage b...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>exploitative and largely devoid of the depth o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>garbus discards the potential for pathological...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a visually flashy but narratively opaque and e...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10657</th>\n",
              "      <td>both exuberantly romantic and serenely melanch...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10658</th>\n",
              "      <td>mazel tov to a film about a family 's joyous l...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10659</th>\n",
              "      <td>standing in the shadows of motown is the best ...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10660</th>\n",
              "      <td>it 's nice to see piscopo again after all thes...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10661</th>\n",
              "      <td>provides a porthole into that noble , tremblin...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10662 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label  split\n",
              "0                       simplistic , silly and tedious .      0  train\n",
              "1      it 's so laddish and juvenile , only teenage b...      0  train\n",
              "2      exploitative and largely devoid of the depth o...      0  train\n",
              "3      garbus discards the potential for pathological...      0  train\n",
              "4      a visually flashy but narratively opaque and e...      0  train\n",
              "...                                                  ...    ...    ...\n",
              "10657  both exuberantly romantic and serenely melanch...      1  train\n",
              "10658  mazel tov to a film about a family 's joyous l...      1  train\n",
              "10659  standing in the shadows of motown is the best ...      1  train\n",
              "10660  it 's nice to see piscopo again after all thes...      1  train\n",
              "10661  provides a porthole into that noble , tremblin...      1  train\n",
              "\n",
              "[10662 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmJoKydWj3WV",
        "outputId": "315e2437-8f60-41b6-bba2-449522f18c97"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10662 entries, 0 to 10661\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10662 non-null  object\n",
            " 1   label     10662 non-null  int64 \n",
            " 2   split     10662 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 250.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "XORWS6mcj3WW",
        "outputId": "fb368012-a8dd-482f-dd7f-ec8f62c56731"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5331</td>\n",
              "      <td>5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5331</td>\n",
              "      <td>5331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          5331   5331\n",
              "1          5331   5331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap1tOeTDj3WX"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hxO50dh-j3WX",
        "outputId": "d8c78ad1-0bc1-4219-d601-aa49145ef445"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'simplistic , silly and tedious .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5cFhVu5j3WY"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUHEdVdnj3WZ"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqCxzAdmj3Wa"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PN6mnYUj3Wb",
        "outputId": "047e67f7-9c52-4121-f71e-e7f79dd5f217"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
            "Into a sequence of int: [3, 544, 1838, 13, 3909, 3366, 4, 658, 2629, 416, 10, 236, 4, 10112]\n",
            "Into a padded sequence: [    3   544  1838    13  3909  3366     4   658  2629   416    10   236\n",
            "     4 10112     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeSSAYfpj3Wc",
        "outputId": "edca944a-f8fa-4f71-f882-d8967b35cbbf"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "a 3\n",
            "and 4\n",
            "of 5\n",
            "to 6\n",
            "is 7\n",
            "'s 8\n",
            "it 9\n",
            "in 10\n",
            "18760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nShziHqhj3Wc"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_uapOVOj3Wd"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gher3K6lj3We"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s_We-KLj3Wf",
        "outputId": "d2936013-9d66-4741-cc71-d54c795e7e4b"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 487,009\n",
            "Trainable params: 487,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgNJyUXYj3Wf"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPuyh9oij3Wg"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JQVVe3Mwj3Wi",
        "outputId": "9c0a2fb3-062c-4c9a-cf3a-47e9ae2046d5"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 53s 146ms/step - loss: 0.6207 - accuracy: 0.6483 - val_loss: 0.4452 - val_accuracy: 0.7882\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.2374 - accuracy: 0.9049 - val_loss: 0.4954 - val_accuracy: 0.7648\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0923 - accuracy: 0.9697 - val_loss: 0.6930 - val_accuracy: 0.7470\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0347 - accuracy: 0.9892 - val_loss: 0.9162 - val_accuracy: 0.7498\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 43s 143ms/step - loss: 0.0163 - accuracy: 0.9959 - val_loss: 0.9315 - val_accuracy: 0.7385\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 42s 141ms/step - loss: 0.0051 - accuracy: 0.9993 - val_loss: 1.1845 - val_accuracy: 0.7601\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 78.81911993026733\n",
            "Training 2: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 59s 160ms/step - loss: 0.6289 - accuracy: 0.6185 - val_loss: 0.4448 - val_accuracy: 0.7835\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 41s 136ms/step - loss: 0.2458 - accuracy: 0.9065 - val_loss: 0.4986 - val_accuracy: 0.7816\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 42s 141ms/step - loss: 0.0835 - accuracy: 0.9735 - val_loss: 0.6476 - val_accuracy: 0.7713\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 43s 142ms/step - loss: 0.0326 - accuracy: 0.9909 - val_loss: 0.8399 - val_accuracy: 0.7798\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 36s 120ms/step - loss: 0.0135 - accuracy: 0.9968 - val_loss: 1.1053 - val_accuracy: 0.7732\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 40s 133ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 1.1968 - val_accuracy: 0.7676\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 78.35051417350769\n",
            "Training 3: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 73s 209ms/step - loss: 0.6228 - accuracy: 0.6304 - val_loss: 0.4764 - val_accuracy: 0.7702\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 0.2552 - accuracy: 0.8979 - val_loss: 0.5250 - val_accuracy: 0.7477\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 48s 161ms/step - loss: 0.0902 - accuracy: 0.9703 - val_loss: 0.7652 - val_accuracy: 0.7495\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 48s 161ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.8496 - val_accuracy: 0.7495\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 49s 163ms/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 1.3190 - val_accuracy: 0.7364\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 50s 167ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 1.3465 - val_accuracy: 0.7289\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 77.01688408851624\n",
            "Training 4: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 81s 232ms/step - loss: 0.6278 - accuracy: 0.6190 - val_loss: 0.4537 - val_accuracy: 0.7702\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 55s 185ms/step - loss: 0.2455 - accuracy: 0.9066 - val_loss: 0.5078 - val_accuracy: 0.7824\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 0.0895 - accuracy: 0.9689 - val_loss: 0.6717 - val_accuracy: 0.7711\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 53s 175ms/step - loss: 0.0333 - accuracy: 0.9903 - val_loss: 0.9168 - val_accuracy: 0.7608\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 48s 161ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 1.1397 - val_accuracy: 0.7580\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 1.1467 - val_accuracy: 0.7683\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.2097 - val_accuracy: 0.7523\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00007: early stopping\n",
            "Test Accuracy: 78.23639512062073\n",
            "Training 5: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 60s 167ms/step - loss: 0.6207 - accuracy: 0.6229 - val_loss: 0.4881 - val_accuracy: 0.7702\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 72s 240ms/step - loss: 0.2498 - accuracy: 0.9043 - val_loss: 0.5391 - val_accuracy: 0.7664\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 68s 225ms/step - loss: 0.0899 - accuracy: 0.9712 - val_loss: 0.6882 - val_accuracy: 0.7730\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 67s 223ms/step - loss: 0.0308 - accuracy: 0.9901 - val_loss: 0.9647 - val_accuracy: 0.7589\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 66s 221ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 1.0233 - val_accuracy: 0.7514\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 66s 220ms/step - loss: 0.0077 - accuracy: 0.9984 - val_loss: 1.4675 - val_accuracy: 0.7542\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 66s 219ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 1.3471 - val_accuracy: 0.7542\n",
            "Epoch 8/15\n",
            "300/300 [==============================] - 60s 201ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 1.5156 - val_accuracy: 0.7514\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00008: early stopping\n",
            "Test Accuracy: 77.29831337928772\n",
            "Training 6: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 79s 227ms/step - loss: 0.6187 - accuracy: 0.6355 - val_loss: 0.4790 - val_accuracy: 0.7608\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 59s 196ms/step - loss: 0.2378 - accuracy: 0.9061 - val_loss: 0.5441 - val_accuracy: 0.7589\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 58s 193ms/step - loss: 0.0873 - accuracy: 0.9744 - val_loss: 0.7441 - val_accuracy: 0.7720\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 0.0318 - accuracy: 0.9905 - val_loss: 0.8605 - val_accuracy: 0.7730\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.0178 - accuracy: 0.9949 - val_loss: 0.9896 - val_accuracy: 0.7552\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 1.3427 - val_accuracy: 0.7580\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 56s 188ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 1.3415 - val_accuracy: 0.7561\n",
            "Epoch 8/15\n",
            "300/300 [==============================] - 57s 189ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.3660 - val_accuracy: 0.7495\n",
            "Epoch 9/15\n",
            "300/300 [==============================] - 57s 191ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 1.5837 - val_accuracy: 0.7542\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 77.29831337928772\n",
            "Training 7: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 81s 235ms/step - loss: 0.6189 - accuracy: 0.6436 - val_loss: 0.4870 - val_accuracy: 0.7683\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.2338 - accuracy: 0.9091 - val_loss: 0.4969 - val_accuracy: 0.7645\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.0842 - accuracy: 0.9717 - val_loss: 0.6881 - val_accuracy: 0.7523\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 1.0513 - val_accuracy: 0.7430\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 1.3554 - val_accuracy: 0.7486\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 68s 226ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 1.2135 - val_accuracy: 0.7495\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 76.82926654815674\n",
            "Training 8: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 57s 159ms/step - loss: 0.6227 - accuracy: 0.6305 - val_loss: 0.4692 - val_accuracy: 0.7739\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.2520 - accuracy: 0.8998 - val_loss: 0.5117 - val_accuracy: 0.7739\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 63s 208ms/step - loss: 0.0912 - accuracy: 0.9708 - val_loss: 0.6269 - val_accuracy: 0.7852\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.0358 - accuracy: 0.9888 - val_loss: 0.8426 - val_accuracy: 0.7720\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.0185 - accuracy: 0.9947 - val_loss: 1.1169 - val_accuracy: 0.7617\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 62s 208ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 1.1526 - val_accuracy: 0.7674\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 62s 208ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 1.2441 - val_accuracy: 0.7674\n",
            "Epoch 8/15\n",
            "300/300 [==============================] - 62s 208ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 1.4074 - val_accuracy: 0.7664\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00008: early stopping\n",
            "Test Accuracy: 78.51782441139221\n",
            "Training 9: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 85s 248ms/step - loss: 0.6201 - accuracy: 0.6359 - val_loss: 0.4857 - val_accuracy: 0.7664\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 56s 187ms/step - loss: 0.2452 - accuracy: 0.9041 - val_loss: 0.5577 - val_accuracy: 0.7645\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 54s 178ms/step - loss: 0.0927 - accuracy: 0.9680 - val_loss: 0.7111 - val_accuracy: 0.7598\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 54s 179ms/step - loss: 0.0344 - accuracy: 0.9904 - val_loss: 0.8728 - val_accuracy: 0.7542\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 53s 177ms/step - loss: 0.0169 - accuracy: 0.9956 - val_loss: 1.3086 - val_accuracy: 0.7495\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 0.0075 - accuracy: 0.9978 - val_loss: 1.3622 - val_accuracy: 0.7458\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 76.64164900779724\n",
            "Training 10: \n",
            "Epoch 1/15\n",
            "300/300 [==============================] - 55s 152ms/step - loss: 0.6235 - accuracy: 0.6354 - val_loss: 0.4739 - val_accuracy: 0.7664\n",
            "Epoch 2/15\n",
            "300/300 [==============================] - 87s 290ms/step - loss: 0.2387 - accuracy: 0.9104 - val_loss: 0.5146 - val_accuracy: 0.7711\n",
            "Epoch 3/15\n",
            "300/300 [==============================] - 88s 295ms/step - loss: 0.0860 - accuracy: 0.9690 - val_loss: 0.7097 - val_accuracy: 0.7645\n",
            "Epoch 4/15\n",
            "300/300 [==============================] - 88s 294ms/step - loss: 0.0328 - accuracy: 0.9902 - val_loss: 0.8107 - val_accuracy: 0.7523\n",
            "Epoch 5/15\n",
            "300/300 [==============================] - 88s 294ms/step - loss: 0.0137 - accuracy: 0.9970 - val_loss: 1.1337 - val_accuracy: 0.7552\n",
            "Epoch 6/15\n",
            "300/300 [==============================] - 88s 293ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 1.0864 - val_accuracy: 0.7730\n",
            "Epoch 7/15\n",
            "300/300 [==============================] - 85s 284ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 1.1369 - val_accuracy: 0.7477\n",
            "Epoch 8/15\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 1.2712 - val_accuracy: 0.7458\n",
            "Epoch 9/15\n",
            "300/300 [==============================] - 85s 283ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 1.4650 - val_accuracy: 0.7467\n",
            "Epoch 10/15\n",
            "300/300 [==============================] - 85s 283ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 1.7114 - val_accuracy: 0.7251\n",
            "Epoch 11/15\n",
            "300/300 [==============================] - 84s 281ms/step - loss: 7.1743e-04 - accuracy: 1.0000 - val_loss: 1.7638 - val_accuracy: 0.7486\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 77.29831337928772\n",
            "\n",
            "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
            "0  78.81912  78.350514  77.016884  78.236395  77.298313  77.298313  76.829267   \n",
            "\n",
            "        acc8       acc9      acc10        AVG  \n",
            "0  78.517824  76.641649  77.298313  77.630659  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q16mpYJj3Wu"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGPb4NO1j3Wz",
        "outputId": "38219e57-ea99-47fc-cd42-84162e44f337"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78.81912</td>\n",
              "      <td>78.350514</td>\n",
              "      <td>77.016884</td>\n",
              "      <td>78.236395</td>\n",
              "      <td>77.298313</td>\n",
              "      <td>77.298313</td>\n",
              "      <td>76.829267</td>\n",
              "      <td>78.517824</td>\n",
              "      <td>76.641649</td>\n",
              "      <td>77.298313</td>\n",
              "      <td>77.630659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
              "0  78.81912  78.350514  77.016884  78.236395  77.298313  77.298313  76.829267   \n",
              "\n",
              "        acc8       acc9      acc10        AVG  \n",
              "0  78.517824  76.641649  77.298313  77.630659  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLNDOk26j3W0"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('LSTM_MR.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEYNudBKj3W1"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq6yCgpuj3W2"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJVyjUnpj3W3"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkmVoHxUj3W4"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Google Colab/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqC0yoffj3W5",
        "outputId": "36797040-6b75-401a-d41e-b6d9267fe0c9"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1_pWDc5j3W6"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utjG_C64j3W7"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4mVRw4sj3W8",
        "outputId": "cfaccba6-73ed-440a-efde-fb342ac9ae7e"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16448 words present from 18760 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptuuPR29j3W9"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALGK6dPRj3W9",
        "outputId": "952d1104-cd24-4844-d7f0-31c9458fd9f7"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8_obpzEj3W-"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf6FXv8aj3W-",
        "outputId": "32e86982-d31f-42df-c5e5-5e0bb4064d2f"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvTB1uLPj3W_"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns4ST2njj3W_"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdD9t45Aj3XA",
        "outputId": "9cf53114-b719-431e-bf9e-e039f6934e75"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 487,009\n",
            "Trainable params: 187,009\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJFGQhqEj3XA"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjU0AG8qj3XB"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ysnQ6c9j3XB",
        "outputId": "0da8fb66-40e2-450c-bfe4-4c78f7950109"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 36s 96ms/step - loss: 0.5884 - accuracy: 0.6811 - val_loss: 0.4515 - val_accuracy: 0.7929\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 25s 85ms/step - loss: 0.4539 - accuracy: 0.7806 - val_loss: 0.4398 - val_accuracy: 0.7929\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 25s 84ms/step - loss: 0.4222 - accuracy: 0.8031 - val_loss: 0.4254 - val_accuracy: 0.8041\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 25s 85ms/step - loss: 0.3955 - accuracy: 0.8185 - val_loss: 0.4230 - val_accuracy: 0.7985\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 25s 84ms/step - loss: 0.3709 - accuracy: 0.8287 - val_loss: 0.4223 - val_accuracy: 0.8051\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 25s 84ms/step - loss: 0.3354 - accuracy: 0.8484 - val_loss: 0.4167 - val_accuracy: 0.8144\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 25s 85ms/step - loss: 0.2957 - accuracy: 0.8707 - val_loss: 0.4536 - val_accuracy: 0.8004\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 25s 85ms/step - loss: 0.2658 - accuracy: 0.8870 - val_loss: 0.4541 - val_accuracy: 0.8022\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 25s 85ms/step - loss: 0.2219 - accuracy: 0.9096 - val_loss: 0.5526 - val_accuracy: 0.7835\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 26s 85ms/step - loss: 0.1893 - accuracy: 0.9259 - val_loss: 0.5700 - val_accuracy: 0.7826\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 25s 84ms/step - loss: 0.1508 - accuracy: 0.9469 - val_loss: 0.5682 - val_accuracy: 0.8022\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 26s 86ms/step - loss: 0.1164 - accuracy: 0.9551 - val_loss: 0.6529 - val_accuracy: 0.7910\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 26s 85ms/step - loss: 0.1065 - accuracy: 0.9626 - val_loss: 0.6713 - val_accuracy: 0.7910\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 25s 85ms/step - loss: 0.0691 - accuracy: 0.9766 - val_loss: 0.8262 - val_accuracy: 0.7910\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 81.44329786300659\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 36s 97ms/step - loss: 0.5775 - accuracy: 0.7003 - val_loss: 0.4847 - val_accuracy: 0.7545\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.4346 - accuracy: 0.7948 - val_loss: 0.4710 - val_accuracy: 0.7751\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 26s 86ms/step - loss: 0.4146 - accuracy: 0.8033 - val_loss: 0.5282 - val_accuracy: 0.7507\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3968 - accuracy: 0.8172 - val_loss: 0.4825 - val_accuracy: 0.7638\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.3630 - accuracy: 0.8298 - val_loss: 0.4731 - val_accuracy: 0.7676\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.3213 - accuracy: 0.8606 - val_loss: 0.4714 - val_accuracy: 0.7685\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 27s 89ms/step - loss: 0.2985 - accuracy: 0.8708 - val_loss: 0.5105 - val_accuracy: 0.7573\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 27s 88ms/step - loss: 0.2589 - accuracy: 0.8922 - val_loss: 0.5234 - val_accuracy: 0.7779\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 26s 87ms/step - loss: 0.2294 - accuracy: 0.8977 - val_loss: 0.6159 - val_accuracy: 0.7610\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.1826 - accuracy: 0.9302 - val_loss: 0.6432 - val_accuracy: 0.7741\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.1518 - accuracy: 0.9402 - val_loss: 0.6948 - val_accuracy: 0.7657\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.1151 - accuracy: 0.9554 - val_loss: 0.7901 - val_accuracy: 0.7666\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 27s 88ms/step - loss: 0.0956 - accuracy: 0.9668 - val_loss: 0.7545 - val_accuracy: 0.7619\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 27s 89ms/step - loss: 0.0883 - accuracy: 0.9702 - val_loss: 0.8837 - val_accuracy: 0.7648\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 0.0631 - accuracy: 0.9775 - val_loss: 0.9720 - val_accuracy: 0.7676\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 27s 89ms/step - loss: 0.0555 - accuracy: 0.9804 - val_loss: 0.9646 - val_accuracy: 0.7498\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Test Accuracy: 77.7881920337677\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 38s 97ms/step - loss: 0.5888 - accuracy: 0.6885 - val_loss: 0.4726 - val_accuracy: 0.7720\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 27s 89ms/step - loss: 0.4456 - accuracy: 0.7873 - val_loss: 0.4606 - val_accuracy: 0.7739\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 27s 89ms/step - loss: 0.4154 - accuracy: 0.8015 - val_loss: 0.4563 - val_accuracy: 0.7786\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 27s 89ms/step - loss: 0.4004 - accuracy: 0.8118 - val_loss: 0.4612 - val_accuracy: 0.7833\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 27s 90ms/step - loss: 0.3614 - accuracy: 0.8381 - val_loss: 0.4711 - val_accuracy: 0.7880\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 27s 90ms/step - loss: 0.3215 - accuracy: 0.8575 - val_loss: 0.4704 - val_accuracy: 0.7814\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.2867 - accuracy: 0.8798 - val_loss: 0.4982 - val_accuracy: 0.7777\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 27s 92ms/step - loss: 0.2585 - accuracy: 0.8860 - val_loss: 0.5592 - val_accuracy: 0.7749\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.2076 - accuracy: 0.9179 - val_loss: 0.6191 - val_accuracy: 0.7739\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 29s 96ms/step - loss: 0.1756 - accuracy: 0.9303 - val_loss: 0.6340 - val_accuracy: 0.7608\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 27s 90ms/step - loss: 0.1482 - accuracy: 0.9451 - val_loss: 0.7319 - val_accuracy: 0.7561\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.1153 - accuracy: 0.9584 - val_loss: 0.8211 - val_accuracy: 0.7636\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 27s 90ms/step - loss: 0.0946 - accuracy: 0.9673 - val_loss: 0.8056 - val_accuracy: 0.7692\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 78.79924774169922\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 37s 100ms/step - loss: 0.5931 - accuracy: 0.6668 - val_loss: 0.4792 - val_accuracy: 0.7411\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 27s 92ms/step - loss: 0.4597 - accuracy: 0.7817 - val_loss: 0.4736 - val_accuracy: 0.7580\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.4259 - accuracy: 0.7993 - val_loss: 0.4400 - val_accuracy: 0.7767\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.3860 - accuracy: 0.8216 - val_loss: 0.4531 - val_accuracy: 0.7711\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.3536 - accuracy: 0.8438 - val_loss: 0.4484 - val_accuracy: 0.7805\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.3316 - accuracy: 0.8497 - val_loss: 0.4390 - val_accuracy: 0.8002\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 27s 92ms/step - loss: 0.2987 - accuracy: 0.8674 - val_loss: 0.4650 - val_accuracy: 0.8011\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.2608 - accuracy: 0.8838 - val_loss: 0.4816 - val_accuracy: 0.7795\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.2183 - accuracy: 0.9081 - val_loss: 0.5567 - val_accuracy: 0.7814\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.1751 - accuracy: 0.9308 - val_loss: 0.5460 - val_accuracy: 0.7852\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 27s 90ms/step - loss: 0.1427 - accuracy: 0.9477 - val_loss: 0.6148 - val_accuracy: 0.7983\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1133 - accuracy: 0.9573 - val_loss: 0.7343 - val_accuracy: 0.7795\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 27s 92ms/step - loss: 0.0942 - accuracy: 0.9651 - val_loss: 0.8798 - val_accuracy: 0.7739\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 27s 90ms/step - loss: 0.0621 - accuracy: 0.9807 - val_loss: 0.7757 - val_accuracy: 0.7814\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 27s 92ms/step - loss: 0.0524 - accuracy: 0.9831 - val_loss: 0.8753 - val_accuracy: 0.7814\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 80.1125705242157\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 37s 101ms/step - loss: 0.5907 - accuracy: 0.6745 - val_loss: 0.4653 - val_accuracy: 0.7833\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.4494 - accuracy: 0.7845 - val_loss: 0.4470 - val_accuracy: 0.7899\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4232 - accuracy: 0.7938 - val_loss: 0.4425 - val_accuracy: 0.7842\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3992 - accuracy: 0.8135 - val_loss: 0.4366 - val_accuracy: 0.7871\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.3569 - accuracy: 0.8374 - val_loss: 0.4503 - val_accuracy: 0.7946\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3257 - accuracy: 0.8550 - val_loss: 0.4645 - val_accuracy: 0.7871\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3024 - accuracy: 0.8670 - val_loss: 0.4734 - val_accuracy: 0.7880\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2631 - accuracy: 0.8926 - val_loss: 0.4986 - val_accuracy: 0.7899\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2196 - accuracy: 0.9103 - val_loss: 0.5709 - val_accuracy: 0.8021\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1985 - accuracy: 0.9202 - val_loss: 0.6283 - val_accuracy: 0.7861\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1472 - accuracy: 0.9471 - val_loss: 0.7010 - val_accuracy: 0.7777\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1154 - accuracy: 0.9563 - val_loss: 0.7317 - val_accuracy: 0.7795\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.0910 - accuracy: 0.9691 - val_loss: 0.7781 - val_accuracy: 0.7664\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.0692 - accuracy: 0.9775 - val_loss: 0.9662 - val_accuracy: 0.7570\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.0523 - accuracy: 0.9858 - val_loss: 0.9699 - val_accuracy: 0.7805\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.0469 - accuracy: 0.9859 - val_loss: 1.0360 - val_accuracy: 0.7674\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.0310 - accuracy: 0.9910 - val_loss: 0.9893 - val_accuracy: 0.7636\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00017: early stopping\n",
            "Test Accuracy: 80.20637631416321\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 38s 103ms/step - loss: 0.5730 - accuracy: 0.6934 - val_loss: 0.4852 - val_accuracy: 0.7683\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4531 - accuracy: 0.7810 - val_loss: 0.5096 - val_accuracy: 0.7580\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4268 - accuracy: 0.7933 - val_loss: 0.4674 - val_accuracy: 0.7842\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.3929 - accuracy: 0.8188 - val_loss: 0.5062 - val_accuracy: 0.7814\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3632 - accuracy: 0.8324 - val_loss: 0.4819 - val_accuracy: 0.7627\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3259 - accuracy: 0.8479 - val_loss: 0.4760 - val_accuracy: 0.7805\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3026 - accuracy: 0.8698 - val_loss: 0.4736 - val_accuracy: 0.7749\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.2468 - accuracy: 0.8948 - val_loss: 0.5428 - val_accuracy: 0.7767\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2124 - accuracy: 0.9168 - val_loss: 0.5705 - val_accuracy: 0.7636\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.1879 - accuracy: 0.9245 - val_loss: 0.6762 - val_accuracy: 0.7598\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1415 - accuracy: 0.9475 - val_loss: 0.6810 - val_accuracy: 0.7608\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 78.42401266098022\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 38s 102ms/step - loss: 0.5828 - accuracy: 0.6848 - val_loss: 0.4732 - val_accuracy: 0.7749\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 28s 95ms/step - loss: 0.4557 - accuracy: 0.7777 - val_loss: 0.4441 - val_accuracy: 0.7861\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.4222 - accuracy: 0.7980 - val_loss: 0.4463 - val_accuracy: 0.7786\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3896 - accuracy: 0.8195 - val_loss: 0.4304 - val_accuracy: 0.7927\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.3619 - accuracy: 0.8336 - val_loss: 0.4461 - val_accuracy: 0.7889\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.3293 - accuracy: 0.8548 - val_loss: 0.4268 - val_accuracy: 0.8077\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2841 - accuracy: 0.8747 - val_loss: 0.4348 - val_accuracy: 0.7946\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2556 - accuracy: 0.8964 - val_loss: 0.4894 - val_accuracy: 0.7927\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2198 - accuracy: 0.9106 - val_loss: 0.5293 - val_accuracy: 0.7824\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1820 - accuracy: 0.9251 - val_loss: 0.6027 - val_accuracy: 0.7749\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1424 - accuracy: 0.9473 - val_loss: 0.6080 - val_accuracy: 0.7936\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1110 - accuracy: 0.9640 - val_loss: 0.6405 - val_accuracy: 0.8021\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 30s 99ms/step - loss: 0.0816 - accuracy: 0.9704 - val_loss: 0.7499 - val_accuracy: 0.7758\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.0627 - accuracy: 0.9773 - val_loss: 0.8213 - val_accuracy: 0.7946\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 80.7692289352417\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 37s 101ms/step - loss: 0.5868 - accuracy: 0.6796 - val_loss: 0.4931 - val_accuracy: 0.7542\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4557 - accuracy: 0.7860 - val_loss: 0.4519 - val_accuracy: 0.7814\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4348 - accuracy: 0.7913 - val_loss: 0.4403 - val_accuracy: 0.7805\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4062 - accuracy: 0.8087 - val_loss: 0.4500 - val_accuracy: 0.7871\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.3661 - accuracy: 0.8302 - val_loss: 0.4420 - val_accuracy: 0.7908\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3385 - accuracy: 0.8546 - val_loss: 0.4501 - val_accuracy: 0.7833\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.2976 - accuracy: 0.8711 - val_loss: 0.5020 - val_accuracy: 0.7852\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.2554 - accuracy: 0.8858 - val_loss: 0.4954 - val_accuracy: 0.7814\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.2394 - accuracy: 0.9023 - val_loss: 0.5487 - val_accuracy: 0.7777\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1997 - accuracy: 0.9164 - val_loss: 0.6055 - val_accuracy: 0.7805\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1495 - accuracy: 0.9440 - val_loss: 0.6145 - val_accuracy: 0.7777\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.1328 - accuracy: 0.9498 - val_loss: 0.7079 - val_accuracy: 0.7842\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.0894 - accuracy: 0.9688 - val_loss: 0.7677 - val_accuracy: 0.7908\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 79.0806770324707\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 37s 101ms/step - loss: 0.5784 - accuracy: 0.6918 - val_loss: 0.4672 - val_accuracy: 0.7627\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.4455 - accuracy: 0.7889 - val_loss: 0.4535 - val_accuracy: 0.7730\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.4159 - accuracy: 0.8009 - val_loss: 0.4496 - val_accuracy: 0.7767\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 27s 92ms/step - loss: 0.3964 - accuracy: 0.8125 - val_loss: 0.4604 - val_accuracy: 0.7767\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.3655 - accuracy: 0.8343 - val_loss: 0.4545 - val_accuracy: 0.7739\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.3393 - accuracy: 0.8492 - val_loss: 0.4732 - val_accuracy: 0.7777\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 30s 99ms/step - loss: 0.3093 - accuracy: 0.8636 - val_loss: 0.4694 - val_accuracy: 0.7730\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2739 - accuracy: 0.8876 - val_loss: 0.4930 - val_accuracy: 0.7795\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.2199 - accuracy: 0.9114 - val_loss: 0.5135 - val_accuracy: 0.7702\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1703 - accuracy: 0.9379 - val_loss: 0.5718 - val_accuracy: 0.7711\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1542 - accuracy: 0.9424 - val_loss: 0.6305 - val_accuracy: 0.7711\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.1155 - accuracy: 0.9571 - val_loss: 0.7015 - val_accuracy: 0.7739\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.0862 - accuracy: 0.9709 - val_loss: 0.7623 - val_accuracy: 0.7711\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.0775 - accuracy: 0.9736 - val_loss: 0.8151 - val_accuracy: 0.7645\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 28s 92ms/step - loss: 0.0606 - accuracy: 0.9814 - val_loss: 0.8918 - val_accuracy: 0.7523\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 27s 91ms/step - loss: 0.0421 - accuracy: 0.9893 - val_loss: 0.9163 - val_accuracy: 0.7711\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Test Accuracy: 77.95497179031372\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 37s 101ms/step - loss: 0.5826 - accuracy: 0.6809 - val_loss: 0.4679 - val_accuracy: 0.7683\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.4511 - accuracy: 0.7852 - val_loss: 0.4465 - val_accuracy: 0.7814\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.4255 - accuracy: 0.7912 - val_loss: 0.4405 - val_accuracy: 0.7871\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.3981 - accuracy: 0.8079 - val_loss: 0.4423 - val_accuracy: 0.7889\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3654 - accuracy: 0.8327 - val_loss: 0.4376 - val_accuracy: 0.7936\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.3415 - accuracy: 0.8497 - val_loss: 0.4835 - val_accuracy: 0.7871\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.3026 - accuracy: 0.8709 - val_loss: 0.4646 - val_accuracy: 0.8096\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2664 - accuracy: 0.8852 - val_loss: 0.5193 - val_accuracy: 0.7852\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2228 - accuracy: 0.9072 - val_loss: 0.5481 - val_accuracy: 0.7936\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.2030 - accuracy: 0.9177 - val_loss: 0.5631 - val_accuracy: 0.7899\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1578 - accuracy: 0.9401 - val_loss: 0.6240 - val_accuracy: 0.7824\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1273 - accuracy: 0.9537 - val_loss: 0.6609 - val_accuracy: 0.7749\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.1048 - accuracy: 0.9630 - val_loss: 0.7448 - val_accuracy: 0.7842\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 28s 93ms/step - loss: 0.0743 - accuracy: 0.9763 - val_loss: 0.7108 - val_accuracy: 0.7674\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 28s 94ms/step - loss: 0.0683 - accuracy: 0.9776 - val_loss: 0.8852 - val_accuracy: 0.7664\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00015: early stopping\n",
            "Test Accuracy: 80.9568464756012\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
            "0  81.443298  77.788192  78.799248  ...  77.954972  80.956846  79.553542\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArIs_PwEj3XD"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "LnFVv_f1j3XE",
        "outputId": "383eaea0-90ee-4019-f5ed-89f19c74f5dd"
      },
      "source": [
        "record2"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.443298</td>\n",
              "      <td>77.788192</td>\n",
              "      <td>78.799248</td>\n",
              "      <td>80.112571</td>\n",
              "      <td>80.206376</td>\n",
              "      <td>78.424013</td>\n",
              "      <td>80.769229</td>\n",
              "      <td>79.080677</td>\n",
              "      <td>77.954972</td>\n",
              "      <td>80.956846</td>\n",
              "      <td>79.553542</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
              "0  81.443298  77.788192  78.799248  ...  77.954972  80.956846  79.553542\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKH2GwRcj3XF"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('LSTM_MR_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP7vjT17j3XF"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zefGpZ7j3XG"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4W-HU5j3XG"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc6HK4BXj3XH"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6QH236Ij3XI",
        "outputId": "2c0cac29-25d5-4734-c3ba-5b01abf80136"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 487,009\n",
            "Trainable params: 487,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJP_IsZj3XK"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szh8LyrJj3XM"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k12QxZn5j3XN",
        "outputId": "19cf20a9-0123-4f6d-c711-e952e7942a7e"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 53s 152ms/step - loss: 0.5731 - accuracy: 0.6913 - val_loss: 0.4129 - val_accuracy: 0.8051\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.2583 - accuracy: 0.8919 - val_loss: 0.4606 - val_accuracy: 0.8022\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.1051 - accuracy: 0.9634 - val_loss: 0.6187 - val_accuracy: 0.7882\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0455 - accuracy: 0.9846 - val_loss: 0.8005 - val_accuracy: 0.7919\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0176 - accuracy: 0.9949 - val_loss: 0.9069 - val_accuracy: 0.7713\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 1.1153 - val_accuracy: 0.7769\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 45s 148ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 1.2528 - val_accuracy: 0.7723\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 1.1542 - val_accuracy: 0.7769\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 45s 148ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 1.3210 - val_accuracy: 0.7788\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.50609230995178\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 55s 151ms/step - loss: 0.5648 - accuracy: 0.6873 - val_loss: 0.4152 - val_accuracy: 0.8004\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.2527 - accuracy: 0.8966 - val_loss: 0.5025 - val_accuracy: 0.7788\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.1126 - accuracy: 0.9599 - val_loss: 0.5977 - val_accuracy: 0.7919\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.0418 - accuracy: 0.9884 - val_loss: 0.8536 - val_accuracy: 0.7919\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 1.1510 - val_accuracy: 0.7666\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 1.1205 - val_accuracy: 0.7751\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 1.2394 - val_accuracy: 0.7713\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 43s 143ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 1.2322 - val_accuracy: 0.7657\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 1.5233 - val_accuracy: 0.7685\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.03748655319214\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 53s 154ms/step - loss: 0.5627 - accuracy: 0.6949 - val_loss: 0.4481 - val_accuracy: 0.7758\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.2632 - accuracy: 0.8958 - val_loss: 0.5019 - val_accuracy: 0.7674\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.1025 - accuracy: 0.9669 - val_loss: 0.6264 - val_accuracy: 0.7608\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0416 - accuracy: 0.9860 - val_loss: 0.9692 - val_accuracy: 0.7542\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.9852 - val_accuracy: 0.7636\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 1.2382 - val_accuracy: 0.7495\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 1.4791 - val_accuracy: 0.7373\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 1.3378 - val_accuracy: 0.7448\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 1.5047 - val_accuracy: 0.7430\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 77.57973670959473\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 55s 162ms/step - loss: 0.5739 - accuracy: 0.6839 - val_loss: 0.4155 - val_accuracy: 0.8133\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.2590 - accuracy: 0.8976 - val_loss: 0.4369 - val_accuracy: 0.8011\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.1160 - accuracy: 0.9616 - val_loss: 0.5961 - val_accuracy: 0.7777\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.0499 - accuracy: 0.9841 - val_loss: 0.8547 - val_accuracy: 0.7842\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.0167 - accuracy: 0.9960 - val_loss: 1.0261 - val_accuracy: 0.7786\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 1.3920 - val_accuracy: 0.7739\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 1.4144 - val_accuracy: 0.7711\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 1.3437 - val_accuracy: 0.7636\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 46s 153ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 1.2201 - val_accuracy: 0.7749\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 81.33208155632019\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 54s 157ms/step - loss: 0.5685 - accuracy: 0.6889 - val_loss: 0.4492 - val_accuracy: 0.8002\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 45s 150ms/step - loss: 0.2546 - accuracy: 0.8965 - val_loss: 0.4677 - val_accuracy: 0.7946\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 45s 152ms/step - loss: 0.1087 - accuracy: 0.9630 - val_loss: 0.5802 - val_accuracy: 0.7917\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0448 - accuracy: 0.9878 - val_loss: 0.8259 - val_accuracy: 0.7655\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.9633 - val_accuracy: 0.7749\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 1.0789 - val_accuracy: 0.7842\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 1.3902 - val_accuracy: 0.7842\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 8.1347e-04 - accuracy: 0.9999 - val_loss: 1.4189 - val_accuracy: 0.7758\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0090 - accuracy: 0.9968 - val_loss: 1.0586 - val_accuracy: 0.7880\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 80.01875877380371\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 54s 158ms/step - loss: 0.5744 - accuracy: 0.6868 - val_loss: 0.4363 - val_accuracy: 0.7936\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 45s 150ms/step - loss: 0.2510 - accuracy: 0.8993 - val_loss: 0.4946 - val_accuracy: 0.7824\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 45s 150ms/step - loss: 0.1011 - accuracy: 0.9637 - val_loss: 0.5796 - val_accuracy: 0.7889\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 45s 150ms/step - loss: 0.0450 - accuracy: 0.9864 - val_loss: 0.9122 - val_accuracy: 0.7702\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0156 - accuracy: 0.9961 - val_loss: 0.9305 - val_accuracy: 0.7795\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 1.1157 - val_accuracy: 0.7711\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 45s 150ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 1.4483 - val_accuracy: 0.7580\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 45s 150ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 1.2402 - val_accuracy: 0.7523\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0077 - accuracy: 0.9984 - val_loss: 1.2703 - val_accuracy: 0.7542\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.36210036277771\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 55s 153ms/step - loss: 0.5740 - accuracy: 0.6817 - val_loss: 0.4323 - val_accuracy: 0.7964\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.2659 - accuracy: 0.8904 - val_loss: 0.4801 - val_accuracy: 0.7889\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 44s 145ms/step - loss: 0.1084 - accuracy: 0.9569 - val_loss: 0.6562 - val_accuracy: 0.7580\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0409 - accuracy: 0.9883 - val_loss: 0.8701 - val_accuracy: 0.7608\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.0199 - accuracy: 0.9941 - val_loss: 1.0035 - val_accuracy: 0.7664\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 1.1928 - val_accuracy: 0.7570\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 1.4047 - val_accuracy: 0.7617\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 1.3295 - val_accuracy: 0.7533\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 1.5013 - val_accuracy: 0.7645\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.6435296535492\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 53s 153ms/step - loss: 0.5624 - accuracy: 0.7042 - val_loss: 0.4335 - val_accuracy: 0.7946\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.2686 - accuracy: 0.8911 - val_loss: 0.4083 - val_accuracy: 0.8068\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.1153 - accuracy: 0.9600 - val_loss: 0.5238 - val_accuracy: 0.7917\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 44s 145ms/step - loss: 0.0494 - accuracy: 0.9853 - val_loss: 0.7270 - val_accuracy: 0.7899\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 43s 145ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 0.8748 - val_accuracy: 0.7908\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 43s 144ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.8308 - val_accuracy: 0.7824\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0145 - accuracy: 0.9971 - val_loss: 1.0923 - val_accuracy: 0.7824\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 45s 148ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 1.1430 - val_accuracy: 0.7749\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 1.2053 - val_accuracy: 0.7786\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4740 - val_accuracy: 0.7749\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00010: early stopping\n",
            "Test Accuracy: 80.67542314529419\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 53s 155ms/step - loss: 0.5543 - accuracy: 0.7057 - val_loss: 0.4989 - val_accuracy: 0.7899\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.2517 - accuracy: 0.8974 - val_loss: 0.5497 - val_accuracy: 0.7655\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.1093 - accuracy: 0.9603 - val_loss: 0.6978 - val_accuracy: 0.7795\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 44s 146ms/step - loss: 0.0472 - accuracy: 0.9851 - val_loss: 0.8877 - val_accuracy: 0.7758\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 1.1183 - val_accuracy: 0.7645\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 1.2068 - val_accuracy: 0.7664\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 1.2627 - val_accuracy: 0.7580\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 1.3988 - val_accuracy: 0.7552\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 1.6969 - val_accuracy: 0.7608\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 78.98686528205872\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 53s 154ms/step - loss: 0.5640 - accuracy: 0.6979 - val_loss: 0.4253 - val_accuracy: 0.7964\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.2592 - accuracy: 0.8950 - val_loss: 0.4741 - val_accuracy: 0.7899\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 45s 149ms/step - loss: 0.1076 - accuracy: 0.9636 - val_loss: 0.6873 - val_accuracy: 0.7824\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 45s 148ms/step - loss: 0.0435 - accuracy: 0.9869 - val_loss: 0.8063 - val_accuracy: 0.7749\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.9756 - val_accuracy: 0.7711\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 1.2207 - val_accuracy: 0.7683\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 44s 147ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 1.2918 - val_accuracy: 0.7730\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 1.3785 - val_accuracy: 0.7617\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 44s 148ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 1.5258 - val_accuracy: 0.7674\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00009: early stopping\n",
            "Test Accuracy: 79.6435296535492\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c58a57bfe936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mrecord3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'record' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2zsyWeXj3XP"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "SBIcIG8Pj3XP",
        "outputId": "194cd6af-26b5-428c-c301-8a00500709e5"
      },
      "source": [
        "record3"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.506092</td>\n",
              "      <td>80.037487</td>\n",
              "      <td>77.579737</td>\n",
              "      <td>81.332082</td>\n",
              "      <td>80.018759</td>\n",
              "      <td>79.3621</td>\n",
              "      <td>79.64353</td>\n",
              "      <td>80.675423</td>\n",
              "      <td>78.986865</td>\n",
              "      <td>79.64353</td>\n",
              "      <td>79.77856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9     acc10       AVG\n",
              "0  80.506092  80.037487  77.579737  ...  78.986865  79.64353  79.77856\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYEttODj3XQ"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('LSTM_MR_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}