{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with CR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3775, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weaknesses are minor the feel and layout of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many of our disney movies do n 't play on this...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player has a problem with dual layer dvd 's su...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know the saying is you get what you pay for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will never purchase apex again .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>so far , the anti spam feature seems to be ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>i did not have any of the installation problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>their products have been great and have saved ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     weaknesses are minor the feel and layout of th...      0  train\n",
       "1     many of our disney movies do n 't play on this...      0  train\n",
       "2     player has a problem with dual layer dvd 's su...      0  train\n",
       "3     i know the saying is you get what you pay for ...      0  train\n",
       "4                      will never purchase apex again .      0  train\n",
       "...                                                 ...    ...    ...\n",
       "3770  so far , the anti spam feature seems to be ver...      1  train\n",
       "3771  i downloaded a trial version of computer assoc...      1  train\n",
       "3772  i did not have any of the installation problem...      1  train\n",
       "3773  their products have been great and have saved ...      1  train\n",
       "3774                                                         1  train\n",
       "\n",
       "[3775 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MPQA/MPQA.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3775 entries, 0 to 3774\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3775 non-null   object\n",
      " 1   label     3775 non-null   int32 \n",
      " 2   split     3775 non-null   object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 73.9+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2407</td>\n",
       "      <td>2407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          1368   1368\n",
       "1          2407   2407"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weaknesses are minor the feel and layout of the remote control are only so so . it does n 't show the complete file names of mp3s with really long names . you must cycle through every zoom setting ( 2x , 3x , 4x , 1 2x , etc . ) before getting back to normal size sorry if i 'm just ignorant of a way to get back to 1x quickly .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  will never purchase apex again .\n",
      "Into a sequence of int: [72, 194, 285, 207, 286]\n",
      "Into a padded sequence: [ 72 194 285 207 286   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "and 3\n",
      "i 4\n",
      "it 5\n",
      "to 6\n",
      "a 7\n",
      "is 8\n",
      "of 9\n",
      "this 10\n",
      "5336\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 659,553\n",
      "Trainable params: 659,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 45s 307ms/step - loss: 0.6394 - accuracy: 0.6517 - val_loss: 0.4827 - val_accuracy: 0.7672\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 29s 273ms/step - loss: 0.3393 - accuracy: 0.8736 - val_loss: 0.4871 - val_accuracy: 0.7937\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 34s 323ms/step - loss: 0.1819 - accuracy: 0.9363 - val_loss: 0.5559 - val_accuracy: 0.7804\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 30s 286ms/step - loss: 0.1078 - accuracy: 0.9634 - val_loss: 0.8102 - val_accuracy: 0.7910\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 25s 237ms/step - loss: 0.0642 - accuracy: 0.9809 - val_loss: 0.7883 - val_accuracy: 0.7566\n",
      "Epoch 6/15\n",
      " 36/107 [=========>....................] - ETA: 13s - loss: 0.0424 - accuracy: 0.9857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-aa71438f7e7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n\u001b[1;32m---> 62\u001b[1;33m               callbacks=[callbacks], validation_data=(Xtest, test_y))\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>Filters</th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>77.513230</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>81.962866</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>80.371563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>83.023870</td>\n",
       "      <td>78.779840</td>\n",
       "      <td>80.371350</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>80.318091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>83.023870</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>77.718830</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>80.317881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>6</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>76.190478</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>78.779840</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>83.023870</td>\n",
       "      <td>80.211711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>77.777779</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>81.746030</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>80.052279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>6</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>75.661373</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>84.350133</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>80.026945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>77.513230</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>82.228118</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>79.761765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>4</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>84.391534</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>78.835976</td>\n",
       "      <td>77.718830</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>77.718830</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>79.707520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "      <td>77.513230</td>\n",
       "      <td>77.777779</td>\n",
       "      <td>78.571427</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>82.010579</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>77.453583</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.549844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>78.571427</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>78.571427</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>79.469777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>82.010579</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>76.984125</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>81.746030</td>\n",
       "      <td>76.127321</td>\n",
       "      <td>80.371350</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>74.801064</td>\n",
       "      <td>79.363114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>2</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>76.984125</td>\n",
       "      <td>76.984125</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>76.719576</td>\n",
       "      <td>77.453583</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>81.962866</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>79.259610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
       "1        relu       2  77.513230  81.481481  79.629630  80.423278  80.158728   \n",
       "4        relu       5  80.158728  78.306878  81.481481  78.306878  82.539684   \n",
       "3        relu       4  80.158728  79.365081  80.158728  81.216931  80.687833   \n",
       "5        relu       6  80.687833  81.216931  76.190478  80.158728  83.597887   \n",
       "8        tanh       3  83.597887  77.777779  79.365081  80.423278  81.746030   \n",
       "11       tanh       6  80.423278  80.423278  75.661373  80.423278  81.481481   \n",
       "0        relu       1  79.629630  78.306878  77.513230  82.539684  80.158728   \n",
       "9        tanh       4  79.629630  84.391534  78.306878  81.481481  78.835976   \n",
       "6        tanh       1  77.513230  77.777779  78.571427  81.216931  82.010579   \n",
       "10       tanh       5  82.539684  78.571427  79.100531  78.571427  80.158728   \n",
       "2        relu       3  82.010579  79.100531  76.984125  81.216931  81.746030   \n",
       "7        tanh       2  81.481481  76.984125  76.984125  79.100531  76.719576   \n",
       "\n",
       "         acc6       acc7       acc8       acc9      acc10        AVG  \n",
       "1   80.106103  79.310346  81.697613  81.962866  81.432360  80.371563  \n",
       "4   83.023870  78.779840  80.371350  81.167108  79.045093  80.318091  \n",
       "3   79.045093  83.023870  80.901855  77.718830  80.901855  80.317881  \n",
       "5   78.779840  81.432360  77.188331  79.840851  83.023870  80.211711  \n",
       "8   77.984083  79.045093  79.575598  81.167108  79.840851  80.052279  \n",
       "11  76.657826  84.350133  80.106103  79.045093  81.697613  80.026945  \n",
       "0   78.249335  82.228118  79.840851  80.106103  79.045093  79.761765  \n",
       "9   77.718830  79.840851  77.718830  81.167108  77.984083  79.707520  \n",
       "6   78.514588  77.453583  81.697613  81.432360  79.310346  79.549844  \n",
       "10  79.840851  79.045093  80.106103  78.249335  78.514588  79.469777  \n",
       "2   76.127321  80.371350  80.636603  80.636603  74.801064  79.363114  \n",
       "7   77.453583  79.840851  81.962866  80.636603  81.432360  79.259610  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>80.371563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanh</th>\n",
       "      <td>80.052279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AVG\n",
       "Activation           \n",
       "relu        80.371563\n",
       "tanh        80.052279"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record[['Activation', 'AVG']].groupby(by='Activation').max().sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record.sort_values(by='AVG', ascending=False)\n",
    "report = report.to_excel('CNN_CR.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5046 words present from 5336 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.7603335 ,  0.41298582,  1.6051669 , ...,  0.07348683,\n",
       "        -0.93163275, -0.64774868],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1437\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1444 (Embedding)   (None, 100, 300)          1524600   \n",
      "_________________________________________________________________\n",
      "conv1d_1439 (Conv1D)         (None, 98, 100)           90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1439 (MaxPooli (None, 49, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1439 (Flatten)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2871 (Dropout)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_2869 (Dense)           (None, 10)                49010     \n",
      "_________________________________________________________________\n",
      "dropout_2872 (Dropout)       (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2870 (Dense)           (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,663,721\n",
      "Trainable params: 139,121\n",
      "Non-trainable params: 1,524,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(vocab_size, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 74.0740716457367\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 73.01587462425232\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 74.60317611694336\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 82.80423283576965\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 82.49337077140808\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 74.27055835723877\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 77.77777910232544\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 73.28042387962341\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Test Accuracy: 73.28042387962341\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 75.59681534767151\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 77.18833088874817\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 73.209547996521\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 82.49337077140808\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 75.86206793785095\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 76.1904776096344\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 73.8095223903656\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 76.4550268650055\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 73.01587462425232\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 74.86772537231445\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 73.209547996521\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 76.92307829856873\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.18833088874817\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 78.77984046936035\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.39682388305664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 79.89417910575867\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 74.60317611694336\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 72.75132536888123\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Test Accuracy: 73.8095223903656\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 78.77984046936035\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 73.74005317687988\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 72.22222089767456\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 71.16402387619019\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 76.98412537574768\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 72.67904281616211\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.45358347892761\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 74.86772537231445\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 77.77777910232544\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 66.1375641822815\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 66.1375641822815\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 75.39682388305664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 61.00795865058899\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 76.92307829856873\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.59681534767151\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "Test Accuracy: 69.49602365493774\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 63.49206566810608\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 63.22751045227051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 63.22751045227051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 70.82228064537048\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 68.43501329421997\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 77.71883010864258\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 62.59946823120117\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
      "6       relu       7  79.365081  63.492066  63.227510  77.248675  63.227510   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
      "6  70.822281  68.435013  77.718830  62.599468  75.066310  70.120274  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 76.1904776096344\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 71.69312238693237\n",
      "Test Accuracy: 72.48677015304565\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 65.6084656715393\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 70.37037014961243\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 70.82228064537048\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 72.41379022598267\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 63.92573118209839\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.98408269882202\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
      "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
      "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
      "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
      "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
      "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
      "6       relu       7  79.365081  63.492066  63.227510  77.248675  63.227510   \n",
      "7       relu       8  76.190478  71.693122  72.486770  65.608466  70.370370   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
      "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
      "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
      "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
      "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
      "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
      "6  70.822281  68.435013  77.718830  62.599468  75.066310  70.120274  \n",
      "7  75.066310  70.822281  72.413790  63.925731  77.984083  71.656140  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>Filters</th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>74.074072</td>\n",
       "      <td>73.015875</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>74.603176</td>\n",
       "      <td>82.804233</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>82.493371</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>74.270558</td>\n",
       "      <td>77.670414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>77.777779</td>\n",
       "      <td>73.280424</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>73.280424</td>\n",
       "      <td>75.596815</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>73.209548</td>\n",
       "      <td>82.493371</td>\n",
       "      <td>75.862068</td>\n",
       "      <td>77.165172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>71.164024</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>76.984125</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>72.679043</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>77.453583</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>76.450114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>75.396824</td>\n",
       "      <td>79.894179</td>\n",
       "      <td>74.603176</td>\n",
       "      <td>72.751325</td>\n",
       "      <td>73.809522</td>\n",
       "      <td>78.779840</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>73.740053</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>75.735688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>76.190478</td>\n",
       "      <td>73.809522</td>\n",
       "      <td>76.455027</td>\n",
       "      <td>73.015875</td>\n",
       "      <td>74.867725</td>\n",
       "      <td>73.209548</td>\n",
       "      <td>76.923078</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>78.779840</td>\n",
       "      <td>75.577099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>6</td>\n",
       "      <td>74.867725</td>\n",
       "      <td>77.777779</td>\n",
       "      <td>66.137564</td>\n",
       "      <td>66.137564</td>\n",
       "      <td>75.396824</td>\n",
       "      <td>61.007959</td>\n",
       "      <td>76.923078</td>\n",
       "      <td>75.596815</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>69.496024</td>\n",
       "      <td>72.159067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>8</td>\n",
       "      <td>76.190478</td>\n",
       "      <td>71.693122</td>\n",
       "      <td>72.486770</td>\n",
       "      <td>65.608466</td>\n",
       "      <td>70.370370</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>70.822281</td>\n",
       "      <td>72.413790</td>\n",
       "      <td>63.925731</td>\n",
       "      <td>77.984083</td>\n",
       "      <td>71.656140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>7</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>63.492066</td>\n",
       "      <td>63.227510</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>63.227510</td>\n",
       "      <td>70.822281</td>\n",
       "      <td>68.435013</td>\n",
       "      <td>77.718830</td>\n",
       "      <td>62.599468</td>\n",
       "      <td>75.066310</td>\n",
       "      <td>70.120274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
       "0       relu       1  74.074072  73.015875  78.042328  74.603176  82.804233   \n",
       "1       relu       2  81.481481  77.777779  73.280424  81.481481  73.280424   \n",
       "4       relu       5  72.222221  71.164024  81.481481  76.984125  81.216931   \n",
       "3       relu       4  75.396824  79.894179  74.603176  72.751325  73.809522   \n",
       "2       relu       3  76.190478  73.809522  76.455027  73.015875  74.867725   \n",
       "5       relu       6  74.867725  77.777779  66.137564  66.137564  75.396824   \n",
       "7       relu       8  76.190478  71.693122  72.486770  65.608466  70.370370   \n",
       "6       relu       7  79.365081  63.492066  63.227510  77.248675  63.227510   \n",
       "\n",
       "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
       "0  80.901855  82.493371  77.984083  78.514588  74.270558  77.670414  \n",
       "1  75.596815  77.188331  73.209548  82.493371  75.862068  77.165172  \n",
       "4  72.679043  77.984083  75.331563  77.453583  77.984083  76.450114  \n",
       "3  78.779840  78.249335  75.066310  73.740053  75.066310  75.735688  \n",
       "2  73.209548  76.923078  77.188331  75.331563  78.779840  75.577099  \n",
       "5  61.007959  76.923078  75.596815  78.249335  69.496024  72.159067  \n",
       "7  75.066310  70.822281  72.413790  63.925731  77.984083  71.656140  \n",
       "6  70.822281  68.435013  77.718830  62.599468  75.066310  70.120274  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2.sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>77.670414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AVG\n",
       "Activation           \n",
       "relu        77.670414"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2[['Activation', 'AVG']].groupby(by='Activation').max().sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2.sort_values(by='AVG', ascending=False)\n",
    "report = report.to_excel('CNN_CR_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1518\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1525 (Embedding)   (None, 100, 300)          1527300   \n",
      "_________________________________________________________________\n",
      "conv1d_1520 (Conv1D)         (None, 98, 100)           90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1520 (MaxPooli (None, 49, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1520 (Flatten)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3033 (Dropout)       (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_3031 (Dense)           (None, 10)                49010     \n",
      "_________________________________________________________________\n",
      "dropout_3034 (Dropout)       (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3032 (Dense)           (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,666,421\n",
      "Trainable params: 1,666,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(vocab_size, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.42327761650085\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.51322984695435\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 83.59788656234741\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 74.80106353759766\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 79.84085083007812\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3      acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.51323  83.597887   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 81.7460298538208\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Test Accuracy: 79.10053133964539\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 82.22811818122864\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 76.71957612037659\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 81.7460298538208\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 80.95238208770752\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 76.71957612037659\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.10610342025757\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 77.18833088874817\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.95238208770752\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 79.0450930595398\n",
      "Test Accuracy: 79.0450930595398\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.16710782051086\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 64.55026268959045\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 78.30687761306763\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 76.65782570838928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Test Accuracy: 80.63660264015198\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 82.75862336158752\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.24867463111877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 78.30687761306763\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 78.83597612380981\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.37135004997253\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 83.02386999130249\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 81.9628655910492\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
      "\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.68783283233643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 79.10053133964539\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 75.33156275749207\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 74.27055835723877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 79.31034564971924\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 79.57559823989868\n",
      "Test Accuracy: 76.1273205280304\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
      "6       relu       7  80.158728  80.158728  80.687833  79.100531  80.158728   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
      "6  75.331563  74.270558  79.310346  79.575598  76.127321  78.487993  \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 76.4550268650055\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 82.27513432502747\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.57142686843872\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 78.24933528900146\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 76.65782570838928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 79.0450930595398\n",
      "\n",
      "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
      "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
      "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
      "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
      "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
      "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
      "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
      "6       relu       7  80.158728  80.158728  80.687833  79.100531  80.158728   \n",
      "7       relu       8  78.042328  76.455027  82.275134  78.571427  78.042328   \n",
      "\n",
      "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
      "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
      "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
      "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
      "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
      "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  \n",
      "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
      "6  75.331563  74.270558  79.310346  79.575598  76.127321  78.487993  \n",
      "7  78.249335  78.249335  81.432360  76.657826  79.045093  78.702019  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>Filters</th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>81.746030</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>82.228118</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>80.715760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>6</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>78.835976</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>80.371350</td>\n",
       "      <td>83.023870</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>81.962866</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.239918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>77.248675</td>\n",
       "      <td>80.952382</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>79.920706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>77.513230</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>74.801064</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>79.627525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>76.719576</td>\n",
       "      <td>81.746030</td>\n",
       "      <td>80.952382</td>\n",
       "      <td>76.719576</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>80.106103</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>77.188331</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>79.496723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>8</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>76.455027</td>\n",
       "      <td>82.275134</td>\n",
       "      <td>78.571427</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>78.249335</td>\n",
       "      <td>81.432360</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>78.702019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>7</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>80.687833</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>74.270558</td>\n",
       "      <td>79.310346</td>\n",
       "      <td>79.575598</td>\n",
       "      <td>76.127321</td>\n",
       "      <td>78.487993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>64.550263</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>78.306878</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>75.331563</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>82.758623</td>\n",
       "      <td>78.041345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Filters       acc1       acc2       acc3       acc4       acc5  \\\n",
       "1       relu       2  80.687833  81.746030  79.365081  79.100531  80.687833   \n",
       "5       relu       6  77.248675  79.629630  78.306878  78.835976  81.481481   \n",
       "3       relu       4  80.687833  80.687833  79.365081  77.248675  80.952382   \n",
       "0       relu       1  80.158728  82.539684  80.423278  77.513230  83.597887   \n",
       "2       relu       3  81.216931  76.719576  81.746030  80.952382  76.719576   \n",
       "7       relu       8  78.042328  76.455027  82.275134  78.571427  78.042328   \n",
       "6       relu       7  80.158728  80.158728  80.687833  79.100531  80.158728   \n",
       "4       relu       5  64.550263  81.481481  80.158728  78.306878  79.629630   \n",
       "\n",
       "        acc6       acc7       acc8       acc9      acc10        AVG  \n",
       "1  82.228118  80.901855  81.432360  80.901855  80.106103  80.715760  \n",
       "5  80.371350  83.023870  80.901855  81.962866  80.636603  80.239918  \n",
       "3  79.575598  79.045093  79.045093  81.432360  81.167108  79.920706  \n",
       "0  79.310346  79.575598  78.514588  74.801064  79.840851  79.627525  \n",
       "2  81.432360  80.106103  78.249335  77.188331  80.636603  79.496723  \n",
       "7  78.249335  78.249335  81.432360  76.657826  79.045093  78.702019  \n",
       "6  75.331563  74.270558  79.310346  79.575598  76.127321  78.487993  \n",
       "4  76.657826  75.331563  80.636603  80.901855  82.758623  78.041345  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3.sort_values(by='AVG', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3.sort_values(by='AVG', ascending=False)\n",
    "report = report.to_excel('CNN_CR_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
