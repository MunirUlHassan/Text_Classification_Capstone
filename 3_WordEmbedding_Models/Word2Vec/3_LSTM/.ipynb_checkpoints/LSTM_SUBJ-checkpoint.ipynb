{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with SUBJ Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smart and alert , thirteen conversations about...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>color , musical bounce and warm seas lapping o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is not a mass market entertainment but an u...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a light hearted french film about the spiritua...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my wife is an actress has its moments in looki...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>in the end , they discover that balance in lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>enter the beautiful and mysterious secret agen...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>after listening to a missionary from china spe...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>looking for a short cut to fame , glass concoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     smart and alert , thirteen conversations about...      0  train\n",
       "1     color , musical bounce and warm seas lapping o...      0  train\n",
       "2     it is not a mass market entertainment but an u...      0  train\n",
       "3     a light hearted french film about the spiritua...      0  train\n",
       "4     my wife is an actress has its moments in looki...      0  train\n",
       "...                                                 ...    ...    ...\n",
       "9995  in the end , they discover that balance in lif...      1  train\n",
       "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
       "9997  enter the beautiful and mysterious secret agen...      1  train\n",
       "9998  after listening to a missionary from china spe...      1  train\n",
       "9999  looking for a short cut to fame , glass concoc...      1  train\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/SUBJ/SUBJ.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10000 non-null  object\n",
      " 1   label     10000 non-null  int32 \n",
      " 2   split     10000 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          5000   5000\n",
       "1          5000   5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smart and alert , thirteen conversations about one thing is a small gem .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
      "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
      "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
      "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
      "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "a 3\n",
      "and 4\n",
      "of 5\n",
      "to 6\n",
      "in 7\n",
      "is 8\n",
      "'s 9\n",
      "it 10\n",
      "21324\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 84s 257ms/step - loss: 0.4351 - accuracy: 0.7757 - val_loss: 0.2216 - val_accuracy: 0.9150\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 70s 248ms/step - loss: 0.0813 - accuracy: 0.9736 - val_loss: 0.2411 - val_accuracy: 0.9010\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 74s 264ms/step - loss: 0.0211 - accuracy: 0.9952 - val_loss: 0.3469 - val_accuracy: 0.9070\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 71s 253ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.4108 - val_accuracy: 0.8900\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 75s 267ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.4150 - val_accuracy: 0.9140\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 79s 279ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.4509 - val_accuracy: 0.9040\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.50000214576721\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 125s 401ms/step - loss: 0.4314 - accuracy: 0.7902 - val_loss: 0.2182 - val_accuracy: 0.9160\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 89s 314ms/step - loss: 0.0817 - accuracy: 0.9765 - val_loss: 0.2644 - val_accuracy: 0.9020\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 110s 390ms/step - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.3410 - val_accuracy: 0.8910\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 104s 370ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.4430 - val_accuracy: 0.8890\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 101s 360ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.5179 - val_accuracy: 0.8910\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 105s 372ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.3471 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.60000085830688\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 114s 361ms/step - loss: 0.4342 - accuracy: 0.7942 - val_loss: 0.2109 - val_accuracy: 0.9220\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 86s 305ms/step - loss: 0.0702 - accuracy: 0.9753 - val_loss: 0.2407 - val_accuracy: 0.9210\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 96s 341ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.3362 - val_accuracy: 0.9030\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 101s 360ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.3528 - val_accuracy: 0.9140\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 105s 373ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.3192 - val_accuracy: 0.9050\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 106s 377ms/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.3718 - val_accuracy: 0.9080\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.1999990940094\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 165s 547ms/step - loss: 0.4353 - accuracy: 0.7873 - val_loss: 0.2183 - val_accuracy: 0.9060\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 136s 482ms/step - loss: 0.0787 - accuracy: 0.9751 - val_loss: 0.2968 - val_accuracy: 0.8960\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 151s 537ms/step - loss: 0.0159 - accuracy: 0.9966 - val_loss: 0.3515 - val_accuracy: 0.8950\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 149s 527ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.3500 - val_accuracy: 0.8840\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 144s 510ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.4359 - val_accuracy: 0.8880\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 143s 509ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.5749 - val_accuracy: 0.8860\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 90.6000018119812\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 119s 385ms/step - loss: 0.4343 - accuracy: 0.7841 - val_loss: 0.1955 - val_accuracy: 0.9290\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 165s 586ms/step - loss: 0.0716 - accuracy: 0.9769 - val_loss: 0.2045 - val_accuracy: 0.9280\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 160s 567ms/step - loss: 0.0202 - accuracy: 0.9952 - val_loss: 0.3264 - val_accuracy: 0.9120\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 160s 567ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.2563 - val_accuracy: 0.9020\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 157s 557ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.3456 - val_accuracy: 0.9110\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 163s 577ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.3827 - val_accuracy: 0.9160\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.90000200271606\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 132s 429ms/step - loss: 0.4296 - accuracy: 0.8008 - val_loss: 0.2507 - val_accuracy: 0.9040\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.0673 - accuracy: 0.9820 - val_loss: 0.3018 - val_accuracy: 0.8980\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 90s 319ms/step - loss: 0.0257 - accuracy: 0.9938 - val_loss: 0.4001 - val_accuracy: 0.8940\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 90s 318ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.4873 - val_accuracy: 0.8850\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 90s 318ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.4236 - val_accuracy: 0.8940\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 90s 319ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5670 - val_accuracy: 0.8910\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 90.39999842643738\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 145s 475ms/step - loss: 0.4437 - accuracy: 0.7881 - val_loss: 0.2203 - val_accuracy: 0.9100\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 111s 392ms/step - loss: 0.0771 - accuracy: 0.9726 - val_loss: 0.2393 - val_accuracy: 0.9040\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 117s 414ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.3564 - val_accuracy: 0.9000\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 110s 390ms/step - loss: 0.0130 - accuracy: 0.9970 - val_loss: 0.3713 - val_accuracy: 0.9070\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 112s 398ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.4522 - val_accuracy: 0.8980\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 113s 400ms/step - loss: 6.4205e-04 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.8930\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.00000262260437\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 74s 230ms/step - loss: 0.4274 - accuracy: 0.7937 - val_loss: 0.1909 - val_accuracy: 0.9350\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 172s 611ms/step - loss: 0.0787 - accuracy: 0.9767 - val_loss: 0.2146 - val_accuracy: 0.9250\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 174s 616ms/step - loss: 0.0247 - accuracy: 0.9924 - val_loss: 0.2331 - val_accuracy: 0.9310\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 173s 614ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.2835 - val_accuracy: 0.9210\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 177s 628ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2969 - val_accuracy: 0.9220\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 177s 628ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.2769 - val_accuracy: 0.9170\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 93.50000023841858\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 178s 593ms/step - loss: 0.4390 - accuracy: 0.7900 - val_loss: 0.2229 - val_accuracy: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "282/282 [==============================] - 139s 491ms/step - loss: 0.0793 - accuracy: 0.9754 - val_loss: 0.2832 - val_accuracy: 0.9080\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 134s 475ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.3074 - val_accuracy: 0.9020\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 132s 467ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.3414 - val_accuracy: 0.9070\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 130s 463ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.4280 - val_accuracy: 0.8930\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 130s 462ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.4531 - val_accuracy: 0.8860\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 113s 365ms/step - loss: 0.4532 - accuracy: 0.7945 - val_loss: 0.2112 - val_accuracy: 0.9240\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 70s 250ms/step - loss: 0.0825 - accuracy: 0.9711 - val_loss: 0.2547 - val_accuracy: 0.9090\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 74s 263ms/step - loss: 0.0216 - accuracy: 0.9941 - val_loss: 0.2866 - val_accuracy: 0.9120\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 74s 264ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.3517 - val_accuracy: 0.9160\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 73s 260ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.3830 - val_accuracy: 0.9220\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 72s 257ms/step - loss: 4.4777e-04 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9140\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
      "\n",
      "        acc7  acc8       acc9      acc10        AVG  \n",
      "0  91.000003  93.5  92.500001  92.400002  91.860001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.500002</td>\n",
       "      <td>91.600001</td>\n",
       "      <td>92.199999</td>\n",
       "      <td>90.600002</td>\n",
       "      <td>92.900002</td>\n",
       "      <td>90.399998</td>\n",
       "      <td>91.000003</td>\n",
       "      <td>93.5</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>91.860001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
       "\n",
       "        acc7  acc8       acc9      acc10        AVG  \n",
       "0  91.000003  93.5  92.500001  92.400002  91.860001  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_SUBJ.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.26828931, -0.69846312, -0.09525476, ..., -0.17587639,\n",
       "        -0.91896605, -1.95361796],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 61s 172ms/step - loss: 0.4522 - accuracy: 0.7766 - val_loss: 0.3196 - val_accuracy: 0.8590\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 41s 146ms/step - loss: 0.2347 - accuracy: 0.9041 - val_loss: 0.2524 - val_accuracy: 0.8980\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 40s 141ms/step - loss: 0.1906 - accuracy: 0.9230 - val_loss: 0.2395 - val_accuracy: 0.9070\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 39s 139ms/step - loss: 0.1557 - accuracy: 0.9480 - val_loss: 0.2440 - val_accuracy: 0.9040\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 38s 135ms/step - loss: 0.1429 - accuracy: 0.9514 - val_loss: 0.2661 - val_accuracy: 0.8920\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 0.1166 - accuracy: 0.9592 - val_loss: 0.2792 - val_accuracy: 0.9060\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 0.0924 - accuracy: 0.9663 - val_loss: 0.2461 - val_accuracy: 0.9090\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 0.0762 - accuracy: 0.9762 - val_loss: 0.3971 - val_accuracy: 0.8580\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 37s 133ms/step - loss: 0.0696 - accuracy: 0.9756 - val_loss: 0.2999 - val_accuracy: 0.8980\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 0.0493 - accuracy: 0.9847 - val_loss: 0.3318 - val_accuracy: 0.9000\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 0.0349 - accuracy: 0.9907 - val_loss: 0.4308 - val_accuracy: 0.8980\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 0.0234 - accuracy: 0.9935 - val_loss: 0.3979 - val_accuracy: 0.9020\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 37s 131ms/step - loss: 0.0180 - accuracy: 0.9947 - val_loss: 0.3919 - val_accuracy: 0.9050\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 34s 119ms/step - loss: 0.0529 - accuracy: 0.9807 - val_loss: 0.3851 - val_accuracy: 0.9000\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 31s 111ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 0.5190 - val_accuracy: 0.8800\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 33s 118ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.5710 - val_accuracy: 0.8740\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 33s 118ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.4243 - val_accuracy: 0.8680\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 90.89999794960022\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 173s 563ms/step - loss: 0.4415 - accuracy: 0.7885 - val_loss: 0.3002 - val_accuracy: 0.8600\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 155s 548ms/step - loss: 0.2450 - accuracy: 0.9035 - val_loss: 0.2595 - val_accuracy: 0.8820\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.1883 - accuracy: 0.9255 - val_loss: 0.2339 - val_accuracy: 0.8970\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 154s 548ms/step - loss: 0.1585 - accuracy: 0.9414 - val_loss: 0.2363 - val_accuracy: 0.8960\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 154s 548ms/step - loss: 0.1333 - accuracy: 0.9508 - val_loss: 0.2408 - val_accuracy: 0.8920\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 154s 547ms/step - loss: 0.1098 - accuracy: 0.9615 - val_loss: 0.2487 - val_accuracy: 0.8960\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0860 - accuracy: 0.9696 - val_loss: 0.2639 - val_accuracy: 0.8860\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0721 - accuracy: 0.9742 - val_loss: 0.2935 - val_accuracy: 0.8920\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0534 - accuracy: 0.9799 - val_loss: 0.3894 - val_accuracy: 0.8690\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 155s 548ms/step - loss: 0.0418 - accuracy: 0.9854 - val_loss: 0.2894 - val_accuracy: 0.9020\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 155s 548ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 0.3458 - val_accuracy: 0.8830\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 155s 548ms/step - loss: 0.0289 - accuracy: 0.9908 - val_loss: 0.3507 - val_accuracy: 0.8930\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 155s 548ms/step - loss: 0.0239 - accuracy: 0.9929 - val_loss: 0.3524 - val_accuracy: 0.9010\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.3710 - val_accuracy: 0.8980\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0065 - accuracy: 0.9991 - val_loss: 0.3734 - val_accuracy: 0.9000\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 155s 549ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.4439 - val_accuracy: 0.8900\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 155s 548ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.4237 - val_accuracy: 0.8720\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.5082 - val_accuracy: 0.8790\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 155s 551ms/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 0.4158 - val_accuracy: 0.8850\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.5377 - val_accuracy: 0.8680\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 90.20000100135803\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 153s 502ms/step - loss: 0.4425 - accuracy: 0.7751 - val_loss: 0.2903 - val_accuracy: 0.8820\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 115s 407ms/step - loss: 0.2327 - accuracy: 0.9058 - val_loss: 0.2843 - val_accuracy: 0.8760\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 101s 360ms/step - loss: 0.1881 - accuracy: 0.9302 - val_loss: 0.3377 - val_accuracy: 0.8590\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 101s 358ms/step - loss: 0.1671 - accuracy: 0.9356 - val_loss: 0.2637 - val_accuracy: 0.8920\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 100s 356ms/step - loss: 0.1381 - accuracy: 0.9502 - val_loss: 0.2613 - val_accuracy: 0.8870\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 101s 357ms/step - loss: 0.1085 - accuracy: 0.9619 - val_loss: 0.2353 - val_accuracy: 0.9000\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 101s 357ms/step - loss: 0.0908 - accuracy: 0.9697 - val_loss: 0.2485 - val_accuracy: 0.8990\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 101s 357ms/step - loss: 0.0885 - accuracy: 0.9697 - val_loss: 0.2582 - val_accuracy: 0.9060\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 100s 355ms/step - loss: 0.0580 - accuracy: 0.9806 - val_loss: 0.2714 - val_accuracy: 0.8960\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 100s 355ms/step - loss: 0.0447 - accuracy: 0.9851 - val_loss: 0.3012 - val_accuracy: 0.8980\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 100s 355ms/step - loss: 0.0336 - accuracy: 0.9881 - val_loss: 0.3276 - val_accuracy: 0.8870\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 101s 358ms/step - loss: 0.0256 - accuracy: 0.9910 - val_loss: 0.3345 - val_accuracy: 0.9010\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 101s 358ms/step - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.3540 - val_accuracy: 0.8880\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 101s 359ms/step - loss: 0.0163 - accuracy: 0.9965 - val_loss: 0.3478 - val_accuracy: 0.8950\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 102s 361ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 0.3313 - val_accuracy: 0.8980\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 102s 360ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.3784 - val_accuracy: 0.8930\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 102s 361ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 0.3843 - val_accuracy: 0.8970\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 102s 362ms/step - loss: 0.0131 - accuracy: 0.9951 - val_loss: 0.3489 - val_accuracy: 0.9030\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 90.6000018119812\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 126s 406ms/step - loss: 0.4481 - accuracy: 0.7697 - val_loss: 0.2649 - val_accuracy: 0.8960\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 118s 420ms/step - loss: 0.2280 - accuracy: 0.9076 - val_loss: 0.2343 - val_accuracy: 0.9050\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 135s 480ms/step - loss: 0.1932 - accuracy: 0.9299 - val_loss: 0.2483 - val_accuracy: 0.8930\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 131s 466ms/step - loss: 0.1534 - accuracy: 0.9433 - val_loss: 0.2677 - val_accuracy: 0.8870\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 125s 444ms/step - loss: 0.1254 - accuracy: 0.9569 - val_loss: 0.3033 - val_accuracy: 0.8790\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 131s 466ms/step - loss: 0.1069 - accuracy: 0.9600 - val_loss: 0.3131 - val_accuracy: 0.8710\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 121s 431ms/step - loss: 0.0882 - accuracy: 0.9675 - val_loss: 0.4053 - val_accuracy: 0.8500\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 119s 421ms/step - loss: 0.0685 - accuracy: 0.9770 - val_loss: 0.4583 - val_accuracy: 0.8420\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 119s 422ms/step - loss: 0.0577 - accuracy: 0.9822 - val_loss: 0.3825 - val_accuracy: 0.8680\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 119s 421ms/step - loss: 0.0457 - accuracy: 0.9850 - val_loss: 0.5042 - val_accuracy: 0.8520\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 115s 408ms/step - loss: 0.0286 - accuracy: 0.9913 - val_loss: 0.4495 - val_accuracy: 0.8730\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 112s 399ms/step - loss: 0.0290 - accuracy: 0.9914 - val_loss: 0.7006 - val_accuracy: 0.8270\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 90.49999713897705\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 118s 381ms/step - loss: 0.4524 - accuracy: 0.7754 - val_loss: 0.2401 - val_accuracy: 0.9160\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 88s 310ms/step - loss: 0.2268 - accuracy: 0.9093 - val_loss: 0.2235 - val_accuracy: 0.9140\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.1857 - accuracy: 0.9296 - val_loss: 0.2115 - val_accuracy: 0.9080\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.1575 - accuracy: 0.9374 - val_loss: 0.2196 - val_accuracy: 0.9110\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.1249 - accuracy: 0.9557 - val_loss: 0.1929 - val_accuracy: 0.9230\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.1106 - accuracy: 0.9593 - val_loss: 0.2583 - val_accuracy: 0.9030\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 88s 314ms/step - loss: 0.0821 - accuracy: 0.9708 - val_loss: 0.2254 - val_accuracy: 0.9170\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 88s 312ms/step - loss: 0.0655 - accuracy: 0.9777 - val_loss: 0.2432 - val_accuracy: 0.9100\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 88s 312ms/step - loss: 0.0547 - accuracy: 0.9827 - val_loss: 0.2460 - val_accuracy: 0.9090\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0423 - accuracy: 0.9860 - val_loss: 0.2615 - val_accuracy: 0.9060\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.3141 - val_accuracy: 0.9060\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 88s 312ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.3711 - val_accuracy: 0.8830\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0277 - accuracy: 0.9921 - val_loss: 0.3384 - val_accuracy: 0.9040\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.3809 - val_accuracy: 0.9100\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.3636 - val_accuracy: 0.9100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 92.29999780654907\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 186s 607ms/step - loss: 0.4505 - accuracy: 0.7771 - val_loss: 0.2647 - val_accuracy: 0.8900\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 161s 572ms/step - loss: 0.2339 - accuracy: 0.9089 - val_loss: 0.2468 - val_accuracy: 0.8960\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 145s 513ms/step - loss: 0.1851 - accuracy: 0.9285 - val_loss: 0.2466 - val_accuracy: 0.9000\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 145s 514ms/step - loss: 0.1476 - accuracy: 0.9457 - val_loss: 0.3120 - val_accuracy: 0.8610\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 143s 508ms/step - loss: 0.1351 - accuracy: 0.9517 - val_loss: 0.2488 - val_accuracy: 0.8960\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 142s 505ms/step - loss: 0.1194 - accuracy: 0.9548 - val_loss: 0.2472 - val_accuracy: 0.9030\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 142s 504ms/step - loss: 0.0956 - accuracy: 0.9648 - val_loss: 0.2474 - val_accuracy: 0.8990\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 141s 500ms/step - loss: 0.0728 - accuracy: 0.9758 - val_loss: 0.2783 - val_accuracy: 0.8910\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 141s 499ms/step - loss: 0.0586 - accuracy: 0.9802 - val_loss: 0.2868 - val_accuracy: 0.8980\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 141s 501ms/step - loss: 0.0364 - accuracy: 0.9875 - val_loss: 0.3017 - val_accuracy: 0.8870\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 141s 501ms/step - loss: 0.0441 - accuracy: 0.9865 - val_loss: 0.3280 - val_accuracy: 0.8870\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 141s 500ms/step - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.3351 - val_accuracy: 0.8950\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 141s 500ms/step - loss: 0.0256 - accuracy: 0.9921 - val_loss: 0.3744 - val_accuracy: 0.8920\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 140s 498ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.3481 - val_accuracy: 0.8860\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 141s 500ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.4049 - val_accuracy: 0.8900\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 140s 498ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.3628 - val_accuracy: 0.8970\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 90.2999997138977\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 176s 586ms/step - loss: 0.4312 - accuracy: 0.7879 - val_loss: 0.2779 - val_accuracy: 0.8880\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 155s 551ms/step - loss: 0.2278 - accuracy: 0.9077 - val_loss: 0.2759 - val_accuracy: 0.8880\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 144s 512ms/step - loss: 0.1767 - accuracy: 0.9348 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 143s 506ms/step - loss: 0.1655 - accuracy: 0.9352 - val_loss: 0.2748 - val_accuracy: 0.8950\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 142s 502ms/step - loss: 0.1328 - accuracy: 0.9520 - val_loss: 0.2910 - val_accuracy: 0.8950\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 141s 501ms/step - loss: 0.1129 - accuracy: 0.9608 - val_loss: 0.3263 - val_accuracy: 0.8800\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 141s 502ms/step - loss: 0.0924 - accuracy: 0.9691 - val_loss: 0.3791 - val_accuracy: 0.8750\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 141s 500ms/step - loss: 0.0718 - accuracy: 0.9765 - val_loss: 0.3071 - val_accuracy: 0.8950\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 141s 502ms/step - loss: 0.0590 - accuracy: 0.9824 - val_loss: 0.3253 - val_accuracy: 0.8930\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 141s 499ms/step - loss: 0.0465 - accuracy: 0.9852 - val_loss: 0.3749 - val_accuracy: 0.8880\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 141s 502ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.4470 - val_accuracy: 0.8840\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 142s 504ms/step - loss: 0.0273 - accuracy: 0.9916 - val_loss: 0.4713 - val_accuracy: 0.8850\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 142s 503ms/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 0.4591 - val_accuracy: 0.8870\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 141s 502ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.5123 - val_accuracy: 0.8810\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 89.49999809265137\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 111s 355ms/step - loss: 0.4502 - accuracy: 0.7806 - val_loss: 0.3106 - val_accuracy: 0.8770\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 104s 369ms/step - loss: 0.2286 - accuracy: 0.9123 - val_loss: 0.2933 - val_accuracy: 0.8740\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 105s 371ms/step - loss: 0.1941 - accuracy: 0.9281 - val_loss: 0.3329 - val_accuracy: 0.8660\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 97s 343ms/step - loss: 0.1509 - accuracy: 0.9446 - val_loss: 0.3339 - val_accuracy: 0.8720\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 98s 347ms/step - loss: 0.1282 - accuracy: 0.9538 - val_loss: 0.2961 - val_accuracy: 0.8880\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 99s 350ms/step - loss: 0.1138 - accuracy: 0.9605 - val_loss: 0.2706 - val_accuracy: 0.8990\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 97s 343ms/step - loss: 0.0810 - accuracy: 0.9735 - val_loss: 0.2873 - val_accuracy: 0.8970\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 97s 343ms/step - loss: 0.0672 - accuracy: 0.9757 - val_loss: 0.3608 - val_accuracy: 0.8960\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 97s 344ms/step - loss: 0.0516 - accuracy: 0.9839 - val_loss: 0.3982 - val_accuracy: 0.8810\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 97s 344ms/step - loss: 0.0453 - accuracy: 0.9851 - val_loss: 0.4788 - val_accuracy: 0.8700\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 97s 343ms/step - loss: 0.0381 - accuracy: 0.9879 - val_loss: 0.4013 - val_accuracy: 0.9000\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0221 - accuracy: 0.9940 - val_loss: 0.4309 - val_accuracy: 0.8790\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0393 - accuracy: 0.9872 - val_loss: 0.4994 - val_accuracy: 0.8820\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 0.5851 - val_accuracy: 0.8710\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 98s 347ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.5963 - val_accuracy: 0.8870\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 97s 344ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.6491 - val_accuracy: 0.8870\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0350 - accuracy: 0.9878 - val_loss: 0.5422 - val_accuracy: 0.8700\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 97s 344ms/step - loss: 0.0108 - accuracy: 0.9958 - val_loss: 0.5455 - val_accuracy: 0.8990\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 97s 344ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.5280 - val_accuracy: 0.8850\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 97s 344ms/step - loss: 0.0090 - accuracy: 0.9980 - val_loss: 0.5605 - val_accuracy: 0.8780\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.6679 - val_accuracy: 0.8840\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Test Accuracy: 89.99999761581421\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 139s 456ms/step - loss: 0.4536 - accuracy: 0.7729 - val_loss: 0.2772 - val_accuracy: 0.8900\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 134s 474ms/step - loss: 0.2310 - accuracy: 0.9085 - val_loss: 0.2444 - val_accuracy: 0.9030\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 134s 477ms/step - loss: 0.1897 - accuracy: 0.9260 - val_loss: 0.2408 - val_accuracy: 0.9040\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 134s 476ms/step - loss: 0.1613 - accuracy: 0.9413 - val_loss: 0.2873 - val_accuracy: 0.8840\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 134s 474ms/step - loss: 0.1252 - accuracy: 0.9550 - val_loss: 0.2512 - val_accuracy: 0.9050\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 133s 472ms/step - loss: 0.1083 - accuracy: 0.9613 - val_loss: 0.2608 - val_accuracy: 0.9080\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 133s 471ms/step - loss: 0.0866 - accuracy: 0.9697 - val_loss: 0.3211 - val_accuracy: 0.8860\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 129s 458ms/step - loss: 0.0609 - accuracy: 0.9800 - val_loss: 0.3218 - val_accuracy: 0.8970\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 131s 464ms/step - loss: 0.0625 - accuracy: 0.9792 - val_loss: 0.3274 - val_accuracy: 0.8950\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 129s 458ms/step - loss: 0.0330 - accuracy: 0.9912 - val_loss: 0.3428 - val_accuracy: 0.8900\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 130s 461ms/step - loss: 0.0275 - accuracy: 0.9924 - val_loss: 0.3689 - val_accuracy: 0.8900\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 129s 456ms/step - loss: 0.0224 - accuracy: 0.9933 - val_loss: 0.4126 - val_accuracy: 0.8860\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 126s 449ms/step - loss: 0.0163 - accuracy: 0.9951 - val_loss: 0.4377 - val_accuracy: 0.8850\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 125s 445ms/step - loss: 0.0199 - accuracy: 0.9950 - val_loss: 0.4503 - val_accuracy: 0.8890\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 125s 445ms/step - loss: 0.0093 - accuracy: 0.9981 - val_loss: 0.4235 - val_accuracy: 0.8830\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 125s 444ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 0.4952 - val_accuracy: 0.8830\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 90.79999923706055\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 171s 573ms/step - loss: 0.4649 - accuracy: 0.7476 - val_loss: 0.3842 - val_accuracy: 0.8300\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 156s 553ms/step - loss: 0.2145 - accuracy: 0.9193 - val_loss: 0.3416 - val_accuracy: 0.8600\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 154s 546ms/step - loss: 0.1858 - accuracy: 0.9314 - val_loss: 0.3597 - val_accuracy: 0.8620\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 161s 571ms/step - loss: 0.1533 - accuracy: 0.9444 - val_loss: 0.3127 - val_accuracy: 0.8740\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 162s 576ms/step - loss: 0.1182 - accuracy: 0.9586 - val_loss: 0.2853 - val_accuracy: 0.8840\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 157s 555ms/step - loss: 0.1075 - accuracy: 0.9571 - val_loss: 0.3194 - val_accuracy: 0.8760\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 153s 543ms/step - loss: 0.0808 - accuracy: 0.9706 - val_loss: 0.3157 - val_accuracy: 0.8960\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 153s 543ms/step - loss: 0.0615 - accuracy: 0.9779 - val_loss: 0.3438 - val_accuracy: 0.8750\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 153s 543ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.3591 - val_accuracy: 0.8930\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 163s 577ms/step - loss: 0.0396 - accuracy: 0.9863 - val_loss: 0.4539 - val_accuracy: 0.8750\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 161s 572ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.4530 - val_accuracy: 0.8680\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 161s 572ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.3640 - val_accuracy: 0.8840\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 161s 571ms/step - loss: 0.0147 - accuracy: 0.9962 - val_loss: 0.5065 - val_accuracy: 0.8670\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 162s 573ms/step - loss: 0.0177 - accuracy: 0.9952 - val_loss: 0.5021 - val_accuracy: 0.8780\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 162s 576ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.3906 - val_accuracy: 0.8860\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 162s 575ms/step - loss: 0.0143 - accuracy: 0.9960 - val_loss: 0.4684 - val_accuracy: 0.8820\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 162s 574ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 0.4284 - val_accuracy: 0.8880\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 89.60000276565552\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5  acc6       acc7  \\\n",
      "0  90.899998  90.200001  90.600002  90.499997  92.299998  90.3  89.499998   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  89.999998  90.799999  89.600003  90.469999  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.899998</td>\n",
       "      <td>90.200001</td>\n",
       "      <td>90.600002</td>\n",
       "      <td>90.499997</td>\n",
       "      <td>92.299998</td>\n",
       "      <td>90.3</td>\n",
       "      <td>89.499998</td>\n",
       "      <td>89.999998</td>\n",
       "      <td>90.799999</td>\n",
       "      <td>89.600003</td>\n",
       "      <td>90.469999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5  acc6       acc7  \\\n",
       "0  90.899998  90.200001  90.600002  90.499997  92.299998  90.3  89.499998   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  89.999998  90.799999  89.600003  90.469999  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_SUBJ_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 259s 877ms/step - loss: 0.4264 - accuracy: 0.7807 - val_loss: 0.2075 - val_accuracy: 0.9140\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 215s 762ms/step - loss: 0.1074 - accuracy: 0.9631 - val_loss: 0.2045 - val_accuracy: 0.9170\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 201s 712ms/step - loss: 0.0243 - accuracy: 0.9932 - val_loss: 0.2441 - val_accuracy: 0.9110\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 199s 705ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.3575 - val_accuracy: 0.8960\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 198s 701ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.2918 - val_accuracy: 0.9160\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 196s 697ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.4398 - val_accuracy: 0.9050\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 197s 700ms/step - loss: 3.0921e-04 - accuracy: 1.0000 - val_loss: 0.4413 - val_accuracy: 0.9070\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 197s 697ms/step - loss: 1.6258e-04 - accuracy: 1.0000 - val_loss: 0.4840 - val_accuracy: 0.9050\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 197s 699ms/step - loss: 9.2153e-05 - accuracy: 1.0000 - val_loss: 0.5010 - val_accuracy: 0.9050\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 197s 698ms/step - loss: 7.9201e-05 - accuracy: 1.0000 - val_loss: 0.5129 - val_accuracy: 0.9060\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 197s 698ms/step - loss: 5.3136e-05 - accuracy: 1.0000 - val_loss: 0.5242 - val_accuracy: 0.9060\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 197s 700ms/step - loss: 4.8511e-05 - accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 0.9050\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 91.69999957084656\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 191s 638ms/step - loss: 0.4220 - accuracy: 0.7886 - val_loss: 0.2516 - val_accuracy: 0.9020\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 183s 648ms/step - loss: 0.1006 - accuracy: 0.9644 - val_loss: 0.2557 - val_accuracy: 0.8920\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 216s 765ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.2668 - val_accuracy: 0.9020\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 216s 767ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.3458 - val_accuracy: 0.8960\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 215s 764ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.3861 - val_accuracy: 0.8910\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 214s 760ms/step - loss: 7.9958e-04 - accuracy: 1.0000 - val_loss: 0.4586 - val_accuracy: 0.8920\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 214s 758ms/step - loss: 2.2788e-04 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8950\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 213s 755ms/step - loss: 1.5214e-04 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8950\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 213s 755ms/step - loss: 9.9684e-05 - accuracy: 1.0000 - val_loss: 0.5250 - val_accuracy: 0.8930\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 214s 759ms/step - loss: 8.3064e-05 - accuracy: 1.0000 - val_loss: 0.5428 - val_accuracy: 0.8930\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 215s 761ms/step - loss: 6.4058e-05 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.8920\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 90.20000100135803\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 175s 585ms/step - loss: 0.4246 - accuracy: 0.7843 - val_loss: 0.1864 - val_accuracy: 0.9360\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 149s 529ms/step - loss: 0.1043 - accuracy: 0.9648 - val_loss: 0.1839 - val_accuracy: 0.9310\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 150s 531ms/step - loss: 0.0406 - accuracy: 0.9874 - val_loss: 0.2119 - val_accuracy: 0.9260\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 148s 527ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.2283 - val_accuracy: 0.9240\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 149s 529ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.2787 - val_accuracy: 0.9200\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 150s 531ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2954 - val_accuracy: 0.9200\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 150s 531ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.3286 - val_accuracy: 0.9260\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 150s 531ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2871 - val_accuracy: 0.9220\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 150s 532ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3644 - val_accuracy: 0.9280\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 150s 532ms/step - loss: 1.7575e-04 - accuracy: 1.0000 - val_loss: 0.3833 - val_accuracy: 0.9260\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 150s 530ms/step - loss: 8.0193e-05 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 0.9270\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 93.59999895095825\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 342s 1s/step - loss: 0.4093 - accuracy: 0.7957 - val_loss: 0.2146 - val_accuracy: 0.9060\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 329s 1s/step - loss: 0.1035 - accuracy: 0.9667 - val_loss: 0.2503 - val_accuracy: 0.8970\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 330s 1s/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.2570 - val_accuracy: 0.9060\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 329s 1s/step - loss: 0.0090 - accuracy: 0.9978 - val_loss: 0.3016 - val_accuracy: 0.9040\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 330s 1s/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 0.4022 - val_accuracy: 0.9010\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 330s 1s/step - loss: 9.1387e-04 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.9020\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 331s 1s/step - loss: 2.5902e-04 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.8910\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 329s 1s/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.4522 - val_accuracy: 0.9010\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 330s 1s/step - loss: 1.5755e-04 - accuracy: 1.0000 - val_loss: 0.4827 - val_accuracy: 0.9080\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 330s 1s/step - loss: 7.8873e-05 - accuracy: 1.0000 - val_loss: 0.5046 - val_accuracy: 0.9040\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 331s 1s/step - loss: 7.4286e-05 - accuracy: 1.0000 - val_loss: 0.5150 - val_accuracy: 0.9040\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 4.1689e-05 - accuracy: 1.0000 - val_loss: 0.5260 - val_accuracy: 0.9030\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 331s 1s/step - loss: 4.2264e-05 - accuracy: 1.0000 - val_loss: 0.5353 - val_accuracy: 0.8990\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 2.7030e-05 - accuracy: 1.0000 - val_loss: 0.5474 - val_accuracy: 0.9020\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 2.0914e-05 - accuracy: 1.0000 - val_loss: 0.5590 - val_accuracy: 0.9010\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 1.8319e-05 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.9020\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 1.6433e-05 - accuracy: 1.0000 - val_loss: 0.5785 - val_accuracy: 0.9020\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 331s 1s/step - loss: 1.1488e-05 - accuracy: 1.0000 - val_loss: 0.5914 - val_accuracy: 0.9030\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 331s 1s/step - loss: 1.2379e-05 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.9010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Test Accuracy: 90.79999923706055\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 336s 1s/step - loss: 0.4230 - accuracy: 0.7961 - val_loss: 0.2400 - val_accuracy: 0.8940\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 291s 1s/step - loss: 0.1078 - accuracy: 0.9615 - val_loss: 0.2189 - val_accuracy: 0.9190\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 263s 932ms/step - loss: 0.0284 - accuracy: 0.9934 - val_loss: 0.2838 - val_accuracy: 0.9140\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 263s 934ms/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.3032 - val_accuracy: 0.9080\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 264s 935ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.3868 - val_accuracy: 0.9090\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 262s 929ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.3497 - val_accuracy: 0.9190\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 263s 931ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.4251 - val_accuracy: 0.9050\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 261s 926ms/step - loss: 9.4314e-04 - accuracy: 0.9999 - val_loss: 0.3959 - val_accuracy: 0.8980\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 263s 931ms/step - loss: 5.7548e-04 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.9060\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 262s 929ms/step - loss: 1.2860e-04 - accuracy: 1.0000 - val_loss: 0.4989 - val_accuracy: 0.9040\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 262s 930ms/step - loss: 7.6002e-05 - accuracy: 1.0000 - val_loss: 0.5234 - val_accuracy: 0.9020\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 262s 928ms/step - loss: 5.1318e-05 - accuracy: 1.0000 - val_loss: 0.5449 - val_accuracy: 0.9030\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 91.90000295639038\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 250s 848ms/step - loss: 0.4197 - accuracy: 0.7996 - val_loss: 0.2077 - val_accuracy: 0.9080\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 219s 777ms/step - loss: 0.1120 - accuracy: 0.9634 - val_loss: 0.2393 - val_accuracy: 0.9040\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 210s 744ms/step - loss: 0.0358 - accuracy: 0.9902 - val_loss: 0.2493 - val_accuracy: 0.9080\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 211s 749ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 0.2477 - val_accuracy: 0.9130\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 211s 749ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 0.3254 - val_accuracy: 0.9170\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 212s 750ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.4392 - val_accuracy: 0.9170\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 219s 778ms/step - loss: 8.8951e-04 - accuracy: 0.9999 - val_loss: 0.4125 - val_accuracy: 0.9130\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 219s 776ms/step - loss: 3.3247e-04 - accuracy: 1.0000 - val_loss: 0.5098 - val_accuracy: 0.9080\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 218s 774ms/step - loss: 1.2340e-04 - accuracy: 1.0000 - val_loss: 0.5397 - val_accuracy: 0.9090\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 220s 779ms/step - loss: 9.3467e-05 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.9090\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 223s 790ms/step - loss: 5.4466e-05 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.9070\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 224s 794ms/step - loss: 4.6617e-05 - accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 0.9080\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 223s 791ms/step - loss: 4.0730e-05 - accuracy: 1.0000 - val_loss: 0.6007 - val_accuracy: 0.9090\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 225s 796ms/step - loss: 2.8934e-05 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.9090\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 224s 795ms/step - loss: 2.3647e-05 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.9070\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 91.69999957084656\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 170s 563ms/step - loss: 0.4198 - accuracy: 0.7864 - val_loss: 0.2154 - val_accuracy: 0.9160\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 164s 583ms/step - loss: 0.1152 - accuracy: 0.9593 - val_loss: 0.2490 - val_accuracy: 0.9140\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 190s 674ms/step - loss: 0.0335 - accuracy: 0.9902 - val_loss: 0.3637 - val_accuracy: 0.8930\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 190s 674ms/step - loss: 0.0265 - accuracy: 0.9911 - val_loss: 0.3507 - val_accuracy: 0.9020\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 192s 681ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.4330 - val_accuracy: 0.8930\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 191s 678ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.8940\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 191s 678ms/step - loss: 3.6388e-04 - accuracy: 1.0000 - val_loss: 0.5813 - val_accuracy: 0.8920\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 191s 678ms/step - loss: 1.8906e-04 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.8900\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 191s 677ms/step - loss: 1.1428e-04 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.8910\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 191s 676ms/step - loss: 8.2483e-05 - accuracy: 1.0000 - val_loss: 0.6703 - val_accuracy: 0.8910\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 191s 677ms/step - loss: 6.2132e-05 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.8910\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 91.60000085830688\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 324s 1s/step - loss: 0.3985 - accuracy: 0.8090 - val_loss: 0.3216 - val_accuracy: 0.8680\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 297s 1s/step - loss: 0.1081 - accuracy: 0.9636 - val_loss: 0.2597 - val_accuracy: 0.8910\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 271s 962ms/step - loss: 0.0377 - accuracy: 0.9879 - val_loss: 0.3291 - val_accuracy: 0.8820\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 268s 951ms/step - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.4063 - val_accuracy: 0.8850\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 268s 951ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.3821 - val_accuracy: 0.8980\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 268s 950ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.4485 - val_accuracy: 0.8890\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 267s 946ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.5815 - val_accuracy: 0.8860\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 268s 950ms/step - loss: 2.4196e-04 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.8800\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 268s 949ms/step - loss: 1.7338e-04 - accuracy: 1.0000 - val_loss: 0.6651 - val_accuracy: 0.8800\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 266s 943ms/step - loss: 8.5782e-05 - accuracy: 1.0000 - val_loss: 0.6947 - val_accuracy: 0.8770\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 265s 941ms/step - loss: 5.6467e-05 - accuracy: 1.0000 - val_loss: 0.7073 - val_accuracy: 0.8810\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 265s 940ms/step - loss: 5.1239e-05 - accuracy: 1.0000 - val_loss: 0.7275 - val_accuracy: 0.8810\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 265s 940ms/step - loss: 3.2883e-05 - accuracy: 1.0000 - val_loss: 0.7545 - val_accuracy: 0.8800\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 265s 939ms/step - loss: 3.2068e-05 - accuracy: 1.0000 - val_loss: 0.7750 - val_accuracy: 0.8780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40\n",
      "282/282 [==============================] - 265s 941ms/step - loss: 3.1217e-05 - accuracy: 1.0000 - val_loss: 0.7881 - val_accuracy: 0.8790\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 89.80000019073486\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 225s 758ms/step - loss: 0.4336 - accuracy: 0.7735 - val_loss: 0.2207 - val_accuracy: 0.9100\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 210s 743ms/step - loss: 0.1048 - accuracy: 0.9641 - val_loss: 0.2008 - val_accuracy: 0.9250\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 210s 745ms/step - loss: 0.0343 - accuracy: 0.9895 - val_loss: 0.2758 - val_accuracy: 0.9090\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 210s 746ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.3506 - val_accuracy: 0.9130\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 211s 747ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.4164 - val_accuracy: 0.9030\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 210s 746ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5150 - val_accuracy: 0.9040\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 211s 746ms/step - loss: 3.1574e-04 - accuracy: 1.0000 - val_loss: 0.4883 - val_accuracy: 0.9140\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 211s 748ms/step - loss: 2.2731e-04 - accuracy: 1.0000 - val_loss: 0.5566 - val_accuracy: 0.9110\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 212s 751ms/step - loss: 8.6608e-05 - accuracy: 1.0000 - val_loss: 0.5824 - val_accuracy: 0.9100\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 210s 745ms/step - loss: 6.3108e-05 - accuracy: 1.0000 - val_loss: 0.5950 - val_accuracy: 0.9090\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 210s 746ms/step - loss: 4.3609e-05 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 0.9090\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 210s 744ms/step - loss: 3.3639e-05 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 0.9070\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 202s 676ms/step - loss: 0.4192 - accuracy: 0.7946 - val_loss: 0.2067 - val_accuracy: 0.9180\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 196s 696ms/step - loss: 0.1042 - accuracy: 0.9646 - val_loss: 0.2092 - val_accuracy: 0.9100\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 214s 760ms/step - loss: 0.0280 - accuracy: 0.9924 - val_loss: 0.2620 - val_accuracy: 0.9010\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 216s 767ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.3719 - val_accuracy: 0.8960\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 217s 768ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.3313 - val_accuracy: 0.8990\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 216s 765ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.3687 - val_accuracy: 0.9120\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 216s 765ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.4505 - val_accuracy: 0.9020\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 215s 764ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3250 - val_accuracy: 0.9090\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 217s 770ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.4537 - val_accuracy: 0.9070\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 217s 768ms/step - loss: 1.2800e-04 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.9050\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 217s 768ms/step - loss: 6.8266e-05 - accuracy: 1.0000 - val_loss: 0.5071 - val_accuracy: 0.9050\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 91.79999828338623\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
      "\n",
      "        acc7  acc8       acc9      acc10        AVG  \n",
      "0  91.000003  93.5  92.500001  92.400002  91.860001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.7</td>\n",
       "      <td>90.200001</td>\n",
       "      <td>93.599999</td>\n",
       "      <td>90.799999</td>\n",
       "      <td>91.900003</td>\n",
       "      <td>91.7</td>\n",
       "      <td>91.600001</td>\n",
       "      <td>89.8</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>91.799998</td>\n",
       "      <td>91.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc1       acc2       acc3       acc4       acc5  acc6       acc7  acc8  \\\n",
       "0  91.7  90.200001  93.599999  90.799999  91.900003  91.7  91.600001  89.8   \n",
       "\n",
       "        acc9      acc10    AVG  \n",
       "0  92.500001  91.799998  91.56  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_SUBJ_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
