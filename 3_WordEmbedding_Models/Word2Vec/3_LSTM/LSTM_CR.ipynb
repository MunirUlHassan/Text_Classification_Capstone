{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with CR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the Customer Reviews Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3775, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weaknesses are minor the feel and layout of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many of our disney movies do n 't play on this...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player has a problem with dual layer dvd 's su...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know the saying is you get what you pay for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will never purchase apex again .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>so far , the anti spam feature seems to be ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>i did not have any of the installation problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>their products have been great and have saved ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     weaknesses are minor the feel and layout of th...      0  train\n",
       "1     many of our disney movies do n 't play on this...      0  train\n",
       "2     player has a problem with dual layer dvd 's su...      0  train\n",
       "3     i know the saying is you get what you pay for ...      0  train\n",
       "4                      will never purchase apex again .      0  train\n",
       "...                                                 ...    ...    ...\n",
       "3770  so far , the anti spam feature seems to be ver...      1  train\n",
       "3771  i downloaded a trial version of computer assoc...      1  train\n",
       "3772  i did not have any of the installation problem...      1  train\n",
       "3773  their products have been great and have saved ...      1  train\n",
       "3774                                                         1  train\n",
       "\n",
       "[3775 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/CR/CR.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3775 entries, 0 to 3774\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3775 non-null   object\n",
      " 1   label     3775 non-null   int32 \n",
      " 2   split     3775 non-null   object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 73.9+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2407</td>\n",
       "      <td>2407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          1368   1368\n",
       "1          2407   2407"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weaknesses are minor the feel and layout of the remote control are only so so . it does n 't show the complete file names of mp3s with really long names . you must cycle through every zoom setting ( 2x , 3x , 4x , 1 2x , etc . ) before getting back to normal size sorry if i 'm just ignorant of a way to get back to 1x quickly .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  will never purchase apex again .\n",
      "Into a sequence of int: [72, 194, 285, 207, 286]\n",
      "Into a padded sequence: [ 72 194 285 207 286   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "and 3\n",
      "i 4\n",
      "it 5\n",
      "to 6\n",
      "a 7\n",
      "is 8\n",
      "of 9\n",
      "this 10\n",
      "5336\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 52s 402ms/step - loss: 0.6235 - accuracy: 0.6519 - val_loss: 0.4560 - val_accuracy: 0.8069\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 35s 326ms/step - loss: 0.2971 - accuracy: 0.8729 - val_loss: 0.4364 - val_accuracy: 0.7989\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 36s 333ms/step - loss: 0.1369 - accuracy: 0.9562 - val_loss: 0.5169 - val_accuracy: 0.8307\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 35s 330ms/step - loss: 0.0758 - accuracy: 0.9762 - val_loss: 0.6673 - val_accuracy: 0.8042\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 36s 333ms/step - loss: 0.0384 - accuracy: 0.9895 - val_loss: 0.7779 - val_accuracy: 0.8228\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 36s 332ms/step - loss: 0.0205 - accuracy: 0.9942 - val_loss: 0.9225 - val_accuracy: 0.8201\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 35s 331ms/step - loss: 0.0417 - accuracy: 0.9904 - val_loss: 0.7853 - val_accuracy: 0.8201\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 36s 332ms/step - loss: 0.0122 - accuracy: 0.9958 - val_loss: 1.0469 - val_accuracy: 0.8069\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 83.06878209114075\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 52s 386ms/step - loss: 0.6164 - accuracy: 0.6600 - val_loss: 0.4330 - val_accuracy: 0.7963\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 38s 353ms/step - loss: 0.2801 - accuracy: 0.8840 - val_loss: 0.4320 - val_accuracy: 0.8016\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 36s 334ms/step - loss: 0.1368 - accuracy: 0.9563 - val_loss: 0.5979 - val_accuracy: 0.7751\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 34s 321ms/step - loss: 0.0808 - accuracy: 0.9727 - val_loss: 0.8016 - val_accuracy: 0.7778\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 34s 319ms/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.9188 - val_accuracy: 0.7646\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 34s 321ms/step - loss: 0.0301 - accuracy: 0.9883 - val_loss: 1.1500 - val_accuracy: 0.7698\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 35s 324ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 1.1401 - val_accuracy: 0.7725\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 50s 375ms/step - loss: 0.6135 - accuracy: 0.6588 - val_loss: 0.4568 - val_accuracy: 0.7672\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 32s 296ms/step - loss: 0.2934 - accuracy: 0.8821 - val_loss: 0.4630 - val_accuracy: 0.7937\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 32s 298ms/step - loss: 0.1362 - accuracy: 0.9516 - val_loss: 0.4722 - val_accuracy: 0.7672\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 32s 295ms/step - loss: 0.0835 - accuracy: 0.9785 - val_loss: 0.7345 - val_accuracy: 0.7778\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 32s 298ms/step - loss: 0.0659 - accuracy: 0.9788 - val_loss: 0.7486 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 32s 295ms/step - loss: 0.0244 - accuracy: 0.9925 - val_loss: 1.0171 - val_accuracy: 0.7302\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 32s 297ms/step - loss: 0.0165 - accuracy: 0.9934 - val_loss: 1.0926 - val_accuracy: 0.7672\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 406ms/step - loss: 0.6213 - accuracy: 0.6491 - val_loss: 0.4172 - val_accuracy: 0.8016\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 41s 380ms/step - loss: 0.2904 - accuracy: 0.8911 - val_loss: 0.4151 - val_accuracy: 0.8095\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 41s 379ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.4879 - val_accuracy: 0.8148\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 41s 380ms/step - loss: 0.0757 - accuracy: 0.9742 - val_loss: 0.5828 - val_accuracy: 0.8069\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 40s 374ms/step - loss: 0.0290 - accuracy: 0.9910 - val_loss: 0.7421 - val_accuracy: 0.8254\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 41s 382ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.7738 - val_accuracy: 0.7989\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 44s 412ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 0.7830 - val_accuracy: 0.7937\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 46s 427ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 1.0566 - val_accuracy: 0.7937\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 44s 414ms/step - loss: 0.0245 - accuracy: 0.9930 - val_loss: 0.8452 - val_accuracy: 0.7989\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 45s 419ms/step - loss: 0.0094 - accuracy: 0.9960 - val_loss: 0.9769 - val_accuracy: 0.7884\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 410ms/step - loss: 0.6167 - accuracy: 0.6667 - val_loss: 0.4425 - val_accuracy: 0.7831\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 0.2819 - accuracy: 0.8893 - val_loss: 0.4710 - val_accuracy: 0.8122\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 38s 353ms/step - loss: 0.1398 - accuracy: 0.9558 - val_loss: 0.5095 - val_accuracy: 0.8069\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 38s 358ms/step - loss: 0.0818 - accuracy: 0.9696 - val_loss: 0.5946 - val_accuracy: 0.7831\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 39s 367ms/step - loss: 0.0474 - accuracy: 0.9895 - val_loss: 0.8707 - val_accuracy: 0.7831\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 40s 377ms/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.8711 - val_accuracy: 0.7751\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 36s 337ms/step - loss: 0.0304 - accuracy: 0.9880 - val_loss: 0.7857 - val_accuracy: 0.7831\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 49s 363ms/step - loss: 0.6215 - accuracy: 0.6486 - val_loss: 0.4616 - val_accuracy: 0.7666\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 35s 327ms/step - loss: 0.2649 - accuracy: 0.8934 - val_loss: 0.4763 - val_accuracy: 0.7613\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.1532 - accuracy: 0.9510 - val_loss: 0.6751 - val_accuracy: 0.7613\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.0823 - accuracy: 0.9747 - val_loss: 0.7299 - val_accuracy: 0.7427\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 329ms/step - loss: 0.0458 - accuracy: 0.9858 - val_loss: 0.9310 - val_accuracy: 0.7347\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.0236 - accuracy: 0.9938 - val_loss: 1.1381 - val_accuracy: 0.7427\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 76.65782570838928\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 51s 382ms/step - loss: 0.6168 - accuracy: 0.6483 - val_loss: 0.4040 - val_accuracy: 0.8064\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 36s 339ms/step - loss: 0.2873 - accuracy: 0.8881 - val_loss: 0.3995 - val_accuracy: 0.8064\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 37s 343ms/step - loss: 0.1394 - accuracy: 0.9521 - val_loss: 0.4507 - val_accuracy: 0.7958\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 37s 344ms/step - loss: 0.0829 - accuracy: 0.9730 - val_loss: 0.6803 - val_accuracy: 0.7639\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 37s 344ms/step - loss: 0.0431 - accuracy: 0.9860 - val_loss: 0.7368 - val_accuracy: 0.7798\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 36s 338ms/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 0.9073 - val_accuracy: 0.7639\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.63660264015198\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 405ms/step - loss: 0.6205 - accuracy: 0.6643 - val_loss: 0.4556 - val_accuracy: 0.7692\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 38s 360ms/step - loss: 0.2743 - accuracy: 0.8893 - val_loss: 0.4527 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 39s 367ms/step - loss: 0.1314 - accuracy: 0.9578 - val_loss: 0.5578 - val_accuracy: 0.7851\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 40s 377ms/step - loss: 0.0575 - accuracy: 0.9851 - val_loss: 0.7943 - val_accuracy: 0.7825\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 40s 374ms/step - loss: 0.0364 - accuracy: 0.9894 - val_loss: 0.7499 - val_accuracy: 0.8064\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 40s 374ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 1.0092 - val_accuracy: 0.7984\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 40s 372ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 1.2059 - val_accuracy: 0.7878\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 40s 373ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 1.1492 - val_accuracy: 0.7958\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 40s 377ms/step - loss: 0.0092 - accuracy: 0.9988 - val_loss: 0.9170 - val_accuracy: 0.7851\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 40s 375ms/step - loss: 0.0179 - accuracy: 0.9934 - val_loss: 1.2219 - val_accuracy: 0.7772\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 56s 426ms/step - loss: 0.6186 - accuracy: 0.6506 - val_loss: 0.4271 - val_accuracy: 0.7825\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.3021 - accuracy: 0.8754 - val_loss: 0.4340 - val_accuracy: 0.8090\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 41s 385ms/step - loss: 0.1420 - accuracy: 0.9539 - val_loss: 0.4686 - val_accuracy: 0.8117\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 41s 384ms/step - loss: 0.0839 - accuracy: 0.9704 - val_loss: 0.4884 - val_accuracy: 0.8117\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 42s 390ms/step - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.8728 - val_accuracy: 0.7905\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 41s 382ms/step - loss: 0.0236 - accuracy: 0.9891 - val_loss: 0.8953 - val_accuracy: 0.8037\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 41s 379ms/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 0.9941 - val_accuracy: 0.7931\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 41s 383ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 1.0146 - val_accuracy: 0.7719\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 81.16710782051086\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 44s 319ms/step - loss: 0.6200 - accuracy: 0.6433 - val_loss: 0.4430 - val_accuracy: 0.8037\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.2820 - accuracy: 0.8810 - val_loss: 0.4540 - val_accuracy: 0.7931\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 26s 246ms/step - loss: 0.1264 - accuracy: 0.9582 - val_loss: 0.5792 - val_accuracy: 0.7851\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.0627 - accuracy: 0.9777 - val_loss: 0.7025 - val_accuracy: 0.7878\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.0414 - accuracy: 0.9886 - val_loss: 0.8860 - val_accuracy: 0.7507\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.9755 - val_accuracy: 0.7825\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 80.37135004997253\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  83.068782  80.158728  79.365081  82.539684  81.216931  76.657826   \n",
      "\n",
      "        acc7       acc8       acc9     acc10        AVG  \n",
      "0  80.636603  80.636603  81.167108  80.37135  80.581869  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.068782</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>80.37135</td>\n",
       "      <td>80.581869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  83.068782  80.158728  79.365081  82.539684  81.216931  76.657826   \n",
       "\n",
       "        acc7       acc8       acc9     acc10        AVG  \n",
       "0  80.636603  80.636603  81.167108  80.37135  80.581869  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_CR.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5046 words present from 5336 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 23s 127ms/step - loss: 0.6122 - accuracy: 0.6386 - val_loss: 0.4855 - val_accuracy: 0.7593\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.4459 - accuracy: 0.7930 - val_loss: 0.4454 - val_accuracy: 0.8042\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 10s 96ms/step - loss: 0.3862 - accuracy: 0.8202 - val_loss: 0.4398 - val_accuracy: 0.7910\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 10s 98ms/step - loss: 0.3577 - accuracy: 0.8356 - val_loss: 0.4218 - val_accuracy: 0.8254\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.3404 - accuracy: 0.8468 - val_loss: 0.4559 - val_accuracy: 0.7698\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 11s 99ms/step - loss: 0.3135 - accuracy: 0.8581 - val_loss: 0.4577 - val_accuracy: 0.7910\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 11s 100ms/step - loss: 0.2729 - accuracy: 0.8879 - val_loss: 0.4512 - val_accuracy: 0.8042\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 11s 106ms/step - loss: 0.2521 - accuracy: 0.8921 - val_loss: 0.4769 - val_accuracy: 0.8069\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 11s 101ms/step - loss: 0.2164 - accuracy: 0.9059 - val_loss: 0.4497 - val_accuracy: 0.8175\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 11s 103ms/step - loss: 0.1848 - accuracy: 0.9225 - val_loss: 0.5099 - val_accuracy: 0.8095\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 11s 106ms/step - loss: 0.2261 - accuracy: 0.9050 - val_loss: 0.4818 - val_accuracy: 0.8201\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 11s 105ms/step - loss: 0.1393 - accuracy: 0.9449 - val_loss: 0.5606 - val_accuracy: 0.8042\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 21s 123ms/step - loss: 0.6156 - accuracy: 0.6342 - val_loss: 0.4741 - val_accuracy: 0.7831\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.4530 - accuracy: 0.7922 - val_loss: 0.4448 - val_accuracy: 0.7778\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 11s 104ms/step - loss: 0.4103 - accuracy: 0.8160 - val_loss: 0.4122 - val_accuracy: 0.7963\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 11s 105ms/step - loss: 0.3707 - accuracy: 0.8404 - val_loss: 0.4049 - val_accuracy: 0.7937\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 11s 105ms/step - loss: 0.3468 - accuracy: 0.8430 - val_loss: 0.4138 - val_accuracy: 0.8016\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 11s 105ms/step - loss: 0.3472 - accuracy: 0.8419 - val_loss: 0.3999 - val_accuracy: 0.7989\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 11s 101ms/step - loss: 0.3035 - accuracy: 0.8714 - val_loss: 0.4304 - val_accuracy: 0.7989\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 11s 101ms/step - loss: 0.2587 - accuracy: 0.8960 - val_loss: 0.3973 - val_accuracy: 0.8042\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 10s 96ms/step - loss: 0.2301 - accuracy: 0.9090 - val_loss: 0.3963 - val_accuracy: 0.8175\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 11s 98ms/step - loss: 0.2009 - accuracy: 0.9145 - val_loss: 0.4535 - val_accuracy: 0.8042\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.1773 - accuracy: 0.9232 - val_loss: 0.4747 - val_accuracy: 0.8095\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 10s 95ms/step - loss: 0.1576 - accuracy: 0.9380 - val_loss: 0.4593 - val_accuracy: 0.8069\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.1337 - accuracy: 0.9500 - val_loss: 0.5210 - val_accuracy: 0.8016\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 11s 98ms/step - loss: 0.0907 - accuracy: 0.9711 - val_loss: 0.5591 - val_accuracy: 0.8095\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 11s 100ms/step - loss: 0.0784 - accuracy: 0.9713 - val_loss: 0.5933 - val_accuracy: 0.8122\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.0644 - accuracy: 0.9793 - val_loss: 0.7079 - val_accuracy: 0.8095\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 10s 96ms/step - loss: 0.0456 - accuracy: 0.9878 - val_loss: 0.6346 - val_accuracy: 0.7989\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 81.7460298538208\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 38s 261ms/step - loss: 0.6025 - accuracy: 0.6761 - val_loss: 0.4802 - val_accuracy: 0.7804\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 0.4464 - accuracy: 0.7905 - val_loss: 0.4386 - val_accuracy: 0.7672\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 16s 151ms/step - loss: 0.3998 - accuracy: 0.8101 - val_loss: 0.4190 - val_accuracy: 0.7884\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 16s 145ms/step - loss: 0.3662 - accuracy: 0.8366 - val_loss: 0.4390 - val_accuracy: 0.7831\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 0.3279 - accuracy: 0.8523 - val_loss: 0.4370 - val_accuracy: 0.7831\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 16s 153ms/step - loss: 0.2968 - accuracy: 0.8573 - val_loss: 0.4281 - val_accuracy: 0.7725\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 16s 151ms/step - loss: 0.2700 - accuracy: 0.8827 - val_loss: 0.4903 - val_accuracy: 0.7698\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 15s 143ms/step - loss: 0.2456 - accuracy: 0.8965 - val_loss: 0.5128 - val_accuracy: 0.8042\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 0.1990 - accuracy: 0.9079 - val_loss: 0.4855 - val_accuracy: 0.7937\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 16s 148ms/step - loss: 0.1474 - accuracy: 0.9416 - val_loss: 0.5455 - val_accuracy: 0.7910\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.1354 - accuracy: 0.9429 - val_loss: 0.6451 - val_accuracy: 0.7540\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.1258 - accuracy: 0.9496 - val_loss: 0.6715 - val_accuracy: 0.7646\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 0.0907 - accuracy: 0.9672 - val_loss: 0.7199 - val_accuracy: 0.7381\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 0.1002 - accuracy: 0.9626 - val_loss: 0.7305 - val_accuracy: 0.7751\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 0.0476 - accuracy: 0.9891 - val_loss: 0.8398 - val_accuracy: 0.7593\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 15s 144ms/step - loss: 0.0766 - accuracy: 0.9735 - val_loss: 0.8881 - val_accuracy: 0.7407\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 80.42327761650085\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 28s 177ms/step - loss: 0.6119 - accuracy: 0.6631 - val_loss: 0.5025 - val_accuracy: 0.7698\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 16s 145ms/step - loss: 0.4460 - accuracy: 0.7921 - val_loss: 0.4547 - val_accuracy: 0.7884\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 18s 166ms/step - loss: 0.3914 - accuracy: 0.8163 - val_loss: 0.4228 - val_accuracy: 0.8175\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 20s 190ms/step - loss: 0.3663 - accuracy: 0.8388 - val_loss: 0.4493 - val_accuracy: 0.8122\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 20s 184ms/step - loss: 0.3423 - accuracy: 0.8428 - val_loss: 0.4052 - val_accuracy: 0.8254\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 19s 174ms/step - loss: 0.3074 - accuracy: 0.8578 - val_loss: 0.4326 - val_accuracy: 0.7857\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 18s 170ms/step - loss: 0.2828 - accuracy: 0.8878 - val_loss: 0.4393 - val_accuracy: 0.8042\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 18s 170ms/step - loss: 0.2284 - accuracy: 0.9063 - val_loss: 0.4353 - val_accuracy: 0.8095\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 17s 162ms/step - loss: 0.2036 - accuracy: 0.9170 - val_loss: 0.4981 - val_accuracy: 0.7937\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.1645 - accuracy: 0.9394 - val_loss: 0.5302 - val_accuracy: 0.7910\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.1382 - accuracy: 0.9482 - val_loss: 0.5868 - val_accuracy: 0.7804\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 18s 171ms/step - loss: 0.1081 - accuracy: 0.9595 - val_loss: 0.6348 - val_accuracy: 0.7857\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 18s 171ms/step - loss: 0.1214 - accuracy: 0.9513 - val_loss: 0.7035 - val_accuracy: 0.7778\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 24s 139ms/step - loss: 0.6260 - accuracy: 0.6476 - val_loss: 0.5303 - val_accuracy: 0.7381\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 11s 98ms/step - loss: 0.4527 - accuracy: 0.7866 - val_loss: 0.5024 - val_accuracy: 0.7566\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 10s 94ms/step - loss: 0.4143 - accuracy: 0.8133 - val_loss: 0.4528 - val_accuracy: 0.7831\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 11s 101ms/step - loss: 0.3690 - accuracy: 0.8373 - val_loss: 0.5033 - val_accuracy: 0.7593\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 10s 94ms/step - loss: 0.3554 - accuracy: 0.8395 - val_loss: 0.4490 - val_accuracy: 0.7937\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.3052 - accuracy: 0.8667 - val_loss: 0.4769 - val_accuracy: 0.7804\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.2541 - accuracy: 0.8896 - val_loss: 0.4623 - val_accuracy: 0.7831\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 10s 91ms/step - loss: 0.2398 - accuracy: 0.9021 - val_loss: 0.5133 - val_accuracy: 0.7831\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.2137 - accuracy: 0.9102 - val_loss: 0.6001 - val_accuracy: 0.7884\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 10s 90ms/step - loss: 0.1742 - accuracy: 0.9330 - val_loss: 0.5419 - val_accuracy: 0.7989\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 10s 91ms/step - loss: 0.1318 - accuracy: 0.9512 - val_loss: 0.5555 - val_accuracy: 0.7937\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 10s 89ms/step - loss: 0.1131 - accuracy: 0.9670 - val_loss: 0.6414 - val_accuracy: 0.7778\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 10s 90ms/step - loss: 0.1018 - accuracy: 0.9670 - val_loss: 0.6305 - val_accuracy: 0.8016\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 9s 89ms/step - loss: 0.1012 - accuracy: 0.9676 - val_loss: 0.6451 - val_accuracy: 0.8042\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 9s 87ms/step - loss: 0.0601 - accuracy: 0.9853 - val_loss: 0.7317 - val_accuracy: 0.8095\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 9s 88ms/step - loss: 0.0925 - accuracy: 0.9721 - val_loss: 0.7880 - val_accuracy: 0.8016\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 9s 87ms/step - loss: 0.0330 - accuracy: 0.9914 - val_loss: 0.8970 - val_accuracy: 0.8069\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 9s 86ms/step - loss: 0.0434 - accuracy: 0.9870 - val_loss: 0.9298 - val_accuracy: 0.8201\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 9s 85ms/step - loss: 0.0212 - accuracy: 0.9941 - val_loss: 1.0352 - val_accuracy: 0.7989\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 9s 86ms/step - loss: 0.0601 - accuracy: 0.9798 - val_loss: 0.9245 - val_accuracy: 0.7804\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 9s 85ms/step - loss: 0.0161 - accuracy: 0.9980 - val_loss: 1.0569 - val_accuracy: 0.8069\n",
      "Epoch 22/100\n",
      "107/107 [==============================] - 9s 85ms/step - loss: 0.0076 - accuracy: 0.9993 - val_loss: 1.0883 - val_accuracy: 0.8122\n",
      "Epoch 23/100\n",
      "107/107 [==============================] - 9s 85ms/step - loss: 0.0371 - accuracy: 0.9881 - val_loss: 1.0400 - val_accuracy: 0.8069\n",
      "Epoch 24/100\n",
      "107/107 [==============================] - 9s 85ms/step - loss: 0.0227 - accuracy: 0.9933 - val_loss: 0.8201 - val_accuracy: 0.7884\n",
      "Epoch 25/100\n",
      "107/107 [==============================] - 9s 85ms/step - loss: 0.0268 - accuracy: 0.9933 - val_loss: 0.9063 - val_accuracy: 0.7989\n",
      "Epoch 26/100\n",
      "107/107 [==============================] - 9s 84ms/step - loss: 0.0173 - accuracy: 0.9953 - val_loss: 0.8448 - val_accuracy: 0.8201\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "Test Accuracy: 82.0105791091919\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 18s 101ms/step - loss: 0.6173 - accuracy: 0.6453 - val_loss: 0.4682 - val_accuracy: 0.7692\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 8s 75ms/step - loss: 0.4445 - accuracy: 0.7941 - val_loss: 0.4334 - val_accuracy: 0.8011\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 8s 73ms/step - loss: 0.4005 - accuracy: 0.8225 - val_loss: 0.4144 - val_accuracy: 0.8011\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 8s 73ms/step - loss: 0.3483 - accuracy: 0.8585 - val_loss: 0.4094 - val_accuracy: 0.7958\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 8s 74ms/step - loss: 0.3342 - accuracy: 0.8499 - val_loss: 0.4224 - val_accuracy: 0.8090\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 8s 74ms/step - loss: 0.3041 - accuracy: 0.8687 - val_loss: 0.4107 - val_accuracy: 0.7851\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.2713 - accuracy: 0.8839 - val_loss: 0.4176 - val_accuracy: 0.7851\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 8s 73ms/step - loss: 0.2611 - accuracy: 0.8914 - val_loss: 0.4312 - val_accuracy: 0.7851\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.1952 - accuracy: 0.9254 - val_loss: 0.4938 - val_accuracy: 0.7692\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.1843 - accuracy: 0.9336 - val_loss: 0.5464 - val_accuracy: 0.7878\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.1443 - accuracy: 0.9496 - val_loss: 0.5209 - val_accuracy: 0.7745\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 8s 73ms/step - loss: 0.1374 - accuracy: 0.9477 - val_loss: 0.5403 - val_accuracy: 0.8117\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 8s 73ms/step - loss: 0.1153 - accuracy: 0.9572 - val_loss: 0.5392 - val_accuracy: 0.7798\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 8s 73ms/step - loss: 0.0835 - accuracy: 0.9729 - val_loss: 0.6042 - val_accuracy: 0.7958\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.0647 - accuracy: 0.9801 - val_loss: 0.7091 - val_accuracy: 0.7798\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 8s 74ms/step - loss: 0.0487 - accuracy: 0.9876 - val_loss: 0.6936 - val_accuracy: 0.8011\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.0432 - accuracy: 0.9899 - val_loss: 0.7789 - val_accuracy: 0.8037\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.0921 - accuracy: 0.9669 - val_loss: 0.6426 - val_accuracy: 0.7825\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.0599 - accuracy: 0.9797 - val_loss: 0.6370 - val_accuracy: 0.7958\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 8s 72ms/step - loss: 0.0344 - accuracy: 0.9903 - val_loss: 0.6861 - val_accuracy: 0.7984\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 81.16710782051086\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 57s 432ms/step - loss: 0.6146 - accuracy: 0.6500 - val_loss: 0.4590 - val_accuracy: 0.7692\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 31s 289ms/step - loss: 0.4452 - accuracy: 0.7927 - val_loss: 0.4730 - val_accuracy: 0.7586\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 0.3988 - accuracy: 0.8209 - val_loss: 0.4064 - val_accuracy: 0.8249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "107/107 [==============================] - 23s 211ms/step - loss: 0.3691 - accuracy: 0.8312 - val_loss: 0.4026 - val_accuracy: 0.8196\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.3344 - accuracy: 0.8553 - val_loss: 0.4019 - val_accuracy: 0.8170\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.2993 - accuracy: 0.8805 - val_loss: 0.4162 - val_accuracy: 0.8170\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.2664 - accuracy: 0.8938 - val_loss: 0.4038 - val_accuracy: 0.8329\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 22s 208ms/step - loss: 0.2582 - accuracy: 0.8805 - val_loss: 0.4224 - val_accuracy: 0.7958\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.2185 - accuracy: 0.9067 - val_loss: 0.4536 - val_accuracy: 0.8408\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.1744 - accuracy: 0.9283 - val_loss: 0.4855 - val_accuracy: 0.8064\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.1718 - accuracy: 0.9313 - val_loss: 0.5984 - val_accuracy: 0.7851\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.1226 - accuracy: 0.9527 - val_loss: 0.5599 - val_accuracy: 0.8011\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.1093 - accuracy: 0.9556 - val_loss: 0.5932 - val_accuracy: 0.8037\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.0839 - accuracy: 0.9717 - val_loss: 0.6633 - val_accuracy: 0.7958\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 22s 206ms/step - loss: 0.0642 - accuracy: 0.9810 - val_loss: 0.6405 - val_accuracy: 0.7984\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.0877 - accuracy: 0.9711 - val_loss: 0.6523 - val_accuracy: 0.8064\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 22s 207ms/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.6814 - val_accuracy: 0.7905\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 84.08488035202026\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 37s 275ms/step - loss: 0.6258 - accuracy: 0.6343 - val_loss: 0.4918 - val_accuracy: 0.7613\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 24s 225ms/step - loss: 0.4392 - accuracy: 0.7915 - val_loss: 0.4570 - val_accuracy: 0.7905\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 18s 170ms/step - loss: 0.4103 - accuracy: 0.8151 - val_loss: 0.5291 - val_accuracy: 0.7162\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.3825 - accuracy: 0.8237 - val_loss: 0.4281 - val_accuracy: 0.8170\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.3277 - accuracy: 0.8586 - val_loss: 0.4477 - val_accuracy: 0.7825\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.3332 - accuracy: 0.8501 - val_loss: 0.4590 - val_accuracy: 0.7825\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.2798 - accuracy: 0.8774 - val_loss: 0.4660 - val_accuracy: 0.7666\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.2482 - accuracy: 0.8907 - val_loss: 0.4750 - val_accuracy: 0.7931\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.2289 - accuracy: 0.9046 - val_loss: 0.5974 - val_accuracy: 0.7427\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.1820 - accuracy: 0.9249 - val_loss: 0.4906 - val_accuracy: 0.7878\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.1621 - accuracy: 0.9356 - val_loss: 0.5514 - val_accuracy: 0.7719\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.1317 - accuracy: 0.9458 - val_loss: 0.5211 - val_accuracy: 0.7958\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 81.69761300086975\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 31s 215ms/step - loss: 0.6131 - accuracy: 0.6596 - val_loss: 0.4714 - val_accuracy: 0.7586\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 21s 194ms/step - loss: 0.4578 - accuracy: 0.7856 - val_loss: 0.4341 - val_accuracy: 0.7878\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 21s 194ms/step - loss: 0.3973 - accuracy: 0.8242 - val_loss: 0.4333 - val_accuracy: 0.7772\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 21s 196ms/step - loss: 0.3597 - accuracy: 0.8421 - val_loss: 0.4218 - val_accuracy: 0.7931\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 0.3365 - accuracy: 0.8637 - val_loss: 0.4179 - val_accuracy: 0.8037\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 0.3101 - accuracy: 0.8625 - val_loss: 0.4891 - val_accuracy: 0.7984\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 0.2913 - accuracy: 0.8699 - val_loss: 0.4385 - val_accuracy: 0.7851\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 21s 198ms/step - loss: 0.2271 - accuracy: 0.9092 - val_loss: 0.4451 - val_accuracy: 0.8011\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.2097 - accuracy: 0.9205 - val_loss: 0.4576 - val_accuracy: 0.8011\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.1680 - accuracy: 0.9303 - val_loss: 0.5045 - val_accuracy: 0.8064\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.1281 - accuracy: 0.9504 - val_loss: 0.6228 - val_accuracy: 0.7719\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 21s 198ms/step - loss: 0.1443 - accuracy: 0.9449 - val_loss: 0.5348 - val_accuracy: 0.7878\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 21s 200ms/step - loss: 0.0980 - accuracy: 0.9622 - val_loss: 0.6114 - val_accuracy: 0.7984\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.0962 - accuracy: 0.9704 - val_loss: 0.7403 - val_accuracy: 0.7772\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.0598 - accuracy: 0.9830 - val_loss: 0.7353 - val_accuracy: 0.7984\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.7679 - val_accuracy: 0.7666\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.0617 - accuracy: 0.9834 - val_loss: 0.7749 - val_accuracy: 0.7825\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 0.0257 - accuracy: 0.9945 - val_loss: 1.0019 - val_accuracy: 0.7798\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 25s 161ms/step - loss: 0.6260 - accuracy: 0.6491 - val_loss: 0.4932 - val_accuracy: 0.7586\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 14s 135ms/step - loss: 0.4509 - accuracy: 0.7896 - val_loss: 0.4556 - val_accuracy: 0.7825\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 13s 119ms/step - loss: 0.4046 - accuracy: 0.8143 - val_loss: 0.4716 - val_accuracy: 0.7533\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 0.3714 - accuracy: 0.8362 - val_loss: 0.4328 - val_accuracy: 0.7984\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 0.3520 - accuracy: 0.8435 - val_loss: 0.4264 - val_accuracy: 0.7984\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 0.3092 - accuracy: 0.8642 - val_loss: 0.5035 - val_accuracy: 0.7745\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 0.2785 - accuracy: 0.8769 - val_loss: 0.4587 - val_accuracy: 0.7692\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 12s 114ms/step - loss: 0.2296 - accuracy: 0.9081 - val_loss: 0.4788 - val_accuracy: 0.7798\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 0.1835 - accuracy: 0.9298 - val_loss: 0.5263 - val_accuracy: 0.7692\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 0.1646 - accuracy: 0.9314 - val_loss: 0.5920 - val_accuracy: 0.7666\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 0.1421 - accuracy: 0.9454 - val_loss: 0.5775 - val_accuracy: 0.7692\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 0.1220 - accuracy: 0.9549 - val_loss: 0.5963 - val_accuracy: 0.7931\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 79.84085083007812\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6      acc7  \\\n",
      "0  82.539684  81.74603  80.423278  82.539684  82.010579  81.167108  84.08488   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  81.697613  80.636603  79.840851  81.668631  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.539684</td>\n",
       "      <td>81.74603</td>\n",
       "      <td>80.423278</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>82.010579</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>84.08488</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>81.668631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1      acc2       acc3       acc4       acc5       acc6      acc7  \\\n",
       "0  82.539684  81.74603  80.423278  82.539684  82.010579  81.167108  84.08488   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  81.697613  80.636603  79.840851  81.668631  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_CR_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 90s 765ms/step - loss: 0.5959 - accuracy: 0.6804 - val_loss: 0.4723 - val_accuracy: 0.7513\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 82s 767ms/step - loss: 0.3288 - accuracy: 0.8641 - val_loss: 0.4127 - val_accuracy: 0.7910\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 78s 732ms/step - loss: 0.2030 - accuracy: 0.9251 - val_loss: 0.4565 - val_accuracy: 0.7884\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 78s 729ms/step - loss: 0.1138 - accuracy: 0.9594 - val_loss: 0.4777 - val_accuracy: 0.8228\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 77s 723ms/step - loss: 0.0540 - accuracy: 0.9831 - val_loss: 0.5978 - val_accuracy: 0.8016\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 78s 727ms/step - loss: 0.0304 - accuracy: 0.9906 - val_loss: 0.6869 - val_accuracy: 0.7884\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 77s 717ms/step - loss: 0.0218 - accuracy: 0.9958 - val_loss: 0.8191 - val_accuracy: 0.8095\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 75s 699ms/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 1.0101 - val_accuracy: 0.8042\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 74s 694ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.7964 - val_accuracy: 0.8069\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 76s 707ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.9981 - val_accuracy: 0.8069\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 77s 721ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 1.1379 - val_accuracy: 0.8148\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 77s 722ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 1.2117 - val_accuracy: 0.8069\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 82.27513432502747\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 93s 791ms/step - loss: 0.6041 - accuracy: 0.6531 - val_loss: 0.4252 - val_accuracy: 0.7989\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 75s 697ms/step - loss: 0.2924 - accuracy: 0.8830 - val_loss: 0.4465 - val_accuracy: 0.7884\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 70s 653ms/step - loss: 0.1992 - accuracy: 0.9269 - val_loss: 0.5246 - val_accuracy: 0.8042\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 70s 655ms/step - loss: 0.1000 - accuracy: 0.9668 - val_loss: 0.6392 - val_accuracy: 0.8095\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 70s 656ms/step - loss: 0.0658 - accuracy: 0.9801 - val_loss: 0.7111 - val_accuracy: 0.8201\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 71s 662ms/step - loss: 0.0297 - accuracy: 0.9897 - val_loss: 1.0004 - val_accuracy: 0.7751\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 71s 662ms/step - loss: 0.0207 - accuracy: 0.9942 - val_loss: 0.9432 - val_accuracy: 0.7884\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 70s 654ms/step - loss: 0.0175 - accuracy: 0.9935 - val_loss: 0.9348 - val_accuracy: 0.8095\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 70s 655ms/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 1.0021 - val_accuracy: 0.7751\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 70s 659ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.9036 - val_accuracy: 0.8095\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 70s 653ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 1.1229 - val_accuracy: 0.7884\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 70s 659ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 1.2281 - val_accuracy: 0.7963\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 70s 655ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 1.2365 - val_accuracy: 0.7963\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 82.0105791091919\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 81s 680ms/step - loss: 0.6004 - accuracy: 0.6584 - val_loss: 0.4311 - val_accuracy: 0.8175\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 61s 571ms/step - loss: 0.3397 - accuracy: 0.8603 - val_loss: 0.3886 - val_accuracy: 0.8201\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 60s 560ms/step - loss: 0.1866 - accuracy: 0.9371 - val_loss: 0.4341 - val_accuracy: 0.8122\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 61s 571ms/step - loss: 0.0979 - accuracy: 0.9697 - val_loss: 0.5543 - val_accuracy: 0.7831\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 61s 571ms/step - loss: 0.0674 - accuracy: 0.9770 - val_loss: 0.7253 - val_accuracy: 0.7884\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 61s 570ms/step - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.8208 - val_accuracy: 0.7910\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 60s 562ms/step - loss: 0.0272 - accuracy: 0.9914 - val_loss: 0.8317 - val_accuracy: 0.7910\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 59s 553ms/step - loss: 0.0122 - accuracy: 0.9952 - val_loss: 1.0352 - val_accuracy: 0.7963\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 60s 557ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 1.0637 - val_accuracy: 0.7989\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 59s 551ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 1.1527 - val_accuracy: 0.8042\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 82.0105791091919\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 61s 497ms/step - loss: 0.5915 - accuracy: 0.6558 - val_loss: 0.4033 - val_accuracy: 0.8360\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 51s 478ms/step - loss: 0.3221 - accuracy: 0.8617 - val_loss: 0.3834 - val_accuracy: 0.8122\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 37s 350ms/step - loss: 0.1891 - accuracy: 0.9279 - val_loss: 0.4034 - val_accuracy: 0.8095\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 0.1023 - accuracy: 0.9702 - val_loss: 0.5645 - val_accuracy: 0.7857\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 0.0580 - accuracy: 0.9797 - val_loss: 0.5713 - val_accuracy: 0.7910\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 0.0351 - accuracy: 0.9885 - val_loss: 0.5843 - val_accuracy: 0.7937\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 0.0262 - accuracy: 0.9946 - val_loss: 0.8121 - val_accuracy: 0.7593\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 38s 351ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.8026 - val_accuracy: 0.7698\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 38s 351ms/step - loss: 0.0178 - accuracy: 0.9947 - val_loss: 0.8420 - val_accuracy: 0.7751\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 83.59788656234741\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 94s 772ms/step - loss: 0.5985 - accuracy: 0.6626 - val_loss: 0.4459 - val_accuracy: 0.7751\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 65s 604ms/step - loss: 0.3258 - accuracy: 0.8681 - val_loss: 0.4045 - val_accuracy: 0.8201\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 54s 502ms/step - loss: 0.1772 - accuracy: 0.9360 - val_loss: 0.4519 - val_accuracy: 0.8069\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 54s 501ms/step - loss: 0.0987 - accuracy: 0.9717 - val_loss: 0.5404 - val_accuracy: 0.7963\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 53s 496ms/step - loss: 0.0550 - accuracy: 0.9831 - val_loss: 0.7026 - val_accuracy: 0.8122\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 53s 495ms/step - loss: 0.0368 - accuracy: 0.9893 - val_loss: 0.7219 - val_accuracy: 0.8122\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 53s 493ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.8066 - val_accuracy: 0.8307\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 53s 493ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 0.6959 - val_accuracy: 0.7989\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 52s 489ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.8894 - val_accuracy: 0.8175\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 52s 489ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 1.0335 - val_accuracy: 0.8175\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 52s 485ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 1.1099 - val_accuracy: 0.8175\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 55s 510ms/step - loss: 9.8190e-04 - accuracy: 0.9998 - val_loss: 1.1772 - val_accuracy: 0.8175\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 54s 507ms/step - loss: 7.6237e-04 - accuracy: 0.9999 - val_loss: 1.2214 - val_accuracy: 0.8122\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 52s 489ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.2496 - val_accuracy: 0.8069\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 52s 489ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.2657 - val_accuracy: 0.8069\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 83.06878209114075\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 61s 496ms/step - loss: 0.5993 - accuracy: 0.6698 - val_loss: 0.4282 - val_accuracy: 0.7851\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.3219 - accuracy: 0.8709 - val_loss: 0.4576 - val_accuracy: 0.7666\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 29s 268ms/step - loss: 0.1827 - accuracy: 0.9327 - val_loss: 0.4695 - val_accuracy: 0.8064\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 29s 269ms/step - loss: 0.1053 - accuracy: 0.9661 - val_loss: 0.5980 - val_accuracy: 0.7958\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 30s 277ms/step - loss: 0.0799 - accuracy: 0.9755 - val_loss: 0.7440 - val_accuracy: 0.7905\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 29s 271ms/step - loss: 0.0382 - accuracy: 0.9874 - val_loss: 0.7757 - val_accuracy: 0.8117\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 29s 270ms/step - loss: 0.0311 - accuracy: 0.9927 - val_loss: 0.7644 - val_accuracy: 0.8196\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 29s 270ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.8616 - val_accuracy: 0.8143\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 29s 269ms/step - loss: 0.0120 - accuracy: 0.9953 - val_loss: 0.9409 - val_accuracy: 0.7851\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 29s 271ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 1.0610 - val_accuracy: 0.7745\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 29s 269ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.9181 - val_accuracy: 0.7905\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 29s 269ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.8962 - val_accuracy: 0.7851\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 29s 268ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 1.2282 - val_accuracy: 0.7878\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 29s 267ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 1.3705 - val_accuracy: 0.7878\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 29s 268ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 1.3981 - val_accuracy: 0.7931\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 81.9628655910492\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 43s 326ms/step - loss: 0.6059 - accuracy: 0.6547 - val_loss: 0.4073 - val_accuracy: 0.8064\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 34s 318ms/step - loss: 0.3226 - accuracy: 0.8694 - val_loss: 0.4227 - val_accuracy: 0.7878\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 35s 327ms/step - loss: 0.1765 - accuracy: 0.9352 - val_loss: 0.5409 - val_accuracy: 0.7958\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 36s 333ms/step - loss: 0.0964 - accuracy: 0.9731 - val_loss: 0.5952 - val_accuracy: 0.7825\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 36s 336ms/step - loss: 0.0576 - accuracy: 0.9830 - val_loss: 0.8970 - val_accuracy: 0.7719\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 36s 335ms/step - loss: 0.0400 - accuracy: 0.9866 - val_loss: 0.8863 - val_accuracy: 0.7692\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 36s 341ms/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 0.8363 - val_accuracy: 0.7825\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 36s 340ms/step - loss: 0.0180 - accuracy: 0.9958 - val_loss: 1.1984 - val_accuracy: 0.7745\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 36s 341ms/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 1.1442 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 50s 390ms/step - loss: 0.6109 - accuracy: 0.6419 - val_loss: 0.4523 - val_accuracy: 0.7931\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 42s 389ms/step - loss: 0.3228 - accuracy: 0.8665 - val_loss: 0.4220 - val_accuracy: 0.8037\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.1814 - accuracy: 0.9346 - val_loss: 0.4660 - val_accuracy: 0.7984\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 44s 411ms/step - loss: 0.0910 - accuracy: 0.9733 - val_loss: 0.5520 - val_accuracy: 0.7984\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 44s 414ms/step - loss: 0.0524 - accuracy: 0.9835 - val_loss: 0.6258 - val_accuracy: 0.8037\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 46s 434ms/step - loss: 0.0358 - accuracy: 0.9897 - val_loss: 0.7851 - val_accuracy: 0.7931\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0198 - accuracy: 0.9955 - val_loss: 0.9569 - val_accuracy: 0.7798\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 46s 434ms/step - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.7801 - val_accuracy: 0.7798\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.0261 - accuracy: 0.9938 - val_loss: 1.1293 - val_accuracy: 0.7931\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 46s 434ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.8675 - val_accuracy: 0.8143\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 1.1328 - val_accuracy: 0.8037\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 1.1293 - val_accuracy: 0.8037\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0228 - accuracy: 0.9951 - val_loss: 1.0343 - val_accuracy: 0.8090\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.9277 - val_accuracy: 0.8143\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 46s 431ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 1.1658 - val_accuracy: 0.8117\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 1.2554 - val_accuracy: 0.8090\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 46s 432ms/step - loss: 9.4967e-04 - accuracy: 0.9997 - val_loss: 1.3008 - val_accuracy: 0.8064\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 1.3578 - val_accuracy: 0.8064\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 81s 646ms/step - loss: 0.5968 - accuracy: 0.6572 - val_loss: 0.4077 - val_accuracy: 0.8011\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 59s 556ms/step - loss: 0.2971 - accuracy: 0.8718 - val_loss: 0.3954 - val_accuracy: 0.8143\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 51s 472ms/step - loss: 0.1759 - accuracy: 0.9394 - val_loss: 0.4670 - val_accuracy: 0.7958\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 50s 464ms/step - loss: 0.1187 - accuracy: 0.9590 - val_loss: 0.6247 - val_accuracy: 0.7798\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 51s 472ms/step - loss: 0.0574 - accuracy: 0.9788 - val_loss: 0.6390 - val_accuracy: 0.8064\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 49s 461ms/step - loss: 0.0352 - accuracy: 0.9908 - val_loss: 0.6776 - val_accuracy: 0.8037\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 49s 458ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.8977 - val_accuracy: 0.7745\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 49s 457ms/step - loss: 0.0106 - accuracy: 0.9956 - val_loss: 1.0945 - val_accuracy: 0.7719\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 49s 459ms/step - loss: 0.0105 - accuracy: 0.9959 - val_loss: 0.9957 - val_accuracy: 0.7772\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 49s 454ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.9583 - val_accuracy: 0.7798\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.43236041069031\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 75s 624ms/step - loss: 0.5847 - accuracy: 0.6720 - val_loss: 0.4855 - val_accuracy: 0.7666\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 46s 433ms/step - loss: 0.3334 - accuracy: 0.8580 - val_loss: 0.4443 - val_accuracy: 0.7719\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 37s 345ms/step - loss: 0.1734 - accuracy: 0.9415 - val_loss: 0.4703 - val_accuracy: 0.7984\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 37s 347ms/step - loss: 0.1059 - accuracy: 0.9697 - val_loss: 0.6077 - val_accuracy: 0.7851\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 37s 348ms/step - loss: 0.0523 - accuracy: 0.9841 - val_loss: 0.6402 - val_accuracy: 0.7772\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 37s 346ms/step - loss: 0.0344 - accuracy: 0.9895 - val_loss: 0.8523 - val_accuracy: 0.7825\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 37s 348ms/step - loss: 0.0161 - accuracy: 0.9953 - val_loss: 0.9546 - val_accuracy: 0.7745\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 37s 349ms/step - loss: 0.0378 - accuracy: 0.9869 - val_loss: 0.9479 - val_accuracy: 0.7825\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 37s 349ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 1.0392 - val_accuracy: 0.7798\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 37s 347ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 1.3081 - val_accuracy: 0.7719\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 37s 347ms/step - loss: 0.0034 - accuracy: 0.9983 - val_loss: 1.3600 - val_accuracy: 0.7825\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 79.84085083007812\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c58a57bfe936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mrecord3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'record' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.275134</td>\n",
       "      <td>82.010579</td>\n",
       "      <td>82.010579</td>\n",
       "      <td>83.597887</td>\n",
       "      <td>83.068782</td>\n",
       "      <td>81.962866</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>81.43236</td>\n",
       "      <td>81.43236</td>\n",
       "      <td>79.840851</td>\n",
       "      <td>81.8268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  82.275134  82.010579  82.010579  83.597887  83.068782  81.962866   \n",
       "\n",
       "        acc7      acc8      acc9      acc10      AVG  \n",
       "0  80.636603  81.43236  81.43236  79.840851  81.8268  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_CR_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
