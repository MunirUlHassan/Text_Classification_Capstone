{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with MPQA Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10606, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaining</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failing to support</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desperately needs</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many years of decay</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no quick fix</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>urged</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>hope</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10606 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sentence  label  split\n",
       "0              complaining      0  train\n",
       "1       failing to support      0  train\n",
       "2        desperately needs      0  train\n",
       "3      many years of decay      0  train\n",
       "4             no quick fix      0  train\n",
       "...                    ...    ...    ...\n",
       "10601                urged      1  train\n",
       "10602       strictly abide      1  train\n",
       "10603                 hope      1  train\n",
       "10604       strictly abide      1  train\n",
       "10605                           1  train\n",
       "\n",
       "[10606 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MPQA/MPQA.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10606 non-null  object\n",
      " 1   label     10606 non-null  int32 \n",
      " 2   split     10606 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 207.3+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3312</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          7294   7294\n",
       "1          3312   3312"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complaining'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  no quick fix\n",
      "Into a sequence of int: [25, 945, 1476]\n",
      "Into a padded sequence: [  25  945 1476    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "of 3\n",
      "to 4\n",
      "a 5\n",
      "and 6\n",
      "not 7\n",
      "is 8\n",
      "in 9\n",
      "be 10\n",
      "6236\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 29s 67ms/step - loss: 0.5662 - accuracy: 0.7385 - val_loss: 0.3534 - val_accuracy: 0.8633\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.2214 - accuracy: 0.9263 - val_loss: 0.3910 - val_accuracy: 0.8596\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.1346 - accuracy: 0.9540 - val_loss: 0.4198 - val_accuracy: 0.8520\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.1008 - accuracy: 0.9631 - val_loss: 0.4683 - val_accuracy: 0.8549\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.0902 - accuracy: 0.9663 - val_loss: 0.5558 - val_accuracy: 0.8473\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.0783 - accuracy: 0.9686 - val_loss: 0.5774 - val_accuracy: 0.8483\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 86.33365035057068\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 33s 77ms/step - loss: 0.5636 - accuracy: 0.7337 - val_loss: 0.3820 - val_accuracy: 0.8332\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1997 - accuracy: 0.9319 - val_loss: 0.4187 - val_accuracy: 0.8247\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1309 - accuracy: 0.9562 - val_loss: 0.4804 - val_accuracy: 0.8134\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1114 - accuracy: 0.9610 - val_loss: 0.5370 - val_accuracy: 0.8002\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0849 - accuracy: 0.9675 - val_loss: 0.5820 - val_accuracy: 0.8087\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0645 - accuracy: 0.9749 - val_loss: 0.6296 - val_accuracy: 0.8332\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 83.31762552261353\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 30s 70ms/step - loss: 0.5687 - accuracy: 0.7336 - val_loss: 0.3791 - val_accuracy: 0.8549\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 16s 55ms/step - loss: 0.2039 - accuracy: 0.9321 - val_loss: 0.3946 - val_accuracy: 0.8483\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 17s 55ms/step - loss: 0.1357 - accuracy: 0.9533 - val_loss: 0.4495 - val_accuracy: 0.8407\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 16s 55ms/step - loss: 0.1079 - accuracy: 0.9630 - val_loss: 0.5069 - val_accuracy: 0.8285\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.0900 - accuracy: 0.9656 - val_loss: 0.5742 - val_accuracy: 0.8294\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 16s 55ms/step - loss: 0.0779 - accuracy: 0.9654 - val_loss: 0.5979 - val_accuracy: 0.8360\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 28s 65ms/step - loss: 0.5633 - accuracy: 0.7424 - val_loss: 0.3647 - val_accuracy: 0.8445\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 22s 72ms/step - loss: 0.2004 - accuracy: 0.9324 - val_loss: 0.3882 - val_accuracy: 0.8388\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.1339 - accuracy: 0.9525 - val_loss: 0.4464 - val_accuracy: 0.8417\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.0999 - accuracy: 0.9637 - val_loss: 0.4691 - val_accuracy: 0.8464\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0863 - accuracy: 0.9689 - val_loss: 0.5376 - val_accuracy: 0.8435\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.0667 - accuracy: 0.9745 - val_loss: 0.6070 - val_accuracy: 0.8407\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0520 - accuracy: 0.9766 - val_loss: 0.6397 - val_accuracy: 0.8369\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0471 - accuracy: 0.9821 - val_loss: 0.7024 - val_accuracy: 0.8464\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 0.7646 - val_accuracy: 0.8322\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 84.63713526725769\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 40s 101ms/step - loss: 0.5675 - accuracy: 0.7309 - val_loss: 0.3309 - val_accuracy: 0.8784\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.2093 - accuracy: 0.9305 - val_loss: 0.3200 - val_accuracy: 0.8812\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 19s 65ms/step - loss: 0.1310 - accuracy: 0.9538 - val_loss: 0.3552 - val_accuracy: 0.8775\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0993 - accuracy: 0.9670 - val_loss: 0.3789 - val_accuracy: 0.8662\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0945 - accuracy: 0.9627 - val_loss: 0.4276 - val_accuracy: 0.8690\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0784 - accuracy: 0.9680 - val_loss: 0.4862 - val_accuracy: 0.8728\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0578 - accuracy: 0.9755 - val_loss: 0.5100 - val_accuracy: 0.8624\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.12441229820251\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 39s 100ms/step - loss: 0.5663 - accuracy: 0.7301 - val_loss: 0.3460 - val_accuracy: 0.8680\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.2064 - accuracy: 0.9309 - val_loss: 0.3592 - val_accuracy: 0.8341\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1426 - accuracy: 0.9522 - val_loss: 0.4011 - val_accuracy: 0.8228\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1072 - accuracy: 0.9616 - val_loss: 0.4300 - val_accuracy: 0.8238\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0896 - accuracy: 0.9642 - val_loss: 0.4745 - val_accuracy: 0.8275\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0730 - accuracy: 0.9701 - val_loss: 0.4872 - val_accuracy: 0.8615\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 86.80490255355835\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 39s 100ms/step - loss: 0.5670 - accuracy: 0.7353 - val_loss: 0.3629 - val_accuracy: 0.8604\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.2022 - accuracy: 0.9323 - val_loss: 0.3693 - val_accuracy: 0.8642\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.1323 - accuracy: 0.9549 - val_loss: 0.3973 - val_accuracy: 0.8575\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 27s 92ms/step - loss: 0.1018 - accuracy: 0.9638 - val_loss: 0.4245 - val_accuracy: 0.8434\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0870 - accuracy: 0.9657 - val_loss: 0.4585 - val_accuracy: 0.8462\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0678 - accuracy: 0.9739 - val_loss: 0.5233 - val_accuracy: 0.8509\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 27s 92ms/step - loss: 0.0595 - accuracy: 0.9754 - val_loss: 0.5847 - val_accuracy: 0.8481\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.41509413719177\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 41s 109ms/step - loss: 0.5669 - accuracy: 0.7376 - val_loss: 0.3532 - val_accuracy: 0.8538\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.2184 - accuracy: 0.9255 - val_loss: 0.3655 - val_accuracy: 0.8698\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.1416 - accuracy: 0.9536 - val_loss: 0.3875 - val_accuracy: 0.8651\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.1104 - accuracy: 0.9611 - val_loss: 0.4061 - val_accuracy: 0.8736\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0928 - accuracy: 0.9650 - val_loss: 0.4286 - val_accuracy: 0.8660\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 28s 94ms/step - loss: 0.0691 - accuracy: 0.9714 - val_loss: 0.4813 - val_accuracy: 0.8651\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.0616 - accuracy: 0.9743 - val_loss: 0.5056 - val_accuracy: 0.8717\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0439 - accuracy: 0.9822 - val_loss: 0.5576 - val_accuracy: 0.8670\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0433 - accuracy: 0.9822 - val_loss: 0.6265 - val_accuracy: 0.8642\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 87.35849261283875\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 41s 105ms/step - loss: 0.5653 - accuracy: 0.7237 - val_loss: 0.3523 - val_accuracy: 0.8642\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.2200 - accuracy: 0.9223 - val_loss: 0.3501 - val_accuracy: 0.8679\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 30s 100ms/step - loss: 0.1417 - accuracy: 0.9469 - val_loss: 0.3947 - val_accuracy: 0.8613\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1085 - accuracy: 0.9596 - val_loss: 0.4206 - val_accuracy: 0.8623\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.0906 - accuracy: 0.9643 - val_loss: 0.4518 - val_accuracy: 0.8575\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0687 - accuracy: 0.9693 - val_loss: 0.5109 - val_accuracy: 0.8566\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.0570 - accuracy: 0.9765 - val_loss: 0.5209 - val_accuracy: 0.8613\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.79245114326477\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 37s 92ms/step - loss: 0.5674 - accuracy: 0.7404 - val_loss: 0.3565 - val_accuracy: 0.8642\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.2052 - accuracy: 0.9297 - val_loss: 0.3577 - val_accuracy: 0.8745\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.4013 - val_accuracy: 0.8670\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.1105 - accuracy: 0.9585 - val_loss: 0.4464 - val_accuracy: 0.8519\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.0931 - accuracy: 0.9641 - val_loss: 0.4821 - val_accuracy: 0.8453\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.0712 - accuracy: 0.9715 - val_loss: 0.5261 - val_accuracy: 0.8557\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.0548 - accuracy: 0.9775 - val_loss: 0.5424 - val_accuracy: 0.8491\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.45282888412476\n",
      "\n",
      "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  86.33365  83.317626  85.485393  84.637135  88.124412  86.804903  86.415094   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  87.358493  86.792451  87.452829  86.272199  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.33365</td>\n",
       "      <td>83.317626</td>\n",
       "      <td>85.485393</td>\n",
       "      <td>84.637135</td>\n",
       "      <td>88.124412</td>\n",
       "      <td>86.804903</td>\n",
       "      <td>86.415094</td>\n",
       "      <td>87.358493</td>\n",
       "      <td>86.792451</td>\n",
       "      <td>87.452829</td>\n",
       "      <td>86.272199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  86.33365  83.317626  85.485393  84.637135  88.124412  86.804903  86.415094   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  87.358493  86.792451  87.452829  86.272199  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_MPQA.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6083 words present from 6236 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 23s 49ms/step - loss: 0.4740 - accuracy: 0.7748 - val_loss: 0.3101 - val_accuracy: 0.8775\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2814 - accuracy: 0.8904 - val_loss: 0.3037 - val_accuracy: 0.8784\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2606 - accuracy: 0.9024 - val_loss: 0.2937 - val_accuracy: 0.8803\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2294 - accuracy: 0.9115 - val_loss: 0.2953 - val_accuracy: 0.8803\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2249 - accuracy: 0.9163 - val_loss: 0.2936 - val_accuracy: 0.8822\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2125 - accuracy: 0.9198 - val_loss: 0.3086 - val_accuracy: 0.8709\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1931 - accuracy: 0.9268 - val_loss: 0.3187 - val_accuracy: 0.8784\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1788 - accuracy: 0.9288 - val_loss: 0.3134 - val_accuracy: 0.8803\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1602 - accuracy: 0.9404 - val_loss: 0.3157 - val_accuracy: 0.8822\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1447 - accuracy: 0.9477 - val_loss: 0.3388 - val_accuracy: 0.8860\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1336 - accuracy: 0.9467 - val_loss: 0.3452 - val_accuracy: 0.8718\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1178 - accuracy: 0.9593 - val_loss: 0.3698 - val_accuracy: 0.8756\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1137 - accuracy: 0.9579 - val_loss: 0.3587 - val_accuracy: 0.8803\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1032 - accuracy: 0.9620 - val_loss: 0.3883 - val_accuracy: 0.8737\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0902 - accuracy: 0.9682 - val_loss: 0.4052 - val_accuracy: 0.8718\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0966 - accuracy: 0.9642 - val_loss: 0.4159 - val_accuracy: 0.8746\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0858 - accuracy: 0.9687 - val_loss: 0.4073 - val_accuracy: 0.8718\n",
      "Epoch 18/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0872 - accuracy: 0.9689 - val_loss: 0.4051 - val_accuracy: 0.8680\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 88.59566450119019\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 22s 47ms/step - loss: 0.4585 - accuracy: 0.7915 - val_loss: 0.3213 - val_accuracy: 0.8718\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2861 - accuracy: 0.8887 - val_loss: 0.3034 - val_accuracy: 0.8822\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2607 - accuracy: 0.8999 - val_loss: 0.3017 - val_accuracy: 0.8831\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2455 - accuracy: 0.9067 - val_loss: 0.3027 - val_accuracy: 0.8794\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2271 - accuracy: 0.9150 - val_loss: 0.3055 - val_accuracy: 0.8878\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2123 - accuracy: 0.9158 - val_loss: 0.3076 - val_accuracy: 0.8831\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1938 - accuracy: 0.9211 - val_loss: 0.3326 - val_accuracy: 0.8841\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1812 - accuracy: 0.9263 - val_loss: 0.3294 - val_accuracy: 0.8746\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1616 - accuracy: 0.9388 - val_loss: 0.3321 - val_accuracy: 0.8822\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1478 - accuracy: 0.9455 - val_loss: 0.3655 - val_accuracy: 0.8831\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1374 - accuracy: 0.9501 - val_loss: 0.3729 - val_accuracy: 0.8860\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1102 - accuracy: 0.9582 - val_loss: 0.4343 - val_accuracy: 0.8662\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1194 - accuracy: 0.9535 - val_loss: 0.4096 - val_accuracy: 0.8869\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 88.78416419029236\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 33s 72ms/step - loss: 0.4610 - accuracy: 0.7860 - val_loss: 0.2913 - val_accuracy: 0.8907\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 23s 77ms/step - loss: 0.2949 - accuracy: 0.8812 - val_loss: 0.2847 - val_accuracy: 0.8888\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 23s 77ms/step - loss: 0.2703 - accuracy: 0.8965 - val_loss: 0.2777 - val_accuracy: 0.8869\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 23s 76ms/step - loss: 0.2531 - accuracy: 0.9078 - val_loss: 0.2823 - val_accuracy: 0.8869\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 22s 74ms/step - loss: 0.2323 - accuracy: 0.9108 - val_loss: 0.2814 - val_accuracy: 0.8907\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 22s 74ms/step - loss: 0.2121 - accuracy: 0.9182 - val_loss: 0.2804 - val_accuracy: 0.8944\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 22s 73ms/step - loss: 0.1881 - accuracy: 0.9288 - val_loss: 0.2862 - val_accuracy: 0.8926\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 22s 73ms/step - loss: 0.1760 - accuracy: 0.9341 - val_loss: 0.3028 - val_accuracy: 0.8850\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 22s 72ms/step - loss: 0.1625 - accuracy: 0.9361 - val_loss: 0.3102 - val_accuracy: 0.8878\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.1481 - accuracy: 0.9430 - val_loss: 0.3126 - val_accuracy: 0.8850\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.1336 - accuracy: 0.9455 - val_loss: 0.3234 - val_accuracy: 0.8794\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.1213 - accuracy: 0.9557 - val_loss: 0.3595 - val_accuracy: 0.8775\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.1152 - accuracy: 0.9582 - val_loss: 0.3680 - val_accuracy: 0.8850\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.0988 - accuracy: 0.9642 - val_loss: 0.3630 - val_accuracy: 0.8907\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 89.44392204284668\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 30s 67ms/step - loss: 0.4606 - accuracy: 0.7863 - val_loss: 0.3309 - val_accuracy: 0.8605\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.2853 - accuracy: 0.8901 - val_loss: 0.3115 - val_accuracy: 0.8680\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 18s 61ms/step - loss: 0.2566 - accuracy: 0.9015 - val_loss: 0.3058 - val_accuracy: 0.8718\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.2363 - accuracy: 0.9085 - val_loss: 0.3009 - val_accuracy: 0.8756\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.2279 - accuracy: 0.9121 - val_loss: 0.3048 - val_accuracy: 0.8699\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 18s 60ms/step - loss: 0.2036 - accuracy: 0.9222 - val_loss: 0.3024 - val_accuracy: 0.8737\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1837 - accuracy: 0.9306 - val_loss: 0.3270 - val_accuracy: 0.8586\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1710 - accuracy: 0.9341 - val_loss: 0.3205 - val_accuracy: 0.8775\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 17s 57ms/step - loss: 0.1530 - accuracy: 0.9457 - val_loss: 0.3204 - val_accuracy: 0.8746\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.1449 - accuracy: 0.9471 - val_loss: 0.3369 - val_accuracy: 0.8728\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.1306 - accuracy: 0.9524 - val_loss: 0.3436 - val_accuracy: 0.8803\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.1247 - accuracy: 0.9535 - val_loss: 0.3787 - val_accuracy: 0.8794\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.1090 - accuracy: 0.9625 - val_loss: 0.3725 - val_accuracy: 0.8822\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.1050 - accuracy: 0.9624 - val_loss: 0.4018 - val_accuracy: 0.8784\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.1030 - accuracy: 0.9596 - val_loss: 0.4103 - val_accuracy: 0.8841\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.0899 - accuracy: 0.9667 - val_loss: 0.4450 - val_accuracy: 0.8784\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0842 - accuracy: 0.9661 - val_loss: 0.4780 - val_accuracy: 0.8784\n",
      "Epoch 18/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0717 - accuracy: 0.9730 - val_loss: 0.5003 - val_accuracy: 0.8841\n",
      "Epoch 19/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.5548 - val_accuracy: 0.8841\n",
      "Epoch 20/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0663 - accuracy: 0.9771 - val_loss: 0.5061 - val_accuracy: 0.8775\n",
      "Epoch 21/100\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.0684 - accuracy: 0.9754 - val_loss: 0.5175 - val_accuracy: 0.8746\n",
      "Epoch 22/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0627 - accuracy: 0.9769 - val_loss: 0.5448 - val_accuracy: 0.8728\n",
      "Epoch 23/100\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0620 - accuracy: 0.9767 - val_loss: 0.5200 - val_accuracy: 0.8596\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Test Accuracy: 88.40716481208801\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 25s 56ms/step - loss: 0.4584 - accuracy: 0.7850 - val_loss: 0.3514 - val_accuracy: 0.8567\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.2838 - accuracy: 0.8952 - val_loss: 0.3353 - val_accuracy: 0.8558\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 13s 45ms/step - loss: 0.2648 - accuracy: 0.8948 - val_loss: 0.3217 - val_accuracy: 0.8633\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 13s 45ms/step - loss: 0.2517 - accuracy: 0.9047 - val_loss: 0.3143 - val_accuracy: 0.8652\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.2257 - accuracy: 0.9170 - val_loss: 0.3136 - val_accuracy: 0.8709\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1988 - accuracy: 0.9249 - val_loss: 0.3240 - val_accuracy: 0.8728\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 13s 45ms/step - loss: 0.1912 - accuracy: 0.9277 - val_loss: 0.3248 - val_accuracy: 0.8671\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 14s 46ms/step - loss: 0.1911 - accuracy: 0.9274 - val_loss: 0.3305 - val_accuracy: 0.8737\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 14s 46ms/step - loss: 0.1564 - accuracy: 0.9442 - val_loss: 0.3369 - val_accuracy: 0.8690\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1558 - accuracy: 0.9422 - val_loss: 0.3342 - val_accuracy: 0.8728\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 13s 45ms/step - loss: 0.1343 - accuracy: 0.9480 - val_loss: 0.3486 - val_accuracy: 0.8728\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1260 - accuracy: 0.9490 - val_loss: 0.3621 - val_accuracy: 0.8756\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 13s 43ms/step - loss: 0.1168 - accuracy: 0.9572 - val_loss: 0.4022 - val_accuracy: 0.8680\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 14s 47ms/step - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.4159 - val_accuracy: 0.8643\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.4396 - val_accuracy: 0.8671\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0857 - accuracy: 0.9706 - val_loss: 0.4156 - val_accuracy: 0.8737\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0803 - accuracy: 0.9699 - val_loss: 0.4611 - val_accuracy: 0.8690\n",
      "Epoch 18/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0775 - accuracy: 0.9723 - val_loss: 0.4613 - val_accuracy: 0.8633\n",
      "Epoch 19/100\n",
      "299/299 [==============================] - 13s 43ms/step - loss: 0.0671 - accuracy: 0.9759 - val_loss: 0.4903 - val_accuracy: 0.8728\n",
      "Epoch 20/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0685 - accuracy: 0.9732 - val_loss: 0.5224 - val_accuracy: 0.8803\n",
      "Epoch 21/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.4943 - val_accuracy: 0.8718\n",
      "Epoch 22/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0579 - accuracy: 0.9796 - val_loss: 0.5238 - val_accuracy: 0.8671\n",
      "Epoch 23/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0612 - accuracy: 0.9782 - val_loss: 0.4817 - val_accuracy: 0.8633\n",
      "Epoch 24/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0635 - accuracy: 0.9780 - val_loss: 0.5497 - val_accuracy: 0.8765\n",
      "Epoch 25/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0592 - accuracy: 0.9781 - val_loss: 0.5801 - val_accuracy: 0.8699\n",
      "Epoch 26/100\n",
      "299/299 [==============================] - 13s 44ms/step - loss: 0.0548 - accuracy: 0.9794 - val_loss: 0.5608 - val_accuracy: 0.8728\n",
      "Epoch 27/100\n",
      "299/299 [==============================] - 13s 45ms/step - loss: 0.0525 - accuracy: 0.9782 - val_loss: 0.5924 - val_accuracy: 0.8680\n",
      "Epoch 28/100\n",
      "299/299 [==============================] - 13s 43ms/step - loss: 0.0510 - accuracy: 0.9796 - val_loss: 0.6145 - val_accuracy: 0.8784\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 88.03015947341919\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 23s 47ms/step - loss: 0.4643 - accuracy: 0.7847 - val_loss: 0.3128 - val_accuracy: 0.8765\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2909 - accuracy: 0.8888 - val_loss: 0.2944 - val_accuracy: 0.8775\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 12s 39ms/step - loss: 0.2657 - accuracy: 0.8960 - val_loss: 0.2796 - val_accuracy: 0.8888\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.2470 - accuracy: 0.9026 - val_loss: 0.2794 - val_accuracy: 0.8878\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.2164 - accuracy: 0.9182 - val_loss: 0.2772 - val_accuracy: 0.8841\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.2145 - accuracy: 0.9176 - val_loss: 0.2859 - val_accuracy: 0.8841\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1892 - accuracy: 0.9281 - val_loss: 0.3008 - val_accuracy: 0.8728\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1765 - accuracy: 0.9343 - val_loss: 0.3009 - val_accuracy: 0.8850\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1558 - accuracy: 0.9398 - val_loss: 0.3093 - val_accuracy: 0.8794\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1483 - accuracy: 0.9460 - val_loss: 0.3118 - val_accuracy: 0.8841\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1473 - accuracy: 0.9422 - val_loss: 0.3198 - val_accuracy: 0.8737\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.87841701507568\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 43s 116ms/step - loss: 0.4633 - accuracy: 0.7826 - val_loss: 0.3220 - val_accuracy: 0.8764\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 33s 111ms/step - loss: 0.2904 - accuracy: 0.8840 - val_loss: 0.3029 - val_accuracy: 0.8925\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2492 - accuracy: 0.9004 - val_loss: 0.3103 - val_accuracy: 0.8849\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2462 - accuracy: 0.9063 - val_loss: 0.2915 - val_accuracy: 0.8953\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.2233 - accuracy: 0.9138 - val_loss: 0.2938 - val_accuracy: 0.8934\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.2105 - accuracy: 0.9189 - val_loss: 0.2994 - val_accuracy: 0.8906\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1964 - accuracy: 0.9247 - val_loss: 0.2948 - val_accuracy: 0.8981\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.1745 - accuracy: 0.9300 - val_loss: 0.3090 - val_accuracy: 0.8877\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 30s 102ms/step - loss: 0.1675 - accuracy: 0.9339 - val_loss: 0.3237 - val_accuracy: 0.8868\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1431 - accuracy: 0.9451 - val_loss: 0.3345 - val_accuracy: 0.8896\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1314 - accuracy: 0.9455 - val_loss: 0.3479 - val_accuracy: 0.8906\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1260 - accuracy: 0.9496 - val_loss: 0.3815 - val_accuracy: 0.8821\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1128 - accuracy: 0.9560 - val_loss: 0.3808 - val_accuracy: 0.8821\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.0911 - accuracy: 0.9663 - val_loss: 0.4309 - val_accuracy: 0.8774\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.0982 - accuracy: 0.9637 - val_loss: 0.4271 - val_accuracy: 0.8736\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 89.81131911277771\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 39s 103ms/step - loss: 0.4663 - accuracy: 0.7767 - val_loss: 0.3368 - val_accuracy: 0.8660\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.2909 - accuracy: 0.8879 - val_loss: 0.3269 - val_accuracy: 0.8726\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.2651 - accuracy: 0.9032 - val_loss: 0.3145 - val_accuracy: 0.8679\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 30s 100ms/step - loss: 0.2395 - accuracy: 0.9072 - val_loss: 0.3146 - val_accuracy: 0.8698\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2179 - accuracy: 0.9193 - val_loss: 0.3141 - val_accuracy: 0.8698\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2091 - accuracy: 0.9182 - val_loss: 0.3308 - val_accuracy: 0.8585\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2023 - accuracy: 0.9181 - val_loss: 0.3240 - val_accuracy: 0.8698\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.1789 - accuracy: 0.9316 - val_loss: 0.3352 - val_accuracy: 0.8632\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1660 - accuracy: 0.9393 - val_loss: 0.3423 - val_accuracy: 0.8651\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 30s 100ms/step - loss: 0.1390 - accuracy: 0.9458 - val_loss: 0.3395 - val_accuracy: 0.8679\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 87.26415038108826\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 41s 109ms/step - loss: 0.4631 - accuracy: 0.7809 - val_loss: 0.3558 - val_accuracy: 0.8651\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.2860 - accuracy: 0.8899 - val_loss: 0.3396 - val_accuracy: 0.8698\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.2603 - accuracy: 0.8996 - val_loss: 0.3334 - val_accuracy: 0.8670\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.2398 - accuracy: 0.9094 - val_loss: 0.3369 - val_accuracy: 0.8670\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.2198 - accuracy: 0.9149 - val_loss: 0.3395 - val_accuracy: 0.8651\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.2041 - accuracy: 0.9225 - val_loss: 0.3535 - val_accuracy: 0.8670\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1798 - accuracy: 0.9286 - val_loss: 0.3558 - val_accuracy: 0.8623\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1617 - accuracy: 0.9399 - val_loss: 0.3765 - val_accuracy: 0.8698\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1615 - accuracy: 0.9357 - val_loss: 0.3933 - val_accuracy: 0.8689\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.1471 - accuracy: 0.9440 - val_loss: 0.4059 - val_accuracy: 0.8670\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 86.98112964630127\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 35s 90ms/step - loss: 0.4536 - accuracy: 0.7869 - val_loss: 0.3836 - val_accuracy: 0.8500\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.2875 - accuracy: 0.8907 - val_loss: 0.3726 - val_accuracy: 0.8528\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.2440 - accuracy: 0.9075 - val_loss: 0.3631 - val_accuracy: 0.8538\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.2309 - accuracy: 0.9116 - val_loss: 0.3650 - val_accuracy: 0.8557\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.2125 - accuracy: 0.9161 - val_loss: 0.3755 - val_accuracy: 0.8585\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.1977 - accuracy: 0.9218 - val_loss: 0.3576 - val_accuracy: 0.8528\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.1892 - accuracy: 0.9287 - val_loss: 0.3671 - val_accuracy: 0.8575\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1619 - accuracy: 0.9396 - val_loss: 0.3722 - val_accuracy: 0.8538\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1483 - accuracy: 0.9428 - val_loss: 0.3885 - val_accuracy: 0.8509\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1437 - accuracy: 0.9448 - val_loss: 0.4154 - val_accuracy: 0.8481\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1179 - accuracy: 0.9531 - val_loss: 0.4318 - val_accuracy: 0.8557\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1139 - accuracy: 0.9548 - val_loss: 0.4301 - val_accuracy: 0.8538\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1053 - accuracy: 0.9605 - val_loss: 0.4631 - val_accuracy: 0.8472\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 85.84905862808228\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  88.595665  88.784164  89.443922  88.407165  88.030159  88.878417   \n",
      "\n",
      "        acc7      acc8      acc9      acc10        AVG  \n",
      "0  89.811319  87.26415  86.98113  85.849059  88.204515  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.595665</td>\n",
       "      <td>88.784164</td>\n",
       "      <td>89.443922</td>\n",
       "      <td>88.407165</td>\n",
       "      <td>88.030159</td>\n",
       "      <td>88.878417</td>\n",
       "      <td>89.811319</td>\n",
       "      <td>87.26415</td>\n",
       "      <td>86.98113</td>\n",
       "      <td>85.849059</td>\n",
       "      <td>88.204515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  88.595665  88.784164  89.443922  88.407165  88.030159  88.878417   \n",
       "\n",
       "        acc7      acc8      acc9      acc10        AVG  \n",
       "0  89.811319  87.26415  86.98113  85.849059  88.204515  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_MPQA_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 77s 230ms/step - loss: 0.4543 - accuracy: 0.7852 - val_loss: 0.2949 - val_accuracy: 0.8888\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 65s 218ms/step - loss: 0.1923 - accuracy: 0.9319 - val_loss: 0.3034 - val_accuracy: 0.8841\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 64s 213ms/step - loss: 0.1256 - accuracy: 0.9554 - val_loss: 0.3528 - val_accuracy: 0.8775\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 64s 213ms/step - loss: 0.0919 - accuracy: 0.9661 - val_loss: 0.4094 - val_accuracy: 0.8690\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 63s 212ms/step - loss: 0.0702 - accuracy: 0.9749 - val_loss: 0.4301 - val_accuracy: 0.8718\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 63s 210ms/step - loss: 0.0567 - accuracy: 0.9785 - val_loss: 0.4975 - val_accuracy: 0.8615\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 63s 211ms/step - loss: 0.0504 - accuracy: 0.9789 - val_loss: 0.5296 - val_accuracy: 0.8596\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 63s 212ms/step - loss: 0.0420 - accuracy: 0.9822 - val_loss: 0.5735 - val_accuracy: 0.8558\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 62s 208ms/step - loss: 0.0426 - accuracy: 0.9819 - val_loss: 0.6289 - val_accuracy: 0.8511\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 88.87841701507568\n",
      "Training 2: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 57s 160ms/step - loss: 0.4568 - accuracy: 0.7845 - val_loss: 0.3065 - val_accuracy: 0.8775\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 44s 148ms/step - loss: 0.1867 - accuracy: 0.9327 - val_loss: 0.3364 - val_accuracy: 0.8662\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.1351 - accuracy: 0.9555 - val_loss: 0.3747 - val_accuracy: 0.8577\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.1040 - accuracy: 0.9619 - val_loss: 0.4439 - val_accuracy: 0.8577\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 44s 148ms/step - loss: 0.0750 - accuracy: 0.9698 - val_loss: 0.4850 - val_accuracy: 0.8577\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 45s 149ms/step - loss: 0.0618 - accuracy: 0.9755 - val_loss: 0.5474 - val_accuracy: 0.8605\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 44s 148ms/step - loss: 0.0453 - accuracy: 0.9813 - val_loss: 0.5909 - val_accuracy: 0.8549\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 44s 147ms/step - loss: 0.0460 - accuracy: 0.9794 - val_loss: 0.6101 - val_accuracy: 0.8464\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 45s 149ms/step - loss: 0.0431 - accuracy: 0.9816 - val_loss: 0.6459 - val_accuracy: 0.8501\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 87.74740695953369\n",
      "Training 3: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 54s 152ms/step - loss: 0.4479 - accuracy: 0.7855 - val_loss: 0.3094 - val_accuracy: 0.8746\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 44s 148ms/step - loss: 0.1891 - accuracy: 0.9317 - val_loss: 0.3187 - val_accuracy: 0.8680\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 46s 153ms/step - loss: 0.1276 - accuracy: 0.9541 - val_loss: 0.3609 - val_accuracy: 0.8633\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 46s 153ms/step - loss: 0.1020 - accuracy: 0.9617 - val_loss: 0.3996 - val_accuracy: 0.8605\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 46s 154ms/step - loss: 0.0784 - accuracy: 0.9671 - val_loss: 0.4514 - val_accuracy: 0.8530\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 45s 152ms/step - loss: 0.0633 - accuracy: 0.9749 - val_loss: 0.5437 - val_accuracy: 0.8511\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 46s 154ms/step - loss: 0.0480 - accuracy: 0.9795 - val_loss: 0.5547 - val_accuracy: 0.8520\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 46s 154ms/step - loss: 0.0433 - accuracy: 0.9804 - val_loss: 0.5932 - val_accuracy: 0.8530\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 46s 155ms/step - loss: 0.0404 - accuracy: 0.9811 - val_loss: 0.6599 - val_accuracy: 0.8454\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 87.4646544456482\n",
      "Training 4: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 56s 161ms/step - loss: 0.4534 - accuracy: 0.7888 - val_loss: 0.3130 - val_accuracy: 0.8812\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 44s 146ms/step - loss: 0.1837 - accuracy: 0.9292 - val_loss: 0.3097 - val_accuracy: 0.8831\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1241 - accuracy: 0.9561 - val_loss: 0.3403 - val_accuracy: 0.8765\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 44s 147ms/step - loss: 0.0946 - accuracy: 0.9644 - val_loss: 0.3676 - val_accuracy: 0.8728\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 44s 147ms/step - loss: 0.0708 - accuracy: 0.9734 - val_loss: 0.4289 - val_accuracy: 0.8728\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 44s 149ms/step - loss: 0.0573 - accuracy: 0.9777 - val_loss: 0.4897 - val_accuracy: 0.8643\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 45s 149ms/step - loss: 0.0484 - accuracy: 0.9797 - val_loss: 0.5044 - val_accuracy: 0.8746\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 44s 148ms/step - loss: 0.0424 - accuracy: 0.9831 - val_loss: 0.5601 - val_accuracy: 0.8737\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 44s 147ms/step - loss: 0.0343 - accuracy: 0.9864 - val_loss: 0.5648 - val_accuracy: 0.8709\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 44s 149ms/step - loss: 0.0324 - accuracy: 0.9861 - val_loss: 0.6023 - val_accuracy: 0.8662\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 88.31291198730469\n",
      "Training 5: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 75s 209ms/step - loss: 0.4415 - accuracy: 0.7966 - val_loss: 0.3396 - val_accuracy: 0.8596\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 65s 217ms/step - loss: 0.1813 - accuracy: 0.9356 - val_loss: 0.3619 - val_accuracy: 0.8596\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 62s 208ms/step - loss: 0.1204 - accuracy: 0.9602 - val_loss: 0.4052 - val_accuracy: 0.8539\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 62s 207ms/step - loss: 0.0957 - accuracy: 0.9634 - val_loss: 0.4613 - val_accuracy: 0.8530\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 62s 208ms/step - loss: 0.0690 - accuracy: 0.9710 - val_loss: 0.5267 - val_accuracy: 0.8464\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 62s 206ms/step - loss: 0.0551 - accuracy: 0.9761 - val_loss: 0.5852 - val_accuracy: 0.8398\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 61s 204ms/step - loss: 0.0451 - accuracy: 0.9824 - val_loss: 0.6258 - val_accuracy: 0.8398\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 61s 204ms/step - loss: 0.0424 - accuracy: 0.9829 - val_loss: 0.6473 - val_accuracy: 0.8426\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 61s 205ms/step - loss: 0.0428 - accuracy: 0.9819 - val_loss: 0.7318 - val_accuracy: 0.8332\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 85.95664501190186\n",
      "Training 6: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 77s 229ms/step - loss: 0.4505 - accuracy: 0.7923 - val_loss: 0.2648 - val_accuracy: 0.8973\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 66s 222ms/step - loss: 0.1964 - accuracy: 0.9296 - val_loss: 0.2769 - val_accuracy: 0.8907\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 65s 218ms/step - loss: 0.1314 - accuracy: 0.9566 - val_loss: 0.2958 - val_accuracy: 0.8897\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 66s 219ms/step - loss: 0.0990 - accuracy: 0.9607 - val_loss: 0.3601 - val_accuracy: 0.8709\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 64s 214ms/step - loss: 0.0764 - accuracy: 0.9685 - val_loss: 0.3842 - val_accuracy: 0.8784\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 64s 214ms/step - loss: 0.0595 - accuracy: 0.9758 - val_loss: 0.4268 - val_accuracy: 0.8699\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 63s 212ms/step - loss: 0.0473 - accuracy: 0.9841 - val_loss: 0.4627 - val_accuracy: 0.8680\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 64s 213ms/step - loss: 0.0438 - accuracy: 0.9824 - val_loss: 0.5203 - val_accuracy: 0.8662\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 64s 215ms/step - loss: 0.0368 - accuracy: 0.9839 - val_loss: 0.5313 - val_accuracy: 0.8662\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 89.72667455673218\n",
      "Training 7: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 70s 205ms/step - loss: 0.4491 - accuracy: 0.7818 - val_loss: 0.3221 - val_accuracy: 0.8802\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 48s 161ms/step - loss: 0.1934 - accuracy: 0.9260 - val_loss: 0.3297 - val_accuracy: 0.8840\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 48s 162ms/step - loss: 0.1279 - accuracy: 0.9535 - val_loss: 0.3603 - val_accuracy: 0.8764\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 48s 160ms/step - loss: 0.0941 - accuracy: 0.9656 - val_loss: 0.3923 - val_accuracy: 0.8642\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 47s 157ms/step - loss: 0.0746 - accuracy: 0.9703 - val_loss: 0.4442 - val_accuracy: 0.8679\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 47s 157ms/step - loss: 0.0675 - accuracy: 0.9724 - val_loss: 0.5203 - val_accuracy: 0.8538\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 47s 158ms/step - loss: 0.0504 - accuracy: 0.9794 - val_loss: 0.5055 - val_accuracy: 0.8538\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 47s 157ms/step - loss: 0.0458 - accuracy: 0.9814 - val_loss: 0.5730 - val_accuracy: 0.8538\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 47s 156ms/step - loss: 0.0416 - accuracy: 0.9808 - val_loss: 0.5960 - val_accuracy: 0.8575\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 46s 155ms/step - loss: 0.0390 - accuracy: 0.9837 - val_loss: 0.6490 - val_accuracy: 0.8575\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 88.39622735977173\n",
      "Training 8: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 52s 142ms/step - loss: 0.4416 - accuracy: 0.8001 - val_loss: 0.3165 - val_accuracy: 0.8717\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 40s 135ms/step - loss: 0.1996 - accuracy: 0.9266 - val_loss: 0.3138 - val_accuracy: 0.8736\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 42s 139ms/step - loss: 0.1339 - accuracy: 0.9484 - val_loss: 0.3406 - val_accuracy: 0.8764\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 42s 140ms/step - loss: 0.0976 - accuracy: 0.9629 - val_loss: 0.4134 - val_accuracy: 0.8679\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 43s 143ms/step - loss: 0.0811 - accuracy: 0.9674 - val_loss: 0.4423 - val_accuracy: 0.8745\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 43s 144ms/step - loss: 0.0589 - accuracy: 0.9771 - val_loss: 0.4657 - val_accuracy: 0.8670\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 43s 143ms/step - loss: 0.0511 - accuracy: 0.9776 - val_loss: 0.4863 - val_accuracy: 0.8613\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 43s 144ms/step - loss: 0.0429 - accuracy: 0.9814 - val_loss: 0.5379 - val_accuracy: 0.8613\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 42s 142ms/step - loss: 0.0422 - accuracy: 0.9823 - val_loss: 0.5571 - val_accuracy: 0.8557\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 42s 142ms/step - loss: 0.0363 - accuracy: 0.9834 - val_loss: 0.6008 - val_accuracy: 0.8717\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 42s 142ms/step - loss: 0.0344 - accuracy: 0.9852 - val_loss: 0.6308 - val_accuracy: 0.8604\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 87.64150738716125\n",
      "Training 9: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 82s 237ms/step - loss: 0.4462 - accuracy: 0.8006 - val_loss: 0.3244 - val_accuracy: 0.8887\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 57s 192ms/step - loss: 0.1833 - accuracy: 0.9353 - val_loss: 0.3281 - val_accuracy: 0.8868\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1281 - accuracy: 0.9527 - val_loss: 0.3691 - val_accuracy: 0.8726\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1027 - accuracy: 0.9620 - val_loss: 0.3983 - val_accuracy: 0.8755\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 53s 176ms/step - loss: 0.0726 - accuracy: 0.9703 - val_loss: 0.4173 - val_accuracy: 0.8717\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.0665 - accuracy: 0.9730 - val_loss: 0.5241 - val_accuracy: 0.8679\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.0544 - accuracy: 0.9771 - val_loss: 0.5111 - val_accuracy: 0.8575\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 53s 176ms/step - loss: 0.0462 - accuracy: 0.9795 - val_loss: 0.5751 - val_accuracy: 0.8651\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.0417 - accuracy: 0.9811 - val_loss: 0.6311 - val_accuracy: 0.8623\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 88.86792659759521\n",
      "Training 10: \n",
      "Epoch 1/100\n",
      "299/299 [==============================] - 62s 180ms/step - loss: 0.4529 - accuracy: 0.7871 - val_loss: 0.3153 - val_accuracy: 0.8651\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 46s 154ms/step - loss: 0.1909 - accuracy: 0.9318 - val_loss: 0.3326 - val_accuracy: 0.8613\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 45s 152ms/step - loss: 0.1232 - accuracy: 0.9580 - val_loss: 0.3832 - val_accuracy: 0.8613\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.0995 - accuracy: 0.9620 - val_loss: 0.4408 - val_accuracy: 0.8594\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.0731 - accuracy: 0.9725 - val_loss: 0.4759 - val_accuracy: 0.8585\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 45s 152ms/step - loss: 0.0600 - accuracy: 0.9745 - val_loss: 0.5290 - val_accuracy: 0.8613\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 46s 154ms/step - loss: 0.0482 - accuracy: 0.9809 - val_loss: 0.5519 - val_accuracy: 0.8509\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.0419 - accuracy: 0.9836 - val_loss: 0.6084 - val_accuracy: 0.8519\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.0344 - accuracy: 0.9857 - val_loss: 0.6401 - val_accuracy: 0.8500\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 86.50943636894226\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  88.878417  87.747407  87.464654  88.312912  85.956645  89.726675   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  88.396227  87.641507  88.867927  86.509436  87.950181  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.878417</td>\n",
       "      <td>87.747407</td>\n",
       "      <td>87.464654</td>\n",
       "      <td>88.312912</td>\n",
       "      <td>85.956645</td>\n",
       "      <td>89.726675</td>\n",
       "      <td>88.396227</td>\n",
       "      <td>87.641507</td>\n",
       "      <td>88.867927</td>\n",
       "      <td>86.509436</td>\n",
       "      <td>87.950181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  88.878417  87.747407  87.464654  88.312912  85.956645  89.726675   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  88.396227  87.641507  88.867927  86.509436  87.950181  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_MPQA_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
