{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LSTM_SUBJ.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DpgSo_BMk1c_",
        "P_bqlrkRk1dB"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_pyerxWk1cU"
      },
      "source": [
        "# LSTM Classification with SUBJ Dataset\n",
        "<hr>\n",
        "\n",
        "We will build a text classification model using LSTM model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
        "\n",
        "## Load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yf32HR1mDkB",
        "outputId": "a9a4560f-f84d-4974-f040-70804517ffee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7hbk0h_k1cs",
        "outputId": "7b89688c-41f9-4703-92a6-ca4fd6ccd2bb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "# nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxMfvnTGk1cw",
        "outputId": "72acb331-162f-4ac8-ab8d-446565b7f5b1"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a822d_yuk1cz"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "7bL3DqJnk1c0",
        "outputId": "89b29ffc-72e2-409c-a829-8565c7bff8bd"
      },
      "source": [
        "corpus = pd.read_pickle('/content/drive/MyDrive/Disertasi/0_data/SUBJ/SUBJ.pkl')\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>smart and alert , thirteen conversations about...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>color , musical bounce and warm seas lapping o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it is not a mass market entertainment but an u...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a light hearted french film about the spiritua...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my wife is an actress has its moments in looki...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>in the end , they discover that balance in lif...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>enter the beautiful and mysterious secret agen...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>after listening to a missionary from china spe...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>looking for a short cut to fame , glass concoc...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label  split\n",
              "0     smart and alert , thirteen conversations about...      0  train\n",
              "1     color , musical bounce and warm seas lapping o...      0  train\n",
              "2     it is not a mass market entertainment but an u...      0  train\n",
              "3     a light hearted french film about the spiritua...      0  train\n",
              "4     my wife is an actress has its moments in looki...      0  train\n",
              "...                                                 ...    ...    ...\n",
              "9995  in the end , they discover that balance in lif...      1  train\n",
              "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
              "9997  enter the beautiful and mysterious secret agen...      1  train\n",
              "9998  after listening to a missionary from china spe...      1  train\n",
              "9999  looking for a short cut to fame , glass concoc...      1  train\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F05YQkA6k1c2",
        "outputId": "f6aada94-1119-4fd8-b6e4-e99230d4b98c"
      },
      "source": [
        "corpus.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sentence  10000 non-null  object\n",
            " 1   label     10000 non-null  int64 \n",
            " 2   split     10000 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 234.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Y6Mwboo4k1c3",
        "outputId": "25a1aa51-67d6-462c-a225-0a2520a9c36c"
      },
      "source": [
        "corpus.groupby( by='label').count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence  split\n",
              "label                 \n",
              "0          5000   5000\n",
              "1          5000   5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmjQKemvk1c5"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ElFMWexlk1c6",
        "outputId": "b08f083c-4a8c-4ccd-b1bd-4422368ee524"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'smart and alert , thirteen conversations about one thing is a small gem .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFjmLF3nk1c7"
      },
      "source": [
        "<!--## Split Dataset-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwm_KjFik1c8"
      },
      "source": [
        "# Data Preprocessing\n",
        "<hr>\n",
        "\n",
        "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
        "\n",
        "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
        "In short, what we will do is:\n",
        "- Puntuations removal\n",
        "- Lower the letter case\n",
        "- Tokenization\n",
        "\n",
        "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
        "\n",
        "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrioLPhQk1c8"
      },
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywVnOVA_k1c9",
        "outputId": "e584d689-0898-4b56-b575-ad0d878c4b4d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
            "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
            "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
            "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
            "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reNAl7Rok1c-",
        "outputId": "1776c9e1-007d-406e-8c44-c780aa1c4e30"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> 1\n",
            "the 2\n",
            "a 3\n",
            "and 4\n",
            "of 5\n",
            "to 6\n",
            "in 7\n",
            "is 8\n",
            "'s 9\n",
            "it 10\n",
            "21324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRQhboX8k1c_"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>\n",
        "\n",
        "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpgSo_BMk1c_"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0pDdas7k1c_"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, )),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPOljZzk1dA",
        "outputId": "3ca34878-61f1-4f10-cc99-d6699c9b8541"
      },
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 487,009\n",
            "Trainable params: 487,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAlg9k3dk1dB"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=5, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_bqlrkRk1dB"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3FYfX8BFk1dC",
        "outputId": "9bf52ad7-3d1c-4d13-a551-463dbf5abe20"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record = record.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 84s 257ms/step - loss: 0.4351 - accuracy: 0.7757 - val_loss: 0.2216 - val_accuracy: 0.9150\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 70s 248ms/step - loss: 0.0813 - accuracy: 0.9736 - val_loss: 0.2411 - val_accuracy: 0.9010\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 74s 264ms/step - loss: 0.0211 - accuracy: 0.9952 - val_loss: 0.3469 - val_accuracy: 0.9070\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 71s 253ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.4108 - val_accuracy: 0.8900\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 75s 267ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.4150 - val_accuracy: 0.9140\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 79s 279ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.4509 - val_accuracy: 0.9040\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.50000214576721\n",
            "Training 2: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 125s 401ms/step - loss: 0.4314 - accuracy: 0.7902 - val_loss: 0.2182 - val_accuracy: 0.9160\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 89s 314ms/step - loss: 0.0817 - accuracy: 0.9765 - val_loss: 0.2644 - val_accuracy: 0.9020\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 110s 390ms/step - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.3410 - val_accuracy: 0.8910\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 104s 370ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.4430 - val_accuracy: 0.8890\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 101s 360ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.5179 - val_accuracy: 0.8910\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 105s 372ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.3471 - val_accuracy: 0.9000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.60000085830688\n",
            "Training 3: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 114s 361ms/step - loss: 0.4342 - accuracy: 0.7942 - val_loss: 0.2109 - val_accuracy: 0.9220\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 86s 305ms/step - loss: 0.0702 - accuracy: 0.9753 - val_loss: 0.2407 - val_accuracy: 0.9210\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 96s 341ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.3362 - val_accuracy: 0.9030\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 101s 360ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.3528 - val_accuracy: 0.9140\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 105s 373ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.3192 - val_accuracy: 0.9050\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 106s 377ms/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.3718 - val_accuracy: 0.9080\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.1999990940094\n",
            "Training 4: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 165s 547ms/step - loss: 0.4353 - accuracy: 0.7873 - val_loss: 0.2183 - val_accuracy: 0.9060\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 136s 482ms/step - loss: 0.0787 - accuracy: 0.9751 - val_loss: 0.2968 - val_accuracy: 0.8960\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 151s 537ms/step - loss: 0.0159 - accuracy: 0.9966 - val_loss: 0.3515 - val_accuracy: 0.8950\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 149s 527ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.3500 - val_accuracy: 0.8840\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 144s 510ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.4359 - val_accuracy: 0.8880\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 143s 509ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.5749 - val_accuracy: 0.8860\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 90.6000018119812\n",
            "Training 5: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 119s 385ms/step - loss: 0.4343 - accuracy: 0.7841 - val_loss: 0.1955 - val_accuracy: 0.9290\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 165s 586ms/step - loss: 0.0716 - accuracy: 0.9769 - val_loss: 0.2045 - val_accuracy: 0.9280\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 160s 567ms/step - loss: 0.0202 - accuracy: 0.9952 - val_loss: 0.3264 - val_accuracy: 0.9120\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 160s 567ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.2563 - val_accuracy: 0.9020\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 157s 557ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.3456 - val_accuracy: 0.9110\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 163s 577ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.3827 - val_accuracy: 0.9160\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.90000200271606\n",
            "Training 6: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 132s 429ms/step - loss: 0.4296 - accuracy: 0.8008 - val_loss: 0.2507 - val_accuracy: 0.9040\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 88s 311ms/step - loss: 0.0673 - accuracy: 0.9820 - val_loss: 0.3018 - val_accuracy: 0.8980\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 0.0257 - accuracy: 0.9938 - val_loss: 0.4001 - val_accuracy: 0.8940\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.4873 - val_accuracy: 0.8850\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.4236 - val_accuracy: 0.8940\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5670 - val_accuracy: 0.8910\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 90.39999842643738\n",
            "Training 7: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 145s 475ms/step - loss: 0.4437 - accuracy: 0.7881 - val_loss: 0.2203 - val_accuracy: 0.9100\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 111s 392ms/step - loss: 0.0771 - accuracy: 0.9726 - val_loss: 0.2393 - val_accuracy: 0.9040\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 117s 414ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.3564 - val_accuracy: 0.9000\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 110s 390ms/step - loss: 0.0130 - accuracy: 0.9970 - val_loss: 0.3713 - val_accuracy: 0.9070\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 112s 398ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.4522 - val_accuracy: 0.8980\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 113s 400ms/step - loss: 6.4205e-04 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.8930\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 91.00000262260437\n",
            "Training 8: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 74s 230ms/step - loss: 0.4274 - accuracy: 0.7937 - val_loss: 0.1909 - val_accuracy: 0.9350\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 172s 611ms/step - loss: 0.0787 - accuracy: 0.9767 - val_loss: 0.2146 - val_accuracy: 0.9250\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 174s 616ms/step - loss: 0.0247 - accuracy: 0.9924 - val_loss: 0.2331 - val_accuracy: 0.9310\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 173s 614ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.2835 - val_accuracy: 0.9210\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 177s 628ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2969 - val_accuracy: 0.9220\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 177s 628ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.2769 - val_accuracy: 0.9170\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 93.50000023841858\n",
            "Training 9: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 178s 593ms/step - loss: 0.4390 - accuracy: 0.7900 - val_loss: 0.2229 - val_accuracy: 0.9250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/15\n",
            "282/282 [==============================] - 139s 491ms/step - loss: 0.0793 - accuracy: 0.9754 - val_loss: 0.2832 - val_accuracy: 0.9080\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 134s 475ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.3074 - val_accuracy: 0.9020\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 132s 467ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.3414 - val_accuracy: 0.9070\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 130s 463ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.4280 - val_accuracy: 0.8930\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 130s 462ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.4531 - val_accuracy: 0.8860\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.5000011920929\n",
            "Training 10: \n",
            "Epoch 1/15\n",
            "282/282 [==============================] - 113s 365ms/step - loss: 0.4532 - accuracy: 0.7945 - val_loss: 0.2112 - val_accuracy: 0.9240\n",
            "Epoch 2/15\n",
            "282/282 [==============================] - 70s 250ms/step - loss: 0.0825 - accuracy: 0.9711 - val_loss: 0.2547 - val_accuracy: 0.9090\n",
            "Epoch 3/15\n",
            "282/282 [==============================] - 74s 263ms/step - loss: 0.0216 - accuracy: 0.9941 - val_loss: 0.2866 - val_accuracy: 0.9120\n",
            "Epoch 4/15\n",
            "282/282 [==============================] - 74s 264ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.3517 - val_accuracy: 0.9160\n",
            "Epoch 5/15\n",
            "282/282 [==============================] - 73s 260ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.3830 - val_accuracy: 0.9220\n",
            "Epoch 6/15\n",
            "282/282 [==============================] - 72s 257ms/step - loss: 4.4777e-04 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9140\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00006: early stopping\n",
            "Test Accuracy: 92.40000247955322\n",
            "\n",
            "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
            "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
            "\n",
            "        acc7  acc8       acc9      acc10        AVG  \n",
            "0  91.000003  93.5  92.500001  92.400002  91.860001  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FL_qggUk1dT"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SgDVm7Lk1dc",
        "outputId": "0fd1f8b7-53e8-4500-d842-77735286b9de"
      },
      "source": [
        "record"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>91.500002</td>\n",
              "      <td>91.600001</td>\n",
              "      <td>92.199999</td>\n",
              "      <td>90.600002</td>\n",
              "      <td>92.900002</td>\n",
              "      <td>90.399998</td>\n",
              "      <td>91.000003</td>\n",
              "      <td>93.5</td>\n",
              "      <td>92.500001</td>\n",
              "      <td>92.400002</td>\n",
              "      <td>91.860001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
              "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
              "\n",
              "        acc7  acc8       acc9      acc10        AVG  \n",
              "0  91.000003  93.5  92.500001  92.400002  91.860001  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTv40Hb_k1dc"
      },
      "source": [
        "report = record\n",
        "report = report.to_excel('LSTM_SUBJ.xlsx', sheet_name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60LbyUAqk1dd"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_96Gt_E9k1dd"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKXkS19-k1de"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtkjPR2ek1de"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Disertasi/WordEmbedding_Models/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkHabeD5k1df",
        "outputId": "ce70ca57-25bd-4eb4-940d-aa607f9bcb73"
      },
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
              "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
              "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
              "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
              "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
              "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
              "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
              "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
              "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
              "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
              "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
              "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
              "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
              "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
              "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
              "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
              "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
              "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
              "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
              "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
              "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
              "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
              "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
              "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
              "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
              "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
              "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
              "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
              "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
              "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
              "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
              "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
              "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
              "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
              "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
              "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
              "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
              "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
              "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
              "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
              "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
              "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
              "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
              "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
              "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
              "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
              "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
              "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
              "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
              "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
              "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
              "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
              "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
              "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
              "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
              "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
              "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
              "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
              "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
              "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
              "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
              "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
              "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
              "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
              "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
              "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
              "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
              "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
              "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
              "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
              "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
              "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
              "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
              "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
              "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHvX1ucek1dg"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc_k2SRjk1dg"
      },
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ICjwjuxk1dh",
        "outputId": "4b92b400-456b-41ac-b087-4909e10641a7"
      },
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxv0CMKNk1dh"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OQafi4Ok1di",
        "outputId": "0f0d6c31-fef1-403f-b10f-41cc0770067f"
      },
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_mean:  -0.003527845\n",
            "emb_std:  0.13315111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z5z_I0fk1di"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za0v_YMUk1dj",
        "outputId": "7d5ea7e8-b3aa-4e18-ae2e-954b7c8d944f"
      },
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
              "        -0.09975549, -0.08595189],\n",
              "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
              "        -0.13156594, -0.05996158],\n",
              "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
              "        -0.03930664,  0.20996094],\n",
              "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
              "        -0.01019287,  0.02075195],\n",
              "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
              "        -0.23144531,  0.04614258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_xQYRn8k1dj"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPOO3N7lk1dk"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = False),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kJXF9nNk1dk",
        "outputId": "6f81fe19-4d30-4a50-8d15-d334979fdbca"
      },
      "source": [
        "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 487,009\n",
            "Trainable params: 187,009\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpCTnwmIk1dl"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7RmItUwk1dl"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=8, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFbwq2vgk1dm",
        "outputId": "51493850-f3a4-4640-b05d-e7600c944193"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record2 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record2 = record2.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record2)\n",
        "print()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 65s 207ms/step - loss: 0.3990 - accuracy: 0.8212 - val_loss: 0.2889 - val_accuracy: 0.8780\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 57s 203ms/step - loss: 0.2361 - accuracy: 0.9068 - val_loss: 0.2483 - val_accuracy: 0.8920\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 56s 200ms/step - loss: 0.2003 - accuracy: 0.9182 - val_loss: 0.2325 - val_accuracy: 0.9040\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 57s 200ms/step - loss: 0.1780 - accuracy: 0.9341 - val_loss: 0.2305 - val_accuracy: 0.9000\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.1404 - accuracy: 0.9477 - val_loss: 0.2187 - val_accuracy: 0.9150\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.1386 - accuracy: 0.9499 - val_loss: 0.2358 - val_accuracy: 0.9110\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.1100 - accuracy: 0.9570 - val_loss: 0.2151 - val_accuracy: 0.9120\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 57s 202ms/step - loss: 0.0956 - accuracy: 0.9655 - val_loss: 0.2459 - val_accuracy: 0.9110\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.0794 - accuracy: 0.9730 - val_loss: 0.2546 - val_accuracy: 0.9090\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.0632 - accuracy: 0.9773 - val_loss: 0.2917 - val_accuracy: 0.9050\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.0437 - accuracy: 0.9855 - val_loss: 0.2996 - val_accuracy: 0.9090\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 57s 201ms/step - loss: 0.0345 - accuracy: 0.9884 - val_loss: 0.2678 - val_accuracy: 0.9060\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0283 - accuracy: 0.9914 - val_loss: 0.3024 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 91.50000214576721\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 67s 214ms/step - loss: 0.3990 - accuracy: 0.8289 - val_loss: 0.2209 - val_accuracy: 0.9160\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.2284 - accuracy: 0.9104 - val_loss: 0.2174 - val_accuracy: 0.9190\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 0.1975 - accuracy: 0.9212 - val_loss: 0.1927 - val_accuracy: 0.9260\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 0.1769 - accuracy: 0.9310 - val_loss: 0.1870 - val_accuracy: 0.9280\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 57s 203ms/step - loss: 0.1453 - accuracy: 0.9406 - val_loss: 0.2066 - val_accuracy: 0.9270\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 0.1324 - accuracy: 0.9516 - val_loss: 0.1981 - val_accuracy: 0.9300\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 0.1095 - accuracy: 0.9586 - val_loss: 0.1872 - val_accuracy: 0.9300\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 0.0890 - accuracy: 0.9685 - val_loss: 0.2740 - val_accuracy: 0.9030\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0806 - accuracy: 0.9702 - val_loss: 0.2128 - val_accuracy: 0.9200\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 0.0554 - accuracy: 0.9835 - val_loss: 0.2187 - val_accuracy: 0.9300\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0338 - accuracy: 0.9886 - val_loss: 0.2245 - val_accuracy: 0.9260\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0349 - accuracy: 0.9896 - val_loss: 0.2788 - val_accuracy: 0.9050\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0352 - accuracy: 0.9906 - val_loss: 0.2602 - val_accuracy: 0.9200\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 0.0212 - accuracy: 0.9946 - val_loss: 0.2806 - val_accuracy: 0.9260\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 93.00000071525574\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 66s 212ms/step - loss: 0.3910 - accuracy: 0.8191 - val_loss: 0.2714 - val_accuracy: 0.8760\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 57s 203ms/step - loss: 0.2241 - accuracy: 0.9104 - val_loss: 0.2176 - val_accuracy: 0.9150\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.1972 - accuracy: 0.9216 - val_loss: 0.1881 - val_accuracy: 0.9330\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.1643 - accuracy: 0.9357 - val_loss: 0.1772 - val_accuracy: 0.9370\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.1439 - accuracy: 0.9445 - val_loss: 0.1840 - val_accuracy: 0.9370\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1329 - accuracy: 0.9509 - val_loss: 0.1686 - val_accuracy: 0.9400\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 0.1172 - accuracy: 0.9548 - val_loss: 0.1754 - val_accuracy: 0.9290\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0929 - accuracy: 0.9657 - val_loss: 0.1689 - val_accuracy: 0.9280\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0776 - accuracy: 0.9717 - val_loss: 0.2093 - val_accuracy: 0.9170\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0595 - accuracy: 0.9799 - val_loss: 0.1929 - val_accuracy: 0.9310\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0399 - accuracy: 0.9883 - val_loss: 0.2054 - val_accuracy: 0.9210\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0425 - accuracy: 0.9840 - val_loss: 0.2551 - val_accuracy: 0.9320\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0552 - accuracy: 0.9783 - val_loss: 0.3404 - val_accuracy: 0.9130\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0197 - accuracy: 0.9946 - val_loss: 0.2740 - val_accuracy: 0.9330\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 93.99999976158142\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 67s 215ms/step - loss: 0.3918 - accuracy: 0.8251 - val_loss: 0.2659 - val_accuracy: 0.8940\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.2245 - accuracy: 0.9158 - val_loss: 0.2174 - val_accuracy: 0.9140\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.2044 - accuracy: 0.9204 - val_loss: 0.1974 - val_accuracy: 0.9250\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 59s 207ms/step - loss: 0.1763 - accuracy: 0.9315 - val_loss: 0.2154 - val_accuracy: 0.9180\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 61s 215ms/step - loss: 0.1496 - accuracy: 0.9411 - val_loss: 0.1955 - val_accuracy: 0.9300\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 0.1200 - accuracy: 0.9541 - val_loss: 0.1839 - val_accuracy: 0.9360\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1053 - accuracy: 0.9597 - val_loss: 0.2513 - val_accuracy: 0.9210\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0977 - accuracy: 0.9662 - val_loss: 0.2110 - val_accuracy: 0.9260\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 0.0688 - accuracy: 0.9773 - val_loss: 0.2229 - val_accuracy: 0.9370\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0531 - accuracy: 0.9830 - val_loss: 0.2262 - val_accuracy: 0.9310\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0411 - accuracy: 0.9863 - val_loss: 0.2306 - val_accuracy: 0.9270\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0342 - accuracy: 0.9905 - val_loss: 0.2783 - val_accuracy: 0.9290\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.2947 - val_accuracy: 0.9220\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0428 - accuracy: 0.9848 - val_loss: 0.3049 - val_accuracy: 0.9220\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.3630 - val_accuracy: 0.9240\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0241 - accuracy: 0.9933 - val_loss: 0.3223 - val_accuracy: 0.9330\n",
            "Epoch 17/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.3336 - val_accuracy: 0.9330\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00017: early stopping\n",
            "Test Accuracy: 93.69999766349792\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 67s 215ms/step - loss: 0.3800 - accuracy: 0.8391 - val_loss: 0.2715 - val_accuracy: 0.8820\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.2208 - accuracy: 0.9139 - val_loss: 0.3216 - val_accuracy: 0.8570\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.1983 - accuracy: 0.9187 - val_loss: 0.2512 - val_accuracy: 0.8930\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1626 - accuracy: 0.9345 - val_loss: 0.2383 - val_accuracy: 0.9030\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.1477 - accuracy: 0.9432 - val_loss: 0.2419 - val_accuracy: 0.9090\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.1174 - accuracy: 0.9580 - val_loss: 0.2514 - val_accuracy: 0.9020\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.1095 - accuracy: 0.9600 - val_loss: 0.2633 - val_accuracy: 0.9020\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0818 - accuracy: 0.9717 - val_loss: 0.3027 - val_accuracy: 0.8910\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0683 - accuracy: 0.9770 - val_loss: 0.2976 - val_accuracy: 0.9050\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0498 - accuracy: 0.9831 - val_loss: 0.3238 - val_accuracy: 0.9040\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0579 - accuracy: 0.9791 - val_loss: 0.3458 - val_accuracy: 0.8900\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0256 - accuracy: 0.9923 - val_loss: 0.3195 - val_accuracy: 0.8970\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0206 - accuracy: 0.9941 - val_loss: 0.3893 - val_accuracy: 0.8910\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 90.89999794960022\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 68s 217ms/step - loss: 0.3988 - accuracy: 0.8258 - val_loss: 0.2488 - val_accuracy: 0.8920\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.2367 - accuracy: 0.9068 - val_loss: 0.2302 - val_accuracy: 0.9110\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.2074 - accuracy: 0.9172 - val_loss: 0.2204 - val_accuracy: 0.9120\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.1673 - accuracy: 0.9340 - val_loss: 0.2218 - val_accuracy: 0.9080\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.1435 - accuracy: 0.9450 - val_loss: 0.2287 - val_accuracy: 0.9070\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.1217 - accuracy: 0.9517 - val_loss: 0.2286 - val_accuracy: 0.9070\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.1046 - accuracy: 0.9599 - val_loss: 0.2630 - val_accuracy: 0.9100\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0825 - accuracy: 0.9727 - val_loss: 0.2578 - val_accuracy: 0.9050\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0698 - accuracy: 0.9752 - val_loss: 0.3018 - val_accuracy: 0.9060\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0589 - accuracy: 0.9780 - val_loss: 0.2623 - val_accuracy: 0.9160\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0480 - accuracy: 0.9839 - val_loss: 0.3160 - val_accuracy: 0.9020\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0284 - accuracy: 0.9902 - val_loss: 0.3654 - val_accuracy: 0.9100\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.3523 - val_accuracy: 0.9110\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0207 - accuracy: 0.9946 - val_loss: 0.3668 - val_accuracy: 0.9170\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0144 - accuracy: 0.9963 - val_loss: 0.3544 - val_accuracy: 0.9110\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.3716 - val_accuracy: 0.9190\n",
            "Epoch 17/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.3897 - val_accuracy: 0.9160\n",
            "Epoch 18/100\n",
            "282/282 [==============================] - 62s 219ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.4891 - val_accuracy: 0.9020\n",
            "Epoch 19/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.3797 - val_accuracy: 0.9140\n",
            "Epoch 20/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.4357 - val_accuracy: 0.9100\n",
            "Epoch 21/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.3783 - val_accuracy: 0.9270\n",
            "Epoch 22/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0074 - accuracy: 0.9970 - val_loss: 0.4182 - val_accuracy: 0.9280\n",
            "Epoch 23/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.3710 - val_accuracy: 0.9170\n",
            "Epoch 24/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0113 - accuracy: 0.9956 - val_loss: 0.4623 - val_accuracy: 0.9200\n",
            "Epoch 25/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.4488 - val_accuracy: 0.9100\n",
            "Epoch 26/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.4134 - val_accuracy: 0.9020\n",
            "Epoch 27/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.4227 - val_accuracy: 0.9120\n",
            "Epoch 28/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.4720 - val_accuracy: 0.9210\n",
            "Epoch 29/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.5028 - val_accuracy: 0.9180\n",
            "Epoch 30/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 8.4479e-04 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.9190\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00030: early stopping\n",
            "Test Accuracy: 92.79999732971191\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 68s 219ms/step - loss: 0.3865 - accuracy: 0.8226 - val_loss: 0.2516 - val_accuracy: 0.8990\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.2208 - accuracy: 0.9138 - val_loss: 0.2328 - val_accuracy: 0.9100\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.1948 - accuracy: 0.9238 - val_loss: 0.2659 - val_accuracy: 0.8960\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.1719 - accuracy: 0.9333 - val_loss: 0.2422 - val_accuracy: 0.9100\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 60s 211ms/step - loss: 0.1539 - accuracy: 0.9391 - val_loss: 0.2371 - val_accuracy: 0.9120\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.1333 - accuracy: 0.9504 - val_loss: 0.2317 - val_accuracy: 0.9080\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.1120 - accuracy: 0.9569 - val_loss: 0.2549 - val_accuracy: 0.9070\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0883 - accuracy: 0.9699 - val_loss: 0.2536 - val_accuracy: 0.9070\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.0761 - accuracy: 0.9749 - val_loss: 0.2262 - val_accuracy: 0.9150\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0609 - accuracy: 0.9794 - val_loss: 0.2446 - val_accuracy: 0.9040\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 59s 209ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.3098 - val_accuracy: 0.9140\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 0.3053 - val_accuracy: 0.9170\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 0.3971 - val_accuracy: 0.9080\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.3352 - val_accuracy: 0.9100\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.3956 - val_accuracy: 0.9120\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0169 - accuracy: 0.9937 - val_loss: 0.3872 - val_accuracy: 0.9070\n",
            "Epoch 17/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.5519 - val_accuracy: 0.8870\n",
            "Epoch 18/100\n",
            "282/282 [==============================] - 59s 211ms/step - loss: 0.0552 - accuracy: 0.9800 - val_loss: 0.3440 - val_accuracy: 0.9100\n",
            "Epoch 19/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.3791 - val_accuracy: 0.9130\n",
            "Epoch 20/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.4675 - val_accuracy: 0.9130\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00020: early stopping\n",
            "Test Accuracy: 91.69999957084656\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 66s 214ms/step - loss: 0.3972 - accuracy: 0.8255 - val_loss: 0.2540 - val_accuracy: 0.8900\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.2252 - accuracy: 0.9143 - val_loss: 0.2297 - val_accuracy: 0.9110\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.2053 - accuracy: 0.9207 - val_loss: 0.2340 - val_accuracy: 0.9030\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1740 - accuracy: 0.9350 - val_loss: 0.2334 - val_accuracy: 0.9150\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 59s 210ms/step - loss: 0.1478 - accuracy: 0.9426 - val_loss: 0.2239 - val_accuracy: 0.9130\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.1297 - accuracy: 0.9511 - val_loss: 0.2303 - val_accuracy: 0.9140\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1102 - accuracy: 0.9584 - val_loss: 0.2650 - val_accuracy: 0.9130\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 59s 208ms/step - loss: 0.0956 - accuracy: 0.9681 - val_loss: 0.2690 - val_accuracy: 0.9030\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.0726 - accuracy: 0.9716 - val_loss: 0.2893 - val_accuracy: 0.9040\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 60s 211ms/step - loss: 0.0669 - accuracy: 0.9773 - val_loss: 0.3293 - val_accuracy: 0.9000\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0420 - accuracy: 0.9856 - val_loss: 0.3489 - val_accuracy: 0.8980\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0345 - accuracy: 0.9883 - val_loss: 0.3306 - val_accuracy: 0.9040\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 91.50000214576721\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 52s 164ms/step - loss: 0.3939 - accuracy: 0.8180 - val_loss: 0.2318 - val_accuracy: 0.9070\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 43s 154ms/step - loss: 0.2261 - accuracy: 0.9135 - val_loss: 0.2100 - val_accuracy: 0.9150\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.1982 - accuracy: 0.9190 - val_loss: 0.2136 - val_accuracy: 0.9190\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.1667 - accuracy: 0.9325 - val_loss: 0.2041 - val_accuracy: 0.9200\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.1438 - accuracy: 0.9471 - val_loss: 0.2103 - val_accuracy: 0.9230\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.1250 - accuracy: 0.9490 - val_loss: 0.2039 - val_accuracy: 0.9260\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.1053 - accuracy: 0.9617 - val_loss: 0.2205 - val_accuracy: 0.9220\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0875 - accuracy: 0.9690 - val_loss: 0.2502 - val_accuracy: 0.9120\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 44s 157ms/step - loss: 0.0714 - accuracy: 0.9740 - val_loss: 0.2413 - val_accuracy: 0.9190\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0549 - accuracy: 0.9812 - val_loss: 0.2531 - val_accuracy: 0.9250\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0409 - accuracy: 0.9876 - val_loss: 0.2960 - val_accuracy: 0.9180\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 44s 156ms/step - loss: 0.0338 - accuracy: 0.9881 - val_loss: 0.3391 - val_accuracy: 0.9090\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.0252 - accuracy: 0.9924 - val_loss: 0.3497 - val_accuracy: 0.9140\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 44s 155ms/step - loss: 0.0267 - accuracy: 0.9896 - val_loss: 0.3154 - val_accuracy: 0.9060\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00014: early stopping\n",
            "Test Accuracy: 92.59999990463257\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 67s 214ms/step - loss: 0.4060 - accuracy: 0.8129 - val_loss: 0.2483 - val_accuracy: 0.8970\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.2281 - accuracy: 0.9118 - val_loss: 0.2160 - val_accuracy: 0.9190\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1977 - accuracy: 0.9223 - val_loss: 0.2023 - val_accuracy: 0.9160\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.1768 - accuracy: 0.9331 - val_loss: 0.2019 - val_accuracy: 0.9100\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.1575 - accuracy: 0.9419 - val_loss: 0.1867 - val_accuracy: 0.9190\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.1303 - accuracy: 0.9495 - val_loss: 0.1942 - val_accuracy: 0.9230\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 0.1092 - accuracy: 0.9583 - val_loss: 0.1820 - val_accuracy: 0.9270\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0847 - accuracy: 0.9704 - val_loss: 0.2240 - val_accuracy: 0.9080\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0846 - accuracy: 0.9687 - val_loss: 0.2100 - val_accuracy: 0.9220\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0604 - accuracy: 0.9790 - val_loss: 0.2247 - val_accuracy: 0.9180\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0395 - accuracy: 0.9887 - val_loss: 0.2331 - val_accuracy: 0.9300\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 0.0294 - accuracy: 0.9898 - val_loss: 0.2568 - val_accuracy: 0.9280\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0301 - accuracy: 0.9916 - val_loss: 0.2575 - val_accuracy: 0.9250\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 0.3133 - val_accuracy: 0.9160\n",
            "Epoch 15/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.3073 - val_accuracy: 0.9310\n",
            "Epoch 16/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.2999 - val_accuracy: 0.9210\n",
            "Epoch 17/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0134 - accuracy: 0.9969 - val_loss: 0.2738 - val_accuracy: 0.9220\n",
            "Epoch 18/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0159 - accuracy: 0.9942 - val_loss: 0.3207 - val_accuracy: 0.9220\n",
            "Epoch 19/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0066 - accuracy: 0.9993 - val_loss: 0.3602 - val_accuracy: 0.9190\n",
            "Epoch 20/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.3158 - val_accuracy: 0.9210\n",
            "Epoch 21/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.3426 - val_accuracy: 0.9230\n",
            "Epoch 22/100\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.3594 - val_accuracy: 0.9270\n",
            "Epoch 23/100\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.3852 - val_accuracy: 0.9220\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00023: early stopping\n",
            "Test Accuracy: 93.09999942779541\n",
            "\n",
            "        acc1       acc2  acc3       acc4  ...       acc8  acc9      acc10    AVG\n",
            "0  91.500002  93.000001  94.0  93.699998  ...  91.500002  92.6  93.099999  92.48\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quKmNarWk1dm"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "2rP18ANAk1dn",
        "outputId": "1ee34278-b573-48de-8d65-8f38ec58f013"
      },
      "source": [
        "record2"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>91.500002</td>\n",
              "      <td>93.000001</td>\n",
              "      <td>94.0</td>\n",
              "      <td>93.699998</td>\n",
              "      <td>90.899998</td>\n",
              "      <td>92.799997</td>\n",
              "      <td>91.7</td>\n",
              "      <td>91.500002</td>\n",
              "      <td>92.6</td>\n",
              "      <td>93.099999</td>\n",
              "      <td>92.48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2  acc3       acc4  ...       acc8  acc9      acc10    AVG\n",
              "0  91.500002  93.000001  94.0  93.699998  ...  91.500002  92.6  93.099999  92.48\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASucg0Dlk1do"
      },
      "source": [
        "report = record2\n",
        "report = report.to_excel('LSTM_SUBJ_2.xlsx', sheet_name='static')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tazk7xRCk1do"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIUPciODk1dp"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9F8zX6jk1dp"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5paCCiPk1dp"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
        "                                  mask_zero= True,\n",
        "                                  output_dim=output_dim, \n",
        "                                  input_length=max_length, \n",
        "                                  input_shape=(max_length, ),\n",
        "                                  # Assign the embedding weight with word2vec embedding marix\n",
        "                                  weights = [emb_matrix],\n",
        "                                  # Set the weight to be not trainable (static)\n",
        "                                  trainable = True),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(64))),\n",
        "#         tf.keras.layers.Dropout(0.5),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Propagate X through a Dense layer with 1 unit\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#     model.summary()\n",
        "    return model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwsHOO2Uk1dq",
        "outputId": "4d754d8a-f808-4668-a43a-066cc228eae3"
      },
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 100, 300)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 487,009\n",
            "Trainable params: 487,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3K3_Bfzk1dq"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKDB6eKVk1dr"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c5n4J5hk1dr",
        "outputId": "b349741c-bd7f-4656-fe31-73557e1bf659"
      },
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
        "record3 = pd.DataFrame(columns = columns)\n",
        "\n",
        "# prepare cross validation with 10 splits and shuffle = True\n",
        "kfold = KFold(10, True)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "exp=0\n",
        "\n",
        "# kfold.split() will return set indices for each split\n",
        "acc_list = []\n",
        "for train, test in kfold.split(sentences):\n",
        "    \n",
        "    exp+=1\n",
        "    print('Training {}: '.format(exp))\n",
        "    \n",
        "    train_x, test_x = [], []\n",
        "    train_y, test_y = [], []\n",
        "\n",
        "    for i in train:\n",
        "        train_x.append(sentences[i])\n",
        "        train_y.append(labels[i])\n",
        "\n",
        "    for i in test:\n",
        "        test_x.append(sentences[i])\n",
        "        test_y.append(labels[i])\n",
        "\n",
        "    # Turn the labels into a numpy array\n",
        "    train_y = np.array(train_y)\n",
        "    test_y = np.array(test_y)\n",
        "\n",
        "    # encode data using\n",
        "    # Cleaning and Tokenization\n",
        "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "    # Turn the text into sequence\n",
        "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "\n",
        "    max_len = max_length(training_sequences)\n",
        "\n",
        "    # Pad the sequence to have the same size\n",
        "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index)+1\n",
        "    \n",
        "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "    # Define the input shape\n",
        "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, train_y, batch_size=32, epochs=100, verbose=1, \n",
        "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
        "\n",
        "    # evaluate the model\n",
        "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
        "    print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "    acc_list.append(acc*100)\n",
        "\n",
        "mean_acc = np.array(acc_list).mean()\n",
        "entries = acc_list + [mean_acc]\n",
        "\n",
        "temp = pd.DataFrame([entries], columns=columns)\n",
        "record3 = record3.append(temp, ignore_index=True)\n",
        "print()\n",
        "print(record3)\n",
        "print()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 98s 325ms/step - loss: 0.3670 - accuracy: 0.8341 - val_loss: 0.1871 - val_accuracy: 0.9330\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 0.0941 - accuracy: 0.9660 - val_loss: 0.2004 - val_accuracy: 0.9270\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 89s 317ms/step - loss: 0.0276 - accuracy: 0.9934 - val_loss: 0.2560 - val_accuracy: 0.9000\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 0.0189 - accuracy: 0.9965 - val_loss: 0.3368 - val_accuracy: 0.9200\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.3999 - val_accuracy: 0.9160\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 5.2121e-04 - accuracy: 1.0000 - val_loss: 0.4287 - val_accuracy: 0.9170\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 1.6038e-04 - accuracy: 1.0000 - val_loss: 0.4490 - val_accuracy: 0.9160\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 1.0058e-04 - accuracy: 1.0000 - val_loss: 0.4659 - val_accuracy: 0.9150\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 6.2686e-05 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 0.9150\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 4.8690e-05 - accuracy: 1.0000 - val_loss: 0.4887 - val_accuracy: 0.9140\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 4.9867e-05 - accuracy: 1.0000 - val_loss: 0.5014 - val_accuracy: 0.9160\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 93.30000281333923\n",
            "Training 2: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 110s 338ms/step - loss: 0.3733 - accuracy: 0.8176 - val_loss: 0.1976 - val_accuracy: 0.9190\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 0.0861 - accuracy: 0.9710 - val_loss: 0.2132 - val_accuracy: 0.9210\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 0.0259 - accuracy: 0.9929 - val_loss: 0.2837 - val_accuracy: 0.9250\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.2965 - val_accuracy: 0.9200\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.3899 - val_accuracy: 0.9150\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 3.0034e-04 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.9150\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 1.6689e-04 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.9120\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 9.4155e-05 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.9120\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 7.3581e-05 - accuracy: 1.0000 - val_loss: 0.4802 - val_accuracy: 0.9110\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 4.6368e-05 - accuracy: 1.0000 - val_loss: 0.4946 - val_accuracy: 0.9120\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 3.3038e-05 - accuracy: 1.0000 - val_loss: 0.5051 - val_accuracy: 0.9140\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 2.5953e-05 - accuracy: 1.0000 - val_loss: 0.5160 - val_accuracy: 0.9130\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 91s 321ms/step - loss: 2.5091e-05 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.9110\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 92.5000011920929\n",
            "Training 3: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 99s 328ms/step - loss: 0.3635 - accuracy: 0.8482 - val_loss: 0.2143 - val_accuracy: 0.9140\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 0.0925 - accuracy: 0.9684 - val_loss: 0.2582 - val_accuracy: 0.9140\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 0.0230 - accuracy: 0.9938 - val_loss: 0.2428 - val_accuracy: 0.9150\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 0.0124 - accuracy: 0.9974 - val_loss: 0.2960 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.4119 - val_accuracy: 0.9080\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4178 - val_accuracy: 0.9020\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.5090 - val_accuracy: 0.9080\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 1.8536e-04 - accuracy: 1.0000 - val_loss: 0.5562 - val_accuracy: 0.9030\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 7.6826e-05 - accuracy: 1.0000 - val_loss: 0.5883 - val_accuracy: 0.9040\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 4.7324e-05 - accuracy: 1.0000 - val_loss: 0.6134 - val_accuracy: 0.9040\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 3.7171e-05 - accuracy: 1.0000 - val_loss: 0.6312 - val_accuracy: 0.9060\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 2.9769e-05 - accuracy: 1.0000 - val_loss: 0.6463 - val_accuracy: 0.9060\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 2.5877e-05 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.9070\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 91.50000214576721\n",
            "Training 4: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 100s 330ms/step - loss: 0.3591 - accuracy: 0.8446 - val_loss: 0.2003 - val_accuracy: 0.9140\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 0.0887 - accuracy: 0.9700 - val_loss: 0.1844 - val_accuracy: 0.9310\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 0.0263 - accuracy: 0.9912 - val_loss: 0.2172 - val_accuracy: 0.9280\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.2444 - val_accuracy: 0.9180\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 0.2890 - val_accuracy: 0.9210\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 92s 327ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.3102 - val_accuracy: 0.9210\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 6.2078e-04 - accuracy: 1.0000 - val_loss: 0.3582 - val_accuracy: 0.9260\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 1.1345e-04 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 0.9250\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 6.7011e-05 - accuracy: 1.0000 - val_loss: 0.3916 - val_accuracy: 0.9250\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 5.9207e-05 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.9240\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 3.4635e-05 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.9240\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 2.5278e-05 - accuracy: 1.0000 - val_loss: 0.4199 - val_accuracy: 0.9240\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 93.09999942779541\n",
            "Training 5: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 99s 327ms/step - loss: 0.3649 - accuracy: 0.8405 - val_loss: 0.1528 - val_accuracy: 0.9460\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 0.0857 - accuracy: 0.9695 - val_loss: 0.1789 - val_accuracy: 0.9310\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 0.0273 - accuracy: 0.9934 - val_loss: 0.2245 - val_accuracy: 0.9370\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.2573 - val_accuracy: 0.9340\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.3344 - val_accuracy: 0.9250\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 90s 321ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3205 - val_accuracy: 0.9300\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 90s 321ms/step - loss: 7.3713e-04 - accuracy: 0.9999 - val_loss: 0.3458 - val_accuracy: 0.9320\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 4.5574e-04 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.9320\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 1.1814e-04 - accuracy: 1.0000 - val_loss: 0.4159 - val_accuracy: 0.9310\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 6.7785e-05 - accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.9300\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 91s 321ms/step - loss: 4.4384e-05 - accuracy: 1.0000 - val_loss: 0.4481 - val_accuracy: 0.9290\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 94.59999799728394\n",
            "Training 6: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 79s 255ms/step - loss: 0.3548 - accuracy: 0.8518 - val_loss: 0.2110 - val_accuracy: 0.9120\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 70s 246ms/step - loss: 0.0953 - accuracy: 0.9673 - val_loss: 0.2453 - val_accuracy: 0.9090\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 70s 247ms/step - loss: 0.0275 - accuracy: 0.9899 - val_loss: 0.3016 - val_accuracy: 0.8950\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 70s 247ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.3700 - val_accuracy: 0.9000\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 70s 247ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.4206 - val_accuracy: 0.8990\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 70s 248ms/step - loss: 6.2975e-04 - accuracy: 1.0000 - val_loss: 0.4818 - val_accuracy: 0.8970\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 70s 249ms/step - loss: 2.3651e-04 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.8970\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 70s 249ms/step - loss: 1.1524e-04 - accuracy: 1.0000 - val_loss: 0.5211 - val_accuracy: 0.8980\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 70s 249ms/step - loss: 7.3781e-05 - accuracy: 1.0000 - val_loss: 0.5450 - val_accuracy: 0.8980\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 71s 251ms/step - loss: 5.5253e-05 - accuracy: 1.0000 - val_loss: 0.5652 - val_accuracy: 0.8970\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 70s 249ms/step - loss: 4.4553e-05 - accuracy: 1.0000 - val_loss: 0.5828 - val_accuracy: 0.8980\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 91.20000004768372\n",
            "Training 7: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 105s 326ms/step - loss: 0.3517 - accuracy: 0.8460 - val_loss: 0.2284 - val_accuracy: 0.9200\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 90s 318ms/step - loss: 0.0935 - accuracy: 0.9698 - val_loss: 0.1871 - val_accuracy: 0.9290\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 0.0283 - accuracy: 0.9934 - val_loss: 0.2445 - val_accuracy: 0.9220\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 91s 321ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.2651 - val_accuracy: 0.9200\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.3271 - val_accuracy: 0.9170\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 91s 321ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.4165 - val_accuracy: 0.9120\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4270 - val_accuracy: 0.9210\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 2.8506e-04 - accuracy: 1.0000 - val_loss: 0.4437 - val_accuracy: 0.9200\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 1.1755e-04 - accuracy: 1.0000 - val_loss: 0.4689 - val_accuracy: 0.9200\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 5.3571e-05 - accuracy: 1.0000 - val_loss: 0.4851 - val_accuracy: 0.9180\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 3.6582e-05 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.9180\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 3.5035e-05 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9190\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00012: early stopping\n",
            "Test Accuracy: 92.90000200271606\n",
            "Training 8: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 100s 332ms/step - loss: 0.3583 - accuracy: 0.8435 - val_loss: 0.2385 - val_accuracy: 0.8990\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 0.0971 - accuracy: 0.9692 - val_loss: 0.2264 - val_accuracy: 0.9090\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 0.0329 - accuracy: 0.9913 - val_loss: 0.2676 - val_accuracy: 0.9100\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0070 - accuracy: 0.9989 - val_loss: 0.3246 - val_accuracy: 0.9070\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.4273 - val_accuracy: 0.8910\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.4883 - val_accuracy: 0.9060\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.6089 - val_accuracy: 0.8940\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 92s 327ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5734 - val_accuracy: 0.8940\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 2.4838e-04 - accuracy: 1.0000 - val_loss: 0.5771 - val_accuracy: 0.9070\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 7.5422e-05 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 0.9050\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 5.2160e-05 - accuracy: 1.0000 - val_loss: 0.6210 - val_accuracy: 0.9030\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 3.8850e-05 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.9040\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 93s 328ms/step - loss: 3.3746e-05 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.9020\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00013: early stopping\n",
            "Test Accuracy: 91.00000262260437\n",
            "Training 9: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 100s 332ms/step - loss: 0.3745 - accuracy: 0.8173 - val_loss: 0.1955 - val_accuracy: 0.9300\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 0.0957 - accuracy: 0.9695 - val_loss: 0.2021 - val_accuracy: 0.9180\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 93s 328ms/step - loss: 0.0306 - accuracy: 0.9924 - val_loss: 0.2204 - val_accuracy: 0.9250\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.2799 - val_accuracy: 0.9150\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 92s 325ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.3508 - val_accuracy: 0.9180\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 93s 329ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.3816 - val_accuracy: 0.9200\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 2.3606e-04 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 0.9200\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 1.0066e-04 - accuracy: 1.0000 - val_loss: 0.4174 - val_accuracy: 0.9210\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 7.0374e-05 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.9200\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 91s 323ms/step - loss: 5.4501e-05 - accuracy: 1.0000 - val_loss: 0.4466 - val_accuracy: 0.9200\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 91s 324ms/step - loss: 3.6075e-05 - accuracy: 1.0000 - val_loss: 0.4598 - val_accuracy: 0.9190\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 93.00000071525574\n",
            "Training 10: \n",
            "Epoch 1/100\n",
            "282/282 [==============================] - 98s 325ms/step - loss: 0.3651 - accuracy: 0.8442 - val_loss: 0.2099 - val_accuracy: 0.9260\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 90s 319ms/step - loss: 0.0854 - accuracy: 0.9686 - val_loss: 0.2231 - val_accuracy: 0.9190\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 90s 321ms/step - loss: 0.0245 - accuracy: 0.9936 - val_loss: 0.3524 - val_accuracy: 0.9100\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 90s 321ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.3430 - val_accuracy: 0.9120\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.2948 - val_accuracy: 0.9140\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 91s 321ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.4432 - val_accuracy: 0.9120\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 90s 320ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.9060\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 91s 322ms/step - loss: 4.4902e-04 - accuracy: 1.0000 - val_loss: 0.5328 - val_accuracy: 0.9070\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 91s 321ms/step - loss: 1.0597e-04 - accuracy: 1.0000 - val_loss: 0.5571 - val_accuracy: 0.9090\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 5.6536e-05 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.9110\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 92s 326ms/step - loss: 4.5188e-05 - accuracy: 1.0000 - val_loss: 0.5975 - val_accuracy: 0.9100\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n",
            "Test Accuracy: 92.59999990463257\n",
            "\n",
            "        acc1       acc2       acc3  ...       acc9  acc10        AVG\n",
            "0  93.300003  92.500001  91.500002  ...  93.000001   92.6  92.570001\n",
            "\n",
            "[1 rows x 11 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPCFzimXk1ds"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "kYX3hG7fk1ds",
        "outputId": "abce5f56-be86-4d89-89a3-5ec05dd41e09"
      },
      "source": [
        "record3"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc1</th>\n",
              "      <th>acc2</th>\n",
              "      <th>acc3</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc5</th>\n",
              "      <th>acc6</th>\n",
              "      <th>acc7</th>\n",
              "      <th>acc8</th>\n",
              "      <th>acc9</th>\n",
              "      <th>acc10</th>\n",
              "      <th>AVG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>93.300003</td>\n",
              "      <td>92.500001</td>\n",
              "      <td>91.500002</td>\n",
              "      <td>93.099999</td>\n",
              "      <td>94.599998</td>\n",
              "      <td>91.2</td>\n",
              "      <td>92.900002</td>\n",
              "      <td>91.000003</td>\n",
              "      <td>93.000001</td>\n",
              "      <td>92.6</td>\n",
              "      <td>92.570001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        acc1       acc2       acc3  ...       acc9  acc10        AVG\n",
              "0  93.300003  92.500001  91.500002  ...  93.000001   92.6  92.570001\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYKI_ovSk1dt"
      },
      "source": [
        "report = record3\n",
        "report = report.to_excel('LSTM_SUBJ_3.xlsx', sheet_name='dynamic')"
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}