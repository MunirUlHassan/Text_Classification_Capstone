{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Classification with MR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using GRU model on the MR Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10662, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it 's so laddish and juvenile , only teenage b...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garbus discards the potential for pathological...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>mazel tov to a film about a family 's joyous l...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>standing in the shadows of motown is the best ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>it 's nice to see piscopo again after all thes...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>provides a porthole into that noble , tremblin...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label  split\n",
       "0                       simplistic , silly and tedious .      0  train\n",
       "1      it 's so laddish and juvenile , only teenage b...      0  train\n",
       "2      exploitative and largely devoid of the depth o...      0  train\n",
       "3      garbus discards the potential for pathological...      0  train\n",
       "4      a visually flashy but narratively opaque and e...      0  train\n",
       "...                                                  ...    ...    ...\n",
       "10657  both exuberantly romantic and serenely melanch...      1  train\n",
       "10658  mazel tov to a film about a family 's joyous l...      1  train\n",
       "10659  standing in the shadows of motown is the best ...      1  train\n",
       "10660  it 's nice to see piscopo again after all thes...      1  train\n",
       "10661  provides a porthole into that noble , tremblin...      1  train\n",
       "\n",
       "[10662 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MR/MR.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10662 entries, 0 to 10661\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10662 non-null  object\n",
      " 1   label     10662 non-null  int32 \n",
      " 2   split     10662 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 208.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5331</td>\n",
       "      <td>5331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5331</td>\n",
       "      <td>5331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          5331   5331\n",
       "1          5331   5331"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simplistic , silly and tedious .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
      "Into a sequence of int: [3, 544, 1838, 13, 3909, 3366, 4, 658, 2629, 416, 10, 236, 4, 10112]\n",
      "Into a padded sequence: [    3   544  1838    13  3909  3366     4   658  2629   416    10   236\n",
      "     4 10112     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "a 3\n",
      "and 4\n",
      "of 5\n",
      "to 6\n",
      "is 7\n",
      "'s 8\n",
      "it 9\n",
      "in 10\n",
      "18760\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,169\n",
      "Trainable params: 515,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "300/300 [==============================] - 91s 207ms/step - loss: 0.6162 - accuracy: 0.6261 - val_loss: 0.4727 - val_accuracy: 0.7723\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 59s 197ms/step - loss: 0.2163 - accuracy: 0.9175 - val_loss: 0.5376 - val_accuracy: 0.7638\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 54s 180ms/step - loss: 0.0619 - accuracy: 0.9828 - val_loss: 0.8385 - val_accuracy: 0.7479\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 51s 171ms/step - loss: 0.0210 - accuracy: 0.9944 - val_loss: 1.1805 - val_accuracy: 0.7582\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.0080 - accuracy: 0.9981 - val_loss: 1.3872 - val_accuracy: 0.7423\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 1.6922 - val_accuracy: 0.7498\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 62s 206ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 1.3107 - val_accuracy: 0.7357\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 59s 198ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 1.4530 - val_accuracy: 0.7488\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 1.7450 - val_accuracy: 0.7366\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 56s 186ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 1.7795 - val_accuracy: 0.7320\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 4.4368e-04 - accuracy: 1.0000 - val_loss: 1.9403 - val_accuracy: 0.7423\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.22586989402771\n",
      "Training 2: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 97s 214ms/step - loss: 0.6144 - accuracy: 0.6391 - val_loss: 0.4697 - val_accuracy: 0.7741\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.2169 - accuracy: 0.9199 - val_loss: 0.6302 - val_accuracy: 0.7545\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.0640 - accuracy: 0.9790 - val_loss: 0.8492 - val_accuracy: 0.7479\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.0240 - accuracy: 0.9930 - val_loss: 1.2644 - val_accuracy: 0.7488\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.0114 - accuracy: 0.9957 - val_loss: 1.2484 - val_accuracy: 0.7582\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 1.3729 - val_accuracy: 0.7535\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 78s 259ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 1.4260 - val_accuracy: 0.7582\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 72s 239ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.4003 - val_accuracy: 0.7488\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 55s 182ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 1.5974 - val_accuracy: 0.7507\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 51s 169ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 1.6232 - val_accuracy: 0.7507\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 1.9267 - val_accuracy: 0.7413\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.41330862045288\n",
      "Training 3: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 81s 188ms/step - loss: 0.6224 - accuracy: 0.6266 - val_loss: 0.4778 - val_accuracy: 0.7720\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.2235 - accuracy: 0.9132 - val_loss: 0.5773 - val_accuracy: 0.7570\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0625 - accuracy: 0.9797 - val_loss: 0.7205 - val_accuracy: 0.7561\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.0241 - accuracy: 0.9937 - val_loss: 1.2080 - val_accuracy: 0.7439\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 1.2325 - val_accuracy: 0.7477\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 1.3988 - val_accuracy: 0.7430\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 1.8366 - val_accuracy: 0.7514\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 1.5754 - val_accuracy: 0.7636\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 1.5633 - val_accuracy: 0.7627\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.0076 - accuracy: 0.9961 - val_loss: 1.9076 - val_accuracy: 0.7467\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.0065 - accuracy: 0.9972 - val_loss: 1.7743 - val_accuracy: 0.7326\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.20450162887573\n",
      "Training 4: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 83s 196ms/step - loss: 0.6164 - accuracy: 0.6268 - val_loss: 0.4959 - val_accuracy: 0.7505\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.2172 - accuracy: 0.9142 - val_loss: 0.5596 - val_accuracy: 0.7477\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0670 - accuracy: 0.9805 - val_loss: 0.7974 - val_accuracy: 0.7364\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0218 - accuracy: 0.9922 - val_loss: 1.3478 - val_accuracy: 0.7242\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0155 - accuracy: 0.9944 - val_loss: 1.3158 - val_accuracy: 0.7298\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 51s 170ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 1.4655 - val_accuracy: 0.7336\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 53s 176ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 1.7741 - val_accuracy: 0.7458\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 58s 193ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 1.7415 - val_accuracy: 0.7308\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 58s 195ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 1.3924 - val_accuracy: 0.7214\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 58s 195ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 1.3607 - val_accuracy: 0.7373\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 64s 213ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 1.4546 - val_accuracy: 0.7308\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 75.04690289497375\n",
      "Training 5: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 103s 238ms/step - loss: 0.6155 - accuracy: 0.6387 - val_loss: 0.4837 - val_accuracy: 0.7711\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.2213 - accuracy: 0.9150 - val_loss: 0.6197 - val_accuracy: 0.7533\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0595 - accuracy: 0.9787 - val_loss: 0.8227 - val_accuracy: 0.7589\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0247 - accuracy: 0.9916 - val_loss: 1.1675 - val_accuracy: 0.7533\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 51s 170ms/step - loss: 0.0135 - accuracy: 0.9950 - val_loss: 1.3138 - val_accuracy: 0.7533\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.0107 - accuracy: 0.9964 - val_loss: 1.2971 - val_accuracy: 0.7392\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0050 - accuracy: 0.9980 - val_loss: 1.3557 - val_accuracy: 0.7420\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 1.7149 - val_accuracy: 0.7702\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0107 - accuracy: 0.9964 - val_loss: 1.7054 - val_accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 1.7120 - val_accuracy: 0.7355\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.0098 - accuracy: 0.9962 - val_loss: 1.5964 - val_accuracy: 0.7402\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.11069583892822\n",
      "Training 6: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 88s 202ms/step - loss: 0.6195 - accuracy: 0.6151 - val_loss: 0.4792 - val_accuracy: 0.7533\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.2263 - accuracy: 0.9129 - val_loss: 0.5940 - val_accuracy: 0.7392\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0647 - accuracy: 0.9787 - val_loss: 0.9533 - val_accuracy: 0.7280\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0228 - accuracy: 0.9934 - val_loss: 1.0524 - val_accuracy: 0.7242\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.0096 - accuracy: 0.9974 - val_loss: 1.3658 - val_accuracy: 0.7176\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 61s 202ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 1.6210 - val_accuracy: 0.7120\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 1.5259 - val_accuracy: 0.7129\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 51s 169ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 1.8140 - val_accuracy: 0.7317\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 51s 169ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 1.9631 - val_accuracy: 0.7270\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 51s 169ms/step - loss: 7.0502e-04 - accuracy: 0.9998 - val_loss: 2.1591 - val_accuracy: 0.7223\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 52s 172ms/step - loss: 9.7335e-05 - accuracy: 1.0000 - val_loss: 2.2656 - val_accuracy: 0.7251\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 75.32833218574524\n",
      "Training 7: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 88s 195ms/step - loss: 0.6246 - accuracy: 0.6139 - val_loss: 0.4951 - val_accuracy: 0.7561\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.2304 - accuracy: 0.9097 - val_loss: 0.5403 - val_accuracy: 0.7702\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.0632 - accuracy: 0.9811 - val_loss: 1.0063 - val_accuracy: 0.7364\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 61s 202ms/step - loss: 0.0262 - accuracy: 0.9909 - val_loss: 0.9397 - val_accuracy: 0.7692\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 66s 220ms/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 1.1787 - val_accuracy: 0.7598\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 63s 211ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 1.2156 - val_accuracy: 0.7598\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 1.4836 - val_accuracy: 0.7298\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 51s 171ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 1.4619 - val_accuracy: 0.7477\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 1.4627 - val_accuracy: 0.7477\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 66s 221ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 1.5849 - val_accuracy: 0.7430\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 66s 220ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 1.6253 - val_accuracy: 0.7505\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 61s 204ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 1.5419 - val_accuracy: 0.7373\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.01688408851624\n",
      "Training 8: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 87s 202ms/step - loss: 0.6107 - accuracy: 0.6417 - val_loss: 0.4772 - val_accuracy: 0.7589\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.2204 - accuracy: 0.9147 - val_loss: 0.5228 - val_accuracy: 0.7664\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 54s 182ms/step - loss: 0.0584 - accuracy: 0.9839 - val_loss: 0.8707 - val_accuracy: 0.7402\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 1.1193 - val_accuracy: 0.7364\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 53s 176ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 1.3159 - val_accuracy: 0.7514\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 53s 175ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 1.6204 - val_accuracy: 0.7420\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 1.5170 - val_accuracy: 0.7270\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 53s 176ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 1.6696 - val_accuracy: 0.7223\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 53s 175ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 1.6042 - val_accuracy: 0.7205\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 1.8183 - val_accuracy: 0.7298\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 52s 175ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 1.8282 - val_accuracy: 0.7261\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.0067 - accuracy: 0.9975 - val_loss: 1.8291 - val_accuracy: 0.7280\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 76.64164900779724\n",
      "Training 9: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 90s 212ms/step - loss: 0.6177 - accuracy: 0.6308 - val_loss: 0.4516 - val_accuracy: 0.7861\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.2199 - accuracy: 0.9183 - val_loss: 0.5027 - val_accuracy: 0.7758\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0634 - accuracy: 0.9790 - val_loss: 0.8280 - val_accuracy: 0.7589\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 58s 195ms/step - loss: 0.0265 - accuracy: 0.9926 - val_loss: 1.0127 - val_accuracy: 0.7608\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 57s 192ms/step - loss: 0.0105 - accuracy: 0.9959 - val_loss: 1.4678 - val_accuracy: 0.7523\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 1.2959 - val_accuracy: 0.7730\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 1.6124 - val_accuracy: 0.7420\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0066 - accuracy: 0.9965 - val_loss: 1.3971 - val_accuracy: 0.7514\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 1.4459 - val_accuracy: 0.7627\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 1.3487 - val_accuracy: 0.7523\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 1.3740 - val_accuracy: 0.7495\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.61163020133972\n",
      "Training 10: \n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 83s 195ms/step - loss: 0.6202 - accuracy: 0.6348 - val_loss: 0.4653 - val_accuracy: 0.7674\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 57s 192ms/step - loss: 0.2306 - accuracy: 0.9089 - val_loss: 0.5503 - val_accuracy: 0.7617\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 67s 224ms/step - loss: 0.0686 - accuracy: 0.9763 - val_loss: 0.8167 - val_accuracy: 0.7674\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 68s 227ms/step - loss: 0.0185 - accuracy: 0.9934 - val_loss: 1.0376 - val_accuracy: 0.7533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.0082 - accuracy: 0.9961 - val_loss: 1.2836 - val_accuracy: 0.7598\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.0047 - accuracy: 0.9981 - val_loss: 1.5718 - val_accuracy: 0.7749\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 1.2794 - val_accuracy: 0.7580\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.0116 - accuracy: 0.9953 - val_loss: 1.1543 - val_accuracy: 0.7627\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 1.6490 - val_accuracy: 0.7345\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0046 - accuracy: 0.9980 - val_loss: 1.6621 - val_accuracy: 0.7542\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 52s 172ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 1.3954 - val_accuracy: 0.7505\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 1.4018 - val_accuracy: 0.7467\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 1.7247 - val_accuracy: 0.7580\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 52s 172ms/step - loss: 3.5741e-04 - accuracy: 1.0000 - val_loss: 1.7718 - val_accuracy: 0.7580\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 3.5965e-04 - accuracy: 1.0000 - val_loss: 1.8613 - val_accuracy: 0.7495\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 52s 172ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 1.7784 - val_accuracy: 0.7420\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 77.48593091964722\n",
      "\n",
      "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  77.22587  77.413309  77.204502  75.046903  77.110696  75.328332  77.016884   \n",
      "\n",
      "        acc8      acc9      acc10        AVG  \n",
      "0  76.641649  78.61163  77.485931  76.908571  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.22587</td>\n",
       "      <td>77.413309</td>\n",
       "      <td>77.204502</td>\n",
       "      <td>75.046903</td>\n",
       "      <td>77.110696</td>\n",
       "      <td>75.328332</td>\n",
       "      <td>77.016884</td>\n",
       "      <td>76.641649</td>\n",
       "      <td>78.61163</td>\n",
       "      <td>77.485931</td>\n",
       "      <td>76.908571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  77.22587  77.413309  77.204502  75.046903  77.110696  75.328332  77.016884   \n",
       "\n",
       "        acc8      acc9      acc10        AVG  \n",
       "0  76.641649  78.61163  77.485931  76.908571  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_MR_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16448 words present from 18760 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.25246422,  1.40718628, -0.3619654 , ..., -0.80374242,\n",
       "        -1.59198165, -0.68927201],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,169\n",
      "Trainable params: 215,169\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "300/300 [==============================] - 72s 150ms/step - loss: 0.6255 - accuracy: 0.6263 - val_loss: 0.5543 - val_accuracy: 0.7207\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.4692 - accuracy: 0.7764 - val_loss: 0.6017 - val_accuracy: 0.7170\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.4252 - accuracy: 0.8029 - val_loss: 0.6452 - val_accuracy: 0.7170\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.3859 - accuracy: 0.8239 - val_loss: 0.6506 - val_accuracy: 0.7263\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3326 - accuracy: 0.8480 - val_loss: 0.5954 - val_accuracy: 0.7470\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 105ms/step - loss: 0.2834 - accuracy: 0.8830 - val_loss: 0.5916 - val_accuracy: 0.7488\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 0.2416 - accuracy: 0.9053 - val_loss: 0.6936 - val_accuracy: 0.7582\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 40s 133ms/step - loss: 0.1999 - accuracy: 0.9191 - val_loss: 0.7245 - val_accuracy: 0.7582\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 35s 116ms/step - loss: 0.1595 - accuracy: 0.9370 - val_loss: 0.7541 - val_accuracy: 0.7545\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.1153 - accuracy: 0.9598 - val_loss: 0.8204 - val_accuracy: 0.7601\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 39s 131ms/step - loss: 0.0830 - accuracy: 0.9709 - val_loss: 0.8844 - val_accuracy: 0.7573\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 39s 129ms/step - loss: 0.0619 - accuracy: 0.9785 - val_loss: 1.2522 - val_accuracy: 0.7357\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 30s 99ms/step - loss: 0.0517 - accuracy: 0.9815 - val_loss: 1.2788 - val_accuracy: 0.7320\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.0303 - accuracy: 0.9908 - val_loss: 1.3491 - val_accuracy: 0.7385\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 36s 119ms/step - loss: 0.0342 - accuracy: 0.9900 - val_loss: 1.4230 - val_accuracy: 0.7451\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 37s 125ms/step - loss: 0.0266 - accuracy: 0.9910 - val_loss: 1.4234 - val_accuracy: 0.7591\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 1.4057 - val_accuracy: 0.7694\n",
      "Epoch 18/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0316 - accuracy: 0.9904 - val_loss: 1.6044 - val_accuracy: 0.7488\n",
      "Epoch 19/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.0256 - accuracy: 0.9909 - val_loss: 1.4484 - val_accuracy: 0.7507\n",
      "Epoch 20/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 1.5176 - val_accuracy: 0.7676\n",
      "Epoch 21/40\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 1.8144 - val_accuracy: 0.7619\n",
      "Epoch 22/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 1.7273 - val_accuracy: 0.7563\n",
      "Epoch 23/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 1.9248 - val_accuracy: 0.7488\n",
      "Epoch 24/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 1.8065 - val_accuracy: 0.7516\n",
      "Epoch 25/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0278 - accuracy: 0.9902 - val_loss: 1.4668 - val_accuracy: 0.7545\n",
      "Epoch 26/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0316 - accuracy: 0.9883 - val_loss: 1.3763 - val_accuracy: 0.7563\n",
      "Epoch 27/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 1.5666 - val_accuracy: 0.7573\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "Test Accuracy: 76.94470286369324\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 64s 130ms/step - loss: 0.6264 - accuracy: 0.6301 - val_loss: 0.5548 - val_accuracy: 0.7451\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.4638 - accuracy: 0.7761 - val_loss: 0.5791 - val_accuracy: 0.7320\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.4297 - accuracy: 0.7953 - val_loss: 0.6143 - val_accuracy: 0.7320\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3808 - accuracy: 0.8333 - val_loss: 0.6613 - val_accuracy: 0.7376\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.3299 - accuracy: 0.8551 - val_loss: 0.6284 - val_accuracy: 0.7526\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 105ms/step - loss: 0.2849 - accuracy: 0.8777 - val_loss: 0.8162 - val_accuracy: 0.7395\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.2428 - accuracy: 0.9012 - val_loss: 0.9823 - val_accuracy: 0.7320\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.1832 - accuracy: 0.9280 - val_loss: 1.0964 - val_accuracy: 0.7291\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1492 - accuracy: 0.9445 - val_loss: 1.2697 - val_accuracy: 0.7170\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.1193 - accuracy: 0.9559 - val_loss: 1.6225 - val_accuracy: 0.7029\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0726 - accuracy: 0.9762 - val_loss: 1.3848 - val_accuracy: 0.7479\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0642 - accuracy: 0.9779 - val_loss: 2.0818 - val_accuracy: 0.7020\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.0652 - accuracy: 0.9731 - val_loss: 2.5999 - val_accuracy: 0.6785\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0412 - accuracy: 0.9848 - val_loss: 2.6020 - val_accuracy: 0.6945\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0286 - accuracy: 0.9896 - val_loss: 2.4426 - val_accuracy: 0.6917\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 75.25773048400879\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 64s 131ms/step - loss: 0.6262 - accuracy: 0.6262 - val_loss: 0.4636 - val_accuracy: 0.7711\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.4743 - accuracy: 0.7692 - val_loss: 0.4542 - val_accuracy: 0.7824\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.4132 - accuracy: 0.8092 - val_loss: 0.5008 - val_accuracy: 0.7514\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3708 - accuracy: 0.8404 - val_loss: 0.4916 - val_accuracy: 0.7664\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3398 - accuracy: 0.8501 - val_loss: 0.5793 - val_accuracy: 0.7570\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.2963 - accuracy: 0.8770 - val_loss: 0.6005 - val_accuracy: 0.7533\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.2459 - accuracy: 0.9018 - val_loss: 0.6447 - val_accuracy: 0.7627\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1880 - accuracy: 0.9283 - val_loss: 0.6690 - val_accuracy: 0.7486\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.1574 - accuracy: 0.9397 - val_loss: 0.7878 - val_accuracy: 0.7589\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1184 - accuracy: 0.9576 - val_loss: 0.8886 - val_accuracy: 0.7580\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0874 - accuracy: 0.9685 - val_loss: 1.0376 - val_accuracy: 0.7486\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0658 - accuracy: 0.9778 - val_loss: 1.0049 - val_accuracy: 0.7420\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.23639512062073\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 78s 129ms/step - loss: 0.6266 - accuracy: 0.6380 - val_loss: 0.5167 - val_accuracy: 0.7411\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.4575 - accuracy: 0.7840 - val_loss: 0.4944 - val_accuracy: 0.7514\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3946 - accuracy: 0.8241 - val_loss: 0.5092 - val_accuracy: 0.7467\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3585 - accuracy: 0.8417 - val_loss: 0.5298 - val_accuracy: 0.7580\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.3220 - accuracy: 0.8631 - val_loss: 0.5996 - val_accuracy: 0.7383\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.2714 - accuracy: 0.8875 - val_loss: 0.6242 - val_accuracy: 0.7402\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.2310 - accuracy: 0.9037 - val_loss: 0.8132 - val_accuracy: 0.7101\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1787 - accuracy: 0.9243 - val_loss: 0.9393 - val_accuracy: 0.7101\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.1409 - accuracy: 0.9437 - val_loss: 1.0172 - val_accuracy: 0.7036\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1101 - accuracy: 0.9601 - val_loss: 1.2901 - val_accuracy: 0.6998\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0730 - accuracy: 0.9735 - val_loss: 1.2990 - val_accuracy: 0.6942\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0611 - accuracy: 0.9798 - val_loss: 1.8216 - val_accuracy: 0.6848\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0410 - accuracy: 0.9861 - val_loss: 2.0262 - val_accuracy: 0.6811\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0296 - accuracy: 0.9893 - val_loss: 2.2874 - val_accuracy: 0.6886\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 75.79737305641174\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 64s 131ms/step - loss: 0.6175 - accuracy: 0.6373 - val_loss: 0.5115 - val_accuracy: 0.7458\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.4574 - accuracy: 0.7795 - val_loss: 0.5281 - val_accuracy: 0.7458\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.4105 - accuracy: 0.8137 - val_loss: 0.5145 - val_accuracy: 0.7448\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3685 - accuracy: 0.8317 - val_loss: 0.5160 - val_accuracy: 0.7364\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3420 - accuracy: 0.8494 - val_loss: 0.5434 - val_accuracy: 0.7458\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 30s 101ms/step - loss: 0.2931 - accuracy: 0.8783 - val_loss: 0.5947 - val_accuracy: 0.7467\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.2495 - accuracy: 0.8955 - val_loss: 0.5990 - val_accuracy: 0.7411\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.2031 - accuracy: 0.9176 - val_loss: 0.6634 - val_accuracy: 0.7458\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.1509 - accuracy: 0.9436 - val_loss: 0.7616 - val_accuracy: 0.7298\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1090 - accuracy: 0.9608 - val_loss: 0.8711 - val_accuracy: 0.7392\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0934 - accuracy: 0.9657 - val_loss: 0.9960 - val_accuracy: 0.7176\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0642 - accuracy: 0.9770 - val_loss: 1.1511 - val_accuracy: 0.7430\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0522 - accuracy: 0.9831 - val_loss: 1.1827 - val_accuracy: 0.7505\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 1.3252 - val_accuracy: 0.7505\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0194 - accuracy: 0.9944 - val_loss: 1.5188 - val_accuracy: 0.7392\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 1.3040 - val_accuracy: 0.7486\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0186 - accuracy: 0.9938 - val_loss: 1.4651 - val_accuracy: 0.7448\n",
      "Epoch 18/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.0179 - accuracy: 0.9941 - val_loss: 1.4716 - val_accuracy: 0.7345\n",
      "Epoch 19/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0085 - accuracy: 0.9984 - val_loss: 1.6929 - val_accuracy: 0.7411\n",
      "Epoch 20/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 1.7215 - val_accuracy: 0.7392\n",
      "Epoch 21/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 1.7080 - val_accuracy: 0.7448\n",
      "Epoch 22/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.0316 - accuracy: 0.9891 - val_loss: 1.6642 - val_accuracy: 0.7242\n",
      "Epoch 23/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 1.8253 - val_accuracy: 0.7158\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Test Accuracy: 75.04690289497375\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 64s 132ms/step - loss: 0.6264 - accuracy: 0.6334 - val_loss: 0.6741 - val_accuracy: 0.6895\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 34s 112ms/step - loss: 0.4587 - accuracy: 0.7844 - val_loss: 0.6691 - val_accuracy: 0.6886\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.4138 - accuracy: 0.8078 - val_loss: 0.6327 - val_accuracy: 0.7036\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 105ms/step - loss: 0.3780 - accuracy: 0.8278 - val_loss: 0.7475 - val_accuracy: 0.7064\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.3270 - accuracy: 0.8558 - val_loss: 0.7287 - val_accuracy: 0.7139\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.2851 - accuracy: 0.8807 - val_loss: 0.9074 - val_accuracy: 0.7064\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 105ms/step - loss: 0.2415 - accuracy: 0.9032 - val_loss: 0.9390 - val_accuracy: 0.7073\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 34s 112ms/step - loss: 0.2025 - accuracy: 0.9187 - val_loss: 0.8998 - val_accuracy: 0.7233\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 32s 105ms/step - loss: 0.1707 - accuracy: 0.9352 - val_loss: 1.0850 - val_accuracy: 0.7054\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.1178 - accuracy: 0.9538 - val_loss: 1.2446 - val_accuracy: 0.7083\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 35s 116ms/step - loss: 0.0914 - accuracy: 0.9678 - val_loss: 1.3268 - val_accuracy: 0.7139\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 34s 114ms/step - loss: 0.0647 - accuracy: 0.9763 - val_loss: 1.4331 - val_accuracy: 0.7120\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 38s 128ms/step - loss: 0.0439 - accuracy: 0.9843 - val_loss: 1.3827 - val_accuracy: 0.7326\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 32s 105ms/step - loss: 0.0403 - accuracy: 0.9866 - val_loss: 1.5416 - val_accuracy: 0.7176\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 32s 108ms/step - loss: 0.0332 - accuracy: 0.9901 - val_loss: 1.6567 - val_accuracy: 0.7345\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 35s 116ms/step - loss: 0.0346 - accuracy: 0.9872 - val_loss: 2.1178 - val_accuracy: 0.7167\n",
      "Epoch 17/40\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 2.1487 - val_accuracy: 0.7251\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 39s 131ms/step - loss: 0.0280 - accuracy: 0.9909 - val_loss: 2.0058 - val_accuracy: 0.7205\n",
      "Epoch 19/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.0178 - accuracy: 0.9948 - val_loss: 2.0512 - val_accuracy: 0.7101\n",
      "Epoch 20/40\n",
      "300/300 [==============================] - 33s 111ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 2.1287 - val_accuracy: 0.7317\n",
      "Epoch 21/40\n",
      "300/300 [==============================] - 33s 111ms/step - loss: 0.0133 - accuracy: 0.9967 - val_loss: 2.5557 - val_accuracy: 0.7026\n",
      "Epoch 22/40\n",
      "300/300 [==============================] - 36s 120ms/step - loss: 0.0143 - accuracy: 0.9958 - val_loss: 1.9884 - val_accuracy: 0.7467\n",
      "Epoch 23/40\n",
      "300/300 [==============================] - 35s 117ms/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 2.1020 - val_accuracy: 0.7139\n",
      "Epoch 24/40\n",
      "300/300 [==============================] - 30s 101ms/step - loss: 0.0272 - accuracy: 0.9911 - val_loss: 1.9449 - val_accuracy: 0.7242\n",
      "Epoch 25/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.0213 - accuracy: 0.9929 - val_loss: 2.1255 - val_accuracy: 0.7083\n",
      "Epoch 26/40\n",
      "300/300 [==============================] - 34s 114ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 2.2260 - val_accuracy: 0.7205\n",
      "Epoch 27/40\n",
      "300/300 [==============================] - 39s 132ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 2.0419 - val_accuracy: 0.7308\n",
      "Epoch 28/40\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 2.3282 - val_accuracy: 0.7270\n",
      "Epoch 29/40\n",
      "300/300 [==============================] - 40s 133ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 2.1641 - val_accuracy: 0.7326\n",
      "Epoch 30/40\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3338 - val_accuracy: 0.7392\n",
      "Epoch 31/40\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 6.8463e-04 - accuracy: 0.9999 - val_loss: 2.3710 - val_accuracy: 0.7364\n",
      "Epoch 32/40\n",
      "300/300 [==============================] - 39s 130ms/step - loss: 4.6424e-04 - accuracy: 0.9999 - val_loss: 2.6517 - val_accuracy: 0.7261\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "Test Accuracy: 74.67166781425476\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 67s 130ms/step - loss: 0.6124 - accuracy: 0.6441 - val_loss: 0.5495 - val_accuracy: 0.7270\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 39s 131ms/step - loss: 0.4614 - accuracy: 0.7743 - val_loss: 0.5039 - val_accuracy: 0.7664\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 34s 113ms/step - loss: 0.4191 - accuracy: 0.8087 - val_loss: 0.4902 - val_accuracy: 0.7711\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 34s 114ms/step - loss: 0.3726 - accuracy: 0.8326 - val_loss: 0.4566 - val_accuracy: 0.7833\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 35s 116ms/step - loss: 0.3317 - accuracy: 0.8568 - val_loss: 0.4803 - val_accuracy: 0.7730\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 36s 121ms/step - loss: 0.2797 - accuracy: 0.8841 - val_loss: 0.5027 - val_accuracy: 0.7711\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 35s 118ms/step - loss: 0.2426 - accuracy: 0.9037 - val_loss: 0.5133 - val_accuracy: 0.7814\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.1982 - accuracy: 0.9183 - val_loss: 0.6253 - val_accuracy: 0.7561\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 32s 108ms/step - loss: 0.1582 - accuracy: 0.9400 - val_loss: 0.6644 - val_accuracy: 0.7702\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 32s 108ms/step - loss: 0.1152 - accuracy: 0.9572 - val_loss: 0.7728 - val_accuracy: 0.7786\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 33s 111ms/step - loss: 0.0838 - accuracy: 0.9701 - val_loss: 0.9266 - val_accuracy: 0.7674acy: 0.\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 34s 114ms/step - loss: 0.0658 - accuracy: 0.9783 - val_loss: 1.0630 - val_accuracy: 0.7608\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 35s 117ms/step - loss: 0.0495 - accuracy: 0.9839 - val_loss: 1.0445 - val_accuracy: 0.7674\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 35s 118ms/step - loss: 0.0404 - accuracy: 0.9863 - val_loss: 1.1740 - val_accuracy: 0.7739\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 78.33020687103271\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 70s 146ms/step - loss: 0.6290 - accuracy: 0.6323 - val_loss: 0.5282 - val_accuracy: 0.7373\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 32s 108ms/step - loss: 0.4543 - accuracy: 0.7779 - val_loss: 0.5066 - val_accuracy: 0.7523\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 33s 111ms/step - loss: 0.4055 - accuracy: 0.8150 - val_loss: 0.5431 - val_accuracy: 0.7533\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 32s 105ms/step - loss: 0.3668 - accuracy: 0.8287 - val_loss: 0.5026 - val_accuracy: 0.7692\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.3164 - accuracy: 0.8679 - val_loss: 0.6132 - val_accuracy: 0.7505\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.2828 - accuracy: 0.8824 - val_loss: 0.5879 - val_accuracy: 0.7561\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 105ms/step - loss: 0.2426 - accuracy: 0.9036 - val_loss: 0.7038 - val_accuracy: 0.7383\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.1853 - accuracy: 0.9289 - val_loss: 1.0290 - val_accuracy: 0.7251\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 32s 108ms/step - loss: 0.1561 - accuracy: 0.9394 - val_loss: 0.8326 - val_accuracy: 0.7364\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.1138 - accuracy: 0.9606 - val_loss: 1.0807 - val_accuracy: 0.7383\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 34s 115ms/step - loss: 0.0903 - accuracy: 0.9680 - val_loss: 1.2448 - val_accuracy: 0.7402\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.0548 - accuracy: 0.9817 - val_loss: 1.2411 - val_accuracy: 0.7523\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0496 - accuracy: 0.9830 - val_loss: 1.5944 - val_accuracy: 0.7402\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.0363 - accuracy: 0.9877 - val_loss: 1.3271 - val_accuracy: 0.7420\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 76.92307829856873\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 88s 132ms/step - loss: 0.6119 - accuracy: 0.6471 - val_loss: 0.4693 - val_accuracy: 0.7645\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.4541 - accuracy: 0.7832 - val_loss: 0.4591 - val_accuracy: 0.7749\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 32s 106ms/step - loss: 0.4108 - accuracy: 0.8071 - val_loss: 0.4332 - val_accuracy: 0.7861\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.3721 - accuracy: 0.8377 - val_loss: 0.4208 - val_accuracy: 0.7964\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.3311 - accuracy: 0.8577 - val_loss: 0.4326 - val_accuracy: 0.7936\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 32s 108ms/step - loss: 0.2871 - accuracy: 0.8818 - val_loss: 0.4338 - val_accuracy: 0.8077\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 33s 110ms/step - loss: 0.2385 - accuracy: 0.9065 - val_loss: 0.4930 - val_accuracy: 0.8039\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.1884 - accuracy: 0.9263 - val_loss: 0.5249 - val_accuracy: 0.7889\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.1508 - accuracy: 0.9428 - val_loss: 0.6195 - val_accuracy: 0.7964\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.1170 - accuracy: 0.9564 - val_loss: 0.7138 - val_accuracy: 0.7983\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0931 - accuracy: 0.9653 - val_loss: 0.8037 - val_accuracy: 0.7908\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0597 - accuracy: 0.9791 - val_loss: 0.9640 - val_accuracy: 0.7992\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0442 - accuracy: 0.9834 - val_loss: 0.9351 - val_accuracy: 0.7936\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0410 - accuracy: 0.9857 - val_loss: 1.0162 - val_accuracy: 0.7927\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0237 - accuracy: 0.9922 - val_loss: 1.0524 - val_accuracy: 0.7861\n",
      "Epoch 16/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0350 - accuracy: 0.9868 - val_loss: 1.1648 - val_accuracy: 0.7805\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 80.7692289352417\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 64s 131ms/step - loss: 0.6199 - accuracy: 0.6484 - val_loss: 0.5014 - val_accuracy: 0.7514\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.4629 - accuracy: 0.7772 - val_loss: 0.4707 - val_accuracy: 0.7711\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.4130 - accuracy: 0.8032 - val_loss: 0.4629 - val_accuracy: 0.7749\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.3815 - accuracy: 0.8330 - val_loss: 0.4593 - val_accuracy: 0.7824\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.3336 - accuracy: 0.8497 - val_loss: 0.4650 - val_accuracy: 0.7946\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.2969 - accuracy: 0.8759 - val_loss: 0.5146 - val_accuracy: 0.7824\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.2562 - accuracy: 0.8967 - val_loss: 0.5873 - val_accuracy: 0.7533\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.2271 - accuracy: 0.9011 - val_loss: 0.6405 - val_accuracy: 0.7552\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.1626 - accuracy: 0.9378 - val_loss: 0.6509 - val_accuracy: 0.7692\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.1314 - accuracy: 0.9488 - val_loss: 0.7611 - val_accuracy: 0.7589\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0905 - accuracy: 0.9716 - val_loss: 0.8395 - val_accuracy: 0.7552\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 31s 104ms/step - loss: 0.0656 - accuracy: 0.9763 - val_loss: 0.9394 - val_accuracy: 0.7420\n",
      "Epoch 13/40\n",
      "300/300 [==============================] - 31s 102ms/step - loss: 0.0486 - accuracy: 0.9826 - val_loss: 1.2367 - val_accuracy: 0.7420\n",
      "Epoch 14/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0488 - accuracy: 0.9816 - val_loss: 1.1394 - val_accuracy: 0.7523\n",
      "Epoch 15/40\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0277 - accuracy: 0.9915 - val_loss: 1.2582 - val_accuracy: 0.7617\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 79.4559121131897\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  76.944703  75.25773  78.236395  75.797373  75.046903  74.671668  78.330207   \n",
      "\n",
      "        acc8       acc9      acc10       AVG  \n",
      "0  76.923078  80.769229  79.455912  77.14332  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.944703</td>\n",
       "      <td>75.25773</td>\n",
       "      <td>78.236395</td>\n",
       "      <td>75.797373</td>\n",
       "      <td>75.046903</td>\n",
       "      <td>74.671668</td>\n",
       "      <td>78.330207</td>\n",
       "      <td>76.923078</td>\n",
       "      <td>80.769229</td>\n",
       "      <td>79.455912</td>\n",
       "      <td>77.14332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  76.944703  75.25773  78.236395  75.797373  75.046903  74.671668  78.330207   \n",
       "\n",
       "        acc8       acc9      acc10       AVG  \n",
       "0  76.923078  80.769229  79.455912  77.14332  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_MR_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_45 (Bidirectio (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,169\n",
      "Trainable params: 515,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diardano Raihan\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "300/300 [==============================] - 85s 199ms/step - loss: 0.6130 - accuracy: 0.6383 - val_loss: 0.4564 - val_accuracy: 0.7713\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 51s 171ms/step - loss: 0.2777 - accuracy: 0.8833 - val_loss: 0.5160 - val_accuracy: 0.7591\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.1076 - accuracy: 0.9624 - val_loss: 0.6985 - val_accuracy: 0.7676\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 1.0300 - val_accuracy: 0.7413\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.0118 - accuracy: 0.9975 - val_loss: 1.2321 - val_accuracy: 0.7591\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 1.2953 - val_accuracy: 0.7498\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 51s 172ms/step - loss: 0.0120 - accuracy: 0.9952 - val_loss: 1.4578 - val_accuracy: 0.7516\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.0079 - accuracy: 0.9969 - val_loss: 1.4374 - val_accuracy: 0.7638\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 56s 185ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 1.6120 - val_accuracy: 0.7545\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 1.4721 - val_accuracy: 0.7488\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 1.7452 - val_accuracy: 0.7516\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 77.13214755058289\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 89s 212ms/step - loss: 0.6078 - accuracy: 0.6499 - val_loss: 0.4697 - val_accuracy: 0.7704\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 66s 219ms/step - loss: 0.2751 - accuracy: 0.8913 - val_loss: 0.4536 - val_accuracy: 0.7938\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 63s 209ms/step - loss: 0.1203 - accuracy: 0.9590 - val_loss: 0.5919 - val_accuracy: 0.7826\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 55s 182ms/step - loss: 0.0417 - accuracy: 0.9879 - val_loss: 0.8859 - val_accuracy: 0.7807\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 56s 186ms/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 1.0569 - val_accuracy: 0.7788\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 1.3700 - val_accuracy: 0.7713\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 54s 178ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 1.4462 - val_accuracy: 0.7694\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 1.3113 - val_accuracy: 0.7704\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 57s 189ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 1.6618 - val_accuracy: 0.7666\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 65s 216ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 1.8217 - val_accuracy: 0.7648\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 75s 251ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 1.7631 - val_accuracy: 0.7601\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 60s 201ms/step - loss: 7.7871e-04 - accuracy: 1.0000 - val_loss: 2.1238 - val_accuracy: 0.7545\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 79.38144207000732\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 122s 262ms/step - loss: 0.6162 - accuracy: 0.6311 - val_loss: 0.4391 - val_accuracy: 0.7871\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 58s 194ms/step - loss: 0.2776 - accuracy: 0.8875 - val_loss: 0.4657 - val_accuracy: 0.7786\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.1153 - accuracy: 0.9593 - val_loss: 0.7147 - val_accuracy: 0.7514\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0381 - accuracy: 0.9889 - val_loss: 0.8547 - val_accuracy: 0.7580\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0145 - accuracy: 0.9973 - val_loss: 1.1416 - val_accuracy: 0.7533\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 1.3446 - val_accuracy: 0.7655\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 50s 168ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 1.4725 - val_accuracy: 0.7495\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 1.7467 - val_accuracy: 0.7355\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 1.7850 - val_accuracy: 0.7486\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 1.7222 - val_accuracy: 0.7561\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 1.3753 - val_accuracy: 0.7458\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 78.70544195175171\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 101s 189ms/step - loss: 0.5965 - accuracy: 0.6644 - val_loss: 0.5096 - val_accuracy: 0.7561\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 51s 171ms/step - loss: 0.2624 - accuracy: 0.8931 - val_loss: 0.4983 - val_accuracy: 0.7833\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 57s 190ms/step - loss: 0.0989 - accuracy: 0.9683 - val_loss: 0.7430 - val_accuracy: 0.7664\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.0352 - accuracy: 0.9877 - val_loss: 0.8854 - val_accuracy: 0.7608\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 1.1389 - val_accuracy: 0.7645\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 1.3834 - val_accuracy: 0.7514\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 1.3163 - val_accuracy: 0.7598\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 1.3777 - val_accuracy: 0.7636\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 1.6445 - val_accuracy: 0.7636\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 1.5596 - val_accuracy: 0.7580\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 71s 238ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 1.6769 - val_accuracy: 0.7523\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 63s 211ms/step - loss: 0.0066 - accuracy: 0.9973 - val_loss: 1.6995 - val_accuracy: 0.7580\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.33020687103271\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 93s 224ms/step - loss: 0.6200 - accuracy: 0.6288 - val_loss: 0.4446 - val_accuracy: 0.7946\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 59s 196ms/step - loss: 0.2858 - accuracy: 0.8804 - val_loss: 0.4505 - val_accuracy: 0.8086\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.1157 - accuracy: 0.9618 - val_loss: 0.6406 - val_accuracy: 0.7889\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 57s 189ms/step - loss: 0.0403 - accuracy: 0.9884 - val_loss: 0.9724 - val_accuracy: 0.7814\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 57s 191ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 1.0893 - val_accuracy: 0.7824\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 1.2614 - val_accuracy: 0.7786\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 1.4710 - val_accuracy: 0.7702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 1.4257 - val_accuracy: 0.7814\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 60s 200ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 1.2272 - val_accuracy: 0.7861\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 1.3872 - val_accuracy: 0.7805\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 57s 191ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 1.4827 - val_accuracy: 0.7880\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 60s 199ms/step - loss: 9.7030e-04 - accuracy: 0.9998 - val_loss: 1.3733 - val_accuracy: 0.7861\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 80.86304068565369\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 89s 210ms/step - loss: 0.6050 - accuracy: 0.6472 - val_loss: 0.5123 - val_accuracy: 0.7542\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 51s 171ms/step - loss: 0.2710 - accuracy: 0.8887 - val_loss: 0.4987 - val_accuracy: 0.7636\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.1118 - accuracy: 0.9595 - val_loss: 0.7683 - val_accuracy: 0.7533\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 51s 171ms/step - loss: 0.0425 - accuracy: 0.9870 - val_loss: 0.9997 - val_accuracy: 0.7345\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.0153 - accuracy: 0.9957 - val_loss: 1.4599 - val_accuracy: 0.7458\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 1.5727 - val_accuracy: 0.7392\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0049 - accuracy: 0.9977 - val_loss: 1.5195 - val_accuracy: 0.7411\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 1.5777 - val_accuracy: 0.7458\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 1.9518 - val_accuracy: 0.7336\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 1.6137 - val_accuracy: 0.7373\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 50s 168ms/step - loss: 0.0041 - accuracy: 0.9984 - val_loss: 1.8826 - val_accuracy: 0.7317\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 2.2653 - val_accuracy: 0.7345\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 76.36022567749023\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 98s 189ms/step - loss: 0.5960 - accuracy: 0.6647 - val_loss: 0.4955 - val_accuracy: 0.7627\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.2776 - accuracy: 0.8872 - val_loss: 0.6019 - val_accuracy: 0.7420\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.1090 - accuracy: 0.9628 - val_loss: 0.9295 - val_accuracy: 0.7383\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0349 - accuracy: 0.9902 - val_loss: 1.4010 - val_accuracy: 0.7064\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 1.3737 - val_accuracy: 0.7280\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 1.9418 - val_accuracy: 0.7129\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 1.5301 - val_accuracy: 0.7289\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 2.2946 - val_accuracy: 0.7223\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 3.7983e-04 - accuracy: 1.0000 - val_loss: 2.3336 - val_accuracy: 0.7176\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 4.7812e-04 - accuracy: 1.0000 - val_loss: 2.5664 - val_accuracy: 0.7223\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 1.3094e-04 - accuracy: 1.0000 - val_loss: 2.4526 - val_accuracy: 0.7326\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 76.26641392707825\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 82s 193ms/step - loss: 0.5920 - accuracy: 0.6608 - val_loss: 0.4777 - val_accuracy: 0.7711\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.2662 - accuracy: 0.8917 - val_loss: 0.4754 - val_accuracy: 0.7871\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 50s 168ms/step - loss: 0.1073 - accuracy: 0.9647 - val_loss: 0.6307 - val_accuracy: 0.7683\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0385 - accuracy: 0.9883 - val_loss: 0.8401 - val_accuracy: 0.7852\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0203 - accuracy: 0.9944 - val_loss: 1.0735 - val_accuracy: 0.7852\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 1.3406 - val_accuracy: 0.7645\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 1.2541 - val_accuracy: 0.7720\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 1.3141 - val_accuracy: 0.7702\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 1.2444 - val_accuracy: 0.7767\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 1.4586 - val_accuracy: 0.7702\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 1.6172 - val_accuracy: 0.7636\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 1.9651 - val_accuracy: 0.7383\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 78.70544195175171\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "300/300 [==============================] - 84s 201ms/step - loss: 0.6050 - accuracy: 0.6422 - val_loss: 0.5037 - val_accuracy: 0.7655\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.2715 - accuracy: 0.8916 - val_loss: 0.5103 - val_accuracy: 0.7749\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.1136 - accuracy: 0.9609 - val_loss: 0.7664 - val_accuracy: 0.7636\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.0341 - accuracy: 0.9888 - val_loss: 0.9864 - val_accuracy: 0.7542\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 1.3108 - val_accuracy: 0.7486\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0125 - accuracy: 0.9968 - val_loss: 1.3583 - val_accuracy: 0.7692\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 1.4507 - val_accuracy: 0.7636\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 49s 165ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 1.3354 - val_accuracy: 0.7636\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.0049 - accuracy: 0.9981 - val_loss: 1.5616 - val_accuracy: 0.7570\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 1.8681 - val_accuracy: 0.7477\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 51s 169ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.9668 - val_accuracy: 0.7617\n",
      "Epoch 12/40\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 1.8710 - val_accuracy: 0.7505\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.48593091964722\n",
      "Training 10: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "300/300 [==============================] - 80s 190ms/step - loss: 0.6018 - accuracy: 0.6546 - val_loss: 0.4254 - val_accuracy: 0.8114\n",
      "Epoch 2/40\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.2771 - accuracy: 0.8872 - val_loss: 0.4744 - val_accuracy: 0.7824\n",
      "Epoch 3/40\n",
      "300/300 [==============================] - 55s 182ms/step - loss: 0.1063 - accuracy: 0.9616 - val_loss: 0.6077 - val_accuracy: 0.7767\n",
      "Epoch 4/40\n",
      "300/300 [==============================] - 53s 176ms/step - loss: 0.0389 - accuracy: 0.9890 - val_loss: 0.8475 - val_accuracy: 0.7786\n",
      "Epoch 5/40\n",
      "300/300 [==============================] - 54s 181ms/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 1.1102 - val_accuracy: 0.7608\n",
      "Epoch 6/40\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 1.2496 - val_accuracy: 0.7730\n",
      "Epoch 7/40\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.0042 - accuracy: 0.9977 - val_loss: 1.2765 - val_accuracy: 0.7739\n",
      "Epoch 8/40\n",
      "300/300 [==============================] - 50s 168ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.3135 - val_accuracy: 0.7636\n",
      "Epoch 9/40\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 1.3301 - val_accuracy: 0.7739\n",
      "Epoch 10/40\n",
      "300/300 [==============================] - 53s 178ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 1.4490 - val_accuracy: 0.7814\n",
      "Epoch 11/40\n",
      "300/300 [==============================] - 58s 193ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 1.5750 - val_accuracy: 0.7664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 81.1444640159607\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  77.132148  79.381442  78.705442  78.330207  80.863041  76.360226   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  76.266414  78.705442  77.485931  81.144464  78.437476  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.132148</td>\n",
       "      <td>79.381442</td>\n",
       "      <td>78.705442</td>\n",
       "      <td>78.330207</td>\n",
       "      <td>80.863041</td>\n",
       "      <td>76.360226</td>\n",
       "      <td>76.266414</td>\n",
       "      <td>78.705442</td>\n",
       "      <td>77.485931</td>\n",
       "      <td>81.144464</td>\n",
       "      <td>78.437476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  77.132148  79.381442  78.705442  78.330207  80.863041  76.360226   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  76.266414  78.705442  77.485931  81.144464  78.437476  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_MR_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
