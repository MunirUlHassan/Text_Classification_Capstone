{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Classification with CR Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using GRU model on the Customer Reviews Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3775, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weaknesses are minor the feel and layout of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many of our disney movies do n 't play on this...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player has a problem with dual layer dvd 's su...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know the saying is you get what you pay for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will never purchase apex again .</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>so far , the anti spam feature seems to be ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>i did not have any of the installation problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>their products have been great and have saved ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     weaknesses are minor the feel and layout of th...      0  train\n",
       "1     many of our disney movies do n 't play on this...      0  train\n",
       "2     player has a problem with dual layer dvd 's su...      0  train\n",
       "3     i know the saying is you get what you pay for ...      0  train\n",
       "4                      will never purchase apex again .      0  train\n",
       "...                                                 ...    ...    ...\n",
       "3770  so far , the anti spam feature seems to be ver...      1  train\n",
       "3771  i downloaded a trial version of computer assoc...      1  train\n",
       "3772  i did not have any of the installation problem...      1  train\n",
       "3773  their products have been great and have saved ...      1  train\n",
       "3774                                                         1  train\n",
       "\n",
       "[3775 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/CR/CR.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3775 entries, 0 to 3774\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3775 non-null   object\n",
      " 1   label     3775 non-null   int32 \n",
      " 2   split     3775 non-null   object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 73.9+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1368</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2407</td>\n",
       "      <td>2407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          1368   1368\n",
       "1          2407   2407"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weaknesses are minor the feel and layout of the remote control are only so so . it does n 't show the complete file names of mp3s with really long names . you must cycle through every zoom setting ( 2x , 3x , 4x , 1 2x , etc . ) before getting back to normal size sorry if i 'm just ignorant of a way to get back to 1x quickly .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  will never purchase apex again .\n",
      "Into a sequence of int: [72, 194, 285, 207, 286]\n",
      "Into a padded sequence: [ 72 194 285 207 286   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "and 3\n",
      "i 4\n",
      "it 5\n",
      "to 6\n",
      "a 7\n",
      "is 8\n",
      "of 9\n",
      "this 10\n",
      "5336\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 52s 402ms/step - loss: 0.6235 - accuracy: 0.6519 - val_loss: 0.4560 - val_accuracy: 0.8069\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 35s 326ms/step - loss: 0.2971 - accuracy: 0.8729 - val_loss: 0.4364 - val_accuracy: 0.7989\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 36s 333ms/step - loss: 0.1369 - accuracy: 0.9562 - val_loss: 0.5169 - val_accuracy: 0.8307\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 35s 330ms/step - loss: 0.0758 - accuracy: 0.9762 - val_loss: 0.6673 - val_accuracy: 0.8042\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 36s 333ms/step - loss: 0.0384 - accuracy: 0.9895 - val_loss: 0.7779 - val_accuracy: 0.8228\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 36s 332ms/step - loss: 0.0205 - accuracy: 0.9942 - val_loss: 0.9225 - val_accuracy: 0.8201\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 35s 331ms/step - loss: 0.0417 - accuracy: 0.9904 - val_loss: 0.7853 - val_accuracy: 0.8201\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 36s 332ms/step - loss: 0.0122 - accuracy: 0.9958 - val_loss: 1.0469 - val_accuracy: 0.8069\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 83.06878209114075\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 52s 386ms/step - loss: 0.6164 - accuracy: 0.6600 - val_loss: 0.4330 - val_accuracy: 0.7963\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 38s 353ms/step - loss: 0.2801 - accuracy: 0.8840 - val_loss: 0.4320 - val_accuracy: 0.8016\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 36s 334ms/step - loss: 0.1368 - accuracy: 0.9563 - val_loss: 0.5979 - val_accuracy: 0.7751\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 34s 321ms/step - loss: 0.0808 - accuracy: 0.9727 - val_loss: 0.8016 - val_accuracy: 0.7778\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 34s 319ms/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.9188 - val_accuracy: 0.7646\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 34s 321ms/step - loss: 0.0301 - accuracy: 0.9883 - val_loss: 1.1500 - val_accuracy: 0.7698\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 35s 324ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 1.1401 - val_accuracy: 0.7725\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 80.15872836112976\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 50s 375ms/step - loss: 0.6135 - accuracy: 0.6588 - val_loss: 0.4568 - val_accuracy: 0.7672\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 32s 296ms/step - loss: 0.2934 - accuracy: 0.8821 - val_loss: 0.4630 - val_accuracy: 0.7937\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 32s 298ms/step - loss: 0.1362 - accuracy: 0.9516 - val_loss: 0.4722 - val_accuracy: 0.7672\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 32s 295ms/step - loss: 0.0835 - accuracy: 0.9785 - val_loss: 0.7345 - val_accuracy: 0.7778\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 32s 298ms/step - loss: 0.0659 - accuracy: 0.9788 - val_loss: 0.7486 - val_accuracy: 0.7593\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 32s 295ms/step - loss: 0.0244 - accuracy: 0.9925 - val_loss: 1.0171 - val_accuracy: 0.7302\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 32s 297ms/step - loss: 0.0165 - accuracy: 0.9934 - val_loss: 1.0926 - val_accuracy: 0.7672\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.36508059501648\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 406ms/step - loss: 0.6213 - accuracy: 0.6491 - val_loss: 0.4172 - val_accuracy: 0.8016\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 41s 380ms/step - loss: 0.2904 - accuracy: 0.8911 - val_loss: 0.4151 - val_accuracy: 0.8095\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 41s 379ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.4879 - val_accuracy: 0.8148\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 41s 380ms/step - loss: 0.0757 - accuracy: 0.9742 - val_loss: 0.5828 - val_accuracy: 0.8069\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 40s 374ms/step - loss: 0.0290 - accuracy: 0.9910 - val_loss: 0.7421 - val_accuracy: 0.8254\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 41s 382ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.7738 - val_accuracy: 0.7989\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 44s 412ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 0.7830 - val_accuracy: 0.7937\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 46s 427ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 1.0566 - val_accuracy: 0.7937\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 44s 414ms/step - loss: 0.0245 - accuracy: 0.9930 - val_loss: 0.8452 - val_accuracy: 0.7989\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 45s 419ms/step - loss: 0.0094 - accuracy: 0.9960 - val_loss: 0.9769 - val_accuracy: 0.7884\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 82.53968358039856\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 410ms/step - loss: 0.6167 - accuracy: 0.6667 - val_loss: 0.4425 - val_accuracy: 0.7831\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 0.2819 - accuracy: 0.8893 - val_loss: 0.4710 - val_accuracy: 0.8122\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 38s 353ms/step - loss: 0.1398 - accuracy: 0.9558 - val_loss: 0.5095 - val_accuracy: 0.8069\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 38s 358ms/step - loss: 0.0818 - accuracy: 0.9696 - val_loss: 0.5946 - val_accuracy: 0.7831\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 39s 367ms/step - loss: 0.0474 - accuracy: 0.9895 - val_loss: 0.8707 - val_accuracy: 0.7831\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 40s 377ms/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.8711 - val_accuracy: 0.7751\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 36s 337ms/step - loss: 0.0304 - accuracy: 0.9880 - val_loss: 0.7857 - val_accuracy: 0.7831\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 49s 363ms/step - loss: 0.6215 - accuracy: 0.6486 - val_loss: 0.4616 - val_accuracy: 0.7666\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 35s 327ms/step - loss: 0.2649 - accuracy: 0.8934 - val_loss: 0.4763 - val_accuracy: 0.7613\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.1532 - accuracy: 0.9510 - val_loss: 0.6751 - val_accuracy: 0.7613\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.0823 - accuracy: 0.9747 - val_loss: 0.7299 - val_accuracy: 0.7427\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 329ms/step - loss: 0.0458 - accuracy: 0.9858 - val_loss: 0.9310 - val_accuracy: 0.7347\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.0236 - accuracy: 0.9938 - val_loss: 1.1381 - val_accuracy: 0.7427\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 76.65782570838928\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 51s 382ms/step - loss: 0.6168 - accuracy: 0.6483 - val_loss: 0.4040 - val_accuracy: 0.8064\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 36s 339ms/step - loss: 0.2873 - accuracy: 0.8881 - val_loss: 0.3995 - val_accuracy: 0.8064\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 37s 343ms/step - loss: 0.1394 - accuracy: 0.9521 - val_loss: 0.4507 - val_accuracy: 0.7958\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 37s 344ms/step - loss: 0.0829 - accuracy: 0.9730 - val_loss: 0.6803 - val_accuracy: 0.7639\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 37s 344ms/step - loss: 0.0431 - accuracy: 0.9860 - val_loss: 0.7368 - val_accuracy: 0.7798\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 36s 338ms/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 0.9073 - val_accuracy: 0.7639\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.63660264015198\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 405ms/step - loss: 0.6205 - accuracy: 0.6643 - val_loss: 0.4556 - val_accuracy: 0.7692\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 38s 360ms/step - loss: 0.2743 - accuracy: 0.8893 - val_loss: 0.4527 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 39s 367ms/step - loss: 0.1314 - accuracy: 0.9578 - val_loss: 0.5578 - val_accuracy: 0.7851\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 40s 377ms/step - loss: 0.0575 - accuracy: 0.9851 - val_loss: 0.7943 - val_accuracy: 0.7825\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 40s 374ms/step - loss: 0.0364 - accuracy: 0.9894 - val_loss: 0.7499 - val_accuracy: 0.8064\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 40s 374ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 1.0092 - val_accuracy: 0.7984\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 40s 372ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 1.2059 - val_accuracy: 0.7878\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 40s 373ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 1.1492 - val_accuracy: 0.7958\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 40s 377ms/step - loss: 0.0092 - accuracy: 0.9988 - val_loss: 0.9170 - val_accuracy: 0.7851\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 40s 375ms/step - loss: 0.0179 - accuracy: 0.9934 - val_loss: 1.2219 - val_accuracy: 0.7772\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 80.63660264015198\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 56s 426ms/step - loss: 0.6186 - accuracy: 0.6506 - val_loss: 0.4271 - val_accuracy: 0.7825\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.3021 - accuracy: 0.8754 - val_loss: 0.4340 - val_accuracy: 0.8090\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 41s 385ms/step - loss: 0.1420 - accuracy: 0.9539 - val_loss: 0.4686 - val_accuracy: 0.8117\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 41s 384ms/step - loss: 0.0839 - accuracy: 0.9704 - val_loss: 0.4884 - val_accuracy: 0.8117\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 42s 390ms/step - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.8728 - val_accuracy: 0.7905\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 41s 382ms/step - loss: 0.0236 - accuracy: 0.9891 - val_loss: 0.8953 - val_accuracy: 0.8037\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 41s 379ms/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 0.9941 - val_accuracy: 0.7931\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 41s 383ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 1.0146 - val_accuracy: 0.7719\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 81.16710782051086\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 44s 319ms/step - loss: 0.6200 - accuracy: 0.6433 - val_loss: 0.4430 - val_accuracy: 0.8037\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.2820 - accuracy: 0.8810 - val_loss: 0.4540 - val_accuracy: 0.7931\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 26s 246ms/step - loss: 0.1264 - accuracy: 0.9582 - val_loss: 0.5792 - val_accuracy: 0.7851\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.0627 - accuracy: 0.9777 - val_loss: 0.7025 - val_accuracy: 0.7878\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 26s 243ms/step - loss: 0.0414 - accuracy: 0.9886 - val_loss: 0.8860 - val_accuracy: 0.7507\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.9755 - val_accuracy: 0.7825\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 80.37135004997253\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  83.068782  80.158728  79.365081  82.539684  81.216931  76.657826   \n",
      "\n",
      "        acc7       acc8       acc9     acc10        AVG  \n",
      "0  80.636603  80.636603  81.167108  80.37135  80.581869  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.068782</td>\n",
       "      <td>80.158728</td>\n",
       "      <td>79.365081</td>\n",
       "      <td>82.539684</td>\n",
       "      <td>81.216931</td>\n",
       "      <td>76.657826</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>80.636603</td>\n",
       "      <td>81.167108</td>\n",
       "      <td>80.37135</td>\n",
       "      <td>80.581869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  83.068782  80.158728  79.365081  82.539684  81.216931  76.657826   \n",
       "\n",
       "        acc7       acc8       acc9     acc10        AVG  \n",
       "0  80.636603  80.636603  81.167108  80.37135  80.581869  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_CR_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5046 words present from 5336 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.92041999,  0.21427323,  0.33355327, ..., -0.92285875,\n",
       "         1.36208236, -1.07814662],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 40s 280ms/step - loss: 0.6281 - accuracy: 0.6414 - val_loss: 0.5536 - val_accuracy: 0.7143\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 28s 262ms/step - loss: 0.4676 - accuracy: 0.7650 - val_loss: 0.4847 - val_accuracy: 0.7460\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 28s 266ms/step - loss: 0.4248 - accuracy: 0.8037 - val_loss: 0.4753 - val_accuracy: 0.7593\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 28s 265ms/step - loss: 0.3802 - accuracy: 0.8348 - val_loss: 0.4776 - val_accuracy: 0.7619\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 29s 269ms/step - loss: 0.3350 - accuracy: 0.8628 - val_loss: 0.4715 - val_accuracy: 0.7646\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 29s 271ms/step - loss: 0.3222 - accuracy: 0.8506 - val_loss: 0.4392 - val_accuracy: 0.7672\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 29s 271ms/step - loss: 0.2673 - accuracy: 0.8852 - val_loss: 0.5057 - val_accuracy: 0.7672\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 28s 265ms/step - loss: 0.2264 - accuracy: 0.8990 - val_loss: 0.4610 - val_accuracy: 0.7698\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 29s 268ms/step - loss: 0.2189 - accuracy: 0.8988 - val_loss: 0.4777 - val_accuracy: 0.7831\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 29s 270ms/step - loss: 0.1872 - accuracy: 0.9216 - val_loss: 0.4669 - val_accuracy: 0.8122\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 29s 267ms/step - loss: 0.1495 - accuracy: 0.9446 - val_loss: 0.4791 - val_accuracy: 0.8042\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 29s 267ms/step - loss: 0.1303 - accuracy: 0.9493 - val_loss: 0.5504 - val_accuracy: 0.7963\n",
      "Epoch 13/15\n",
      "107/107 [==============================] - 29s 274ms/step - loss: 0.1432 - accuracy: 0.9433 - val_loss: 0.6248 - val_accuracy: 0.8016\n",
      "Epoch 14/15\n",
      "107/107 [==============================] - 29s 266ms/step - loss: 0.0902 - accuracy: 0.9661 - val_loss: 0.6882 - val_accuracy: 0.7857\n",
      "Epoch 15/15\n",
      "107/107 [==============================] - 29s 267ms/step - loss: 0.0950 - accuracy: 0.9651 - val_loss: 0.8333 - val_accuracy: 0.7989\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 81.21693134307861\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 46s 340ms/step - loss: 0.6283 - accuracy: 0.6434 - val_loss: 0.5488 - val_accuracy: 0.7063\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 29s 269ms/step - loss: 0.4902 - accuracy: 0.7561 - val_loss: 0.4643 - val_accuracy: 0.7751\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 29s 273ms/step - loss: 0.4205 - accuracy: 0.7955 - val_loss: 0.5692 - val_accuracy: 0.6825\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 29s 267ms/step - loss: 0.3828 - accuracy: 0.8220 - val_loss: 0.4776 - val_accuracy: 0.7646\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 28s 261ms/step - loss: 0.3450 - accuracy: 0.8460 - val_loss: 0.5152 - val_accuracy: 0.7698\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 28s 263ms/step - loss: 0.3009 - accuracy: 0.8704 - val_loss: 0.5322 - val_accuracy: 0.7619\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 29s 268ms/step - loss: 0.2700 - accuracy: 0.8836 - val_loss: 0.5865 - val_accuracy: 0.7381\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 77.51322984695435\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 40s 289ms/step - loss: 0.6321 - accuracy: 0.6327 - val_loss: 0.5000 - val_accuracy: 0.7381\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.4683 - accuracy: 0.7760 - val_loss: 0.4772 - val_accuracy: 0.7566\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 28s 259ms/step - loss: 0.4407 - accuracy: 0.7933 - val_loss: 0.4751 - val_accuracy: 0.7354\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 28s 261ms/step - loss: 0.3985 - accuracy: 0.8199 - val_loss: 0.4671 - val_accuracy: 0.7540\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 29s 268ms/step - loss: 0.3324 - accuracy: 0.8539 - val_loss: 0.4443 - val_accuracy: 0.7804\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 29s 275ms/step - loss: 0.2982 - accuracy: 0.8817 - val_loss: 0.4975 - val_accuracy: 0.7407\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 29s 274ms/step - loss: 0.2717 - accuracy: 0.8823 - val_loss: 0.6549 - val_accuracy: 0.7143\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 29s 273ms/step - loss: 0.2561 - accuracy: 0.8889 - val_loss: 0.5550 - val_accuracy: 0.7249\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 29s 275ms/step - loss: 0.1964 - accuracy: 0.9186 - val_loss: 0.5793 - val_accuracy: 0.7434\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 30s 277ms/step - loss: 0.1625 - accuracy: 0.9303 - val_loss: 0.5294 - val_accuracy: 0.7672\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 78.04232835769653\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 36s 250ms/step - loss: 0.6288 - accuracy: 0.6465 - val_loss: 0.5077 - val_accuracy: 0.7407\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 26s 240ms/step - loss: 0.4711 - accuracy: 0.7678 - val_loss: 0.5217 - val_accuracy: 0.7513\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 26s 242ms/step - loss: 0.4225 - accuracy: 0.8000 - val_loss: 0.4595 - val_accuracy: 0.7831\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 25s 235ms/step - loss: 0.3653 - accuracy: 0.8301 - val_loss: 0.4372 - val_accuracy: 0.7937\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 26s 240ms/step - loss: 0.3393 - accuracy: 0.8463 - val_loss: 0.4405 - val_accuracy: 0.7910\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 26s 240ms/step - loss: 0.3179 - accuracy: 0.8644 - val_loss: 0.5029 - val_accuracy: 0.8069\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 25s 237ms/step - loss: 0.2671 - accuracy: 0.8794 - val_loss: 0.4575 - val_accuracy: 0.8148\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 26s 239ms/step - loss: 0.2556 - accuracy: 0.8881 - val_loss: 0.5209 - val_accuracy: 0.7963\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 26s 242ms/step - loss: 0.2241 - accuracy: 0.9096 - val_loss: 0.6149 - val_accuracy: 0.7989\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 26s 241ms/step - loss: 0.1694 - accuracy: 0.9268 - val_loss: 0.5568 - val_accuracy: 0.7857\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 26s 239ms/step - loss: 0.1719 - accuracy: 0.9332 - val_loss: 0.5688 - val_accuracy: 0.8095\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 26s 240ms/step - loss: 0.1256 - accuracy: 0.9489 - val_loss: 0.5995 - val_accuracy: 0.8148\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 81.4814805984497\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 402ms/step - loss: 0.6253 - accuracy: 0.6526 - val_loss: 0.5475 - val_accuracy: 0.7090\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 35s 330ms/step - loss: 0.5131 - accuracy: 0.7484 - val_loss: 0.4599 - val_accuracy: 0.7646\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 33s 305ms/step - loss: 0.4115 - accuracy: 0.8057 - val_loss: 0.4704 - val_accuracy: 0.7857\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 31s 291ms/step - loss: 0.3642 - accuracy: 0.8377 - val_loss: 0.4663 - val_accuracy: 0.7831\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 31s 289ms/step - loss: 0.3250 - accuracy: 0.8533 - val_loss: 0.4757 - val_accuracy: 0.7884\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 31s 292ms/step - loss: 0.2857 - accuracy: 0.8844 - val_loss: 0.4739 - val_accuracy: 0.7831\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 31s 290ms/step - loss: 0.2583 - accuracy: 0.8899 - val_loss: 0.4749 - val_accuracy: 0.7910\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 31s 290ms/step - loss: 0.2049 - accuracy: 0.9136 - val_loss: 0.5588 - val_accuracy: 0.7487\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 31s 289ms/step - loss: 0.1783 - accuracy: 0.9293 - val_loss: 0.5671 - val_accuracy: 0.7725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "107/107 [==============================] - 31s 294ms/step - loss: 0.1466 - accuracy: 0.9381 - val_loss: 0.5887 - val_accuracy: 0.7381\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 31s 291ms/step - loss: 0.1176 - accuracy: 0.9557 - val_loss: 0.7098 - val_accuracy: 0.7725\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 31s 290ms/step - loss: 0.1213 - accuracy: 0.9466 - val_loss: 0.8423 - val_accuracy: 0.7751\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 79.10053133964539\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 40s 285ms/step - loss: 0.6290 - accuracy: 0.6485 - val_loss: 0.5880 - val_accuracy: 0.6870\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 0.5026 - accuracy: 0.7461 - val_loss: 0.5061 - val_accuracy: 0.7454\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 23s 216ms/step - loss: 0.4157 - accuracy: 0.8048 - val_loss: 0.4775 - val_accuracy: 0.7878\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 23s 211ms/step - loss: 0.3686 - accuracy: 0.8367 - val_loss: 0.5346 - val_accuracy: 0.7454\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 23s 215ms/step - loss: 0.3302 - accuracy: 0.8568 - val_loss: 0.5391 - val_accuracy: 0.7825\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 23s 217ms/step - loss: 0.2954 - accuracy: 0.8744 - val_loss: 0.5093 - val_accuracy: 0.7798\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 23s 215ms/step - loss: 0.2447 - accuracy: 0.8941 - val_loss: 0.5758 - val_accuracy: 0.7692\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 23s 215ms/step - loss: 0.2069 - accuracy: 0.9087 - val_loss: 0.5550 - val_accuracy: 0.7905\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 0.1854 - accuracy: 0.9234 - val_loss: 0.5716 - val_accuracy: 0.7772\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 0.1611 - accuracy: 0.9319 - val_loss: 0.6613 - val_accuracy: 0.7454\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 0.1559 - accuracy: 0.9355 - val_loss: 0.6430 - val_accuracy: 0.7639\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 0.1306 - accuracy: 0.9471 - val_loss: 0.8721 - val_accuracy: 0.7427\n",
      "Epoch 13/15\n",
      "107/107 [==============================] - 23s 217ms/step - loss: 0.1212 - accuracy: 0.9507 - val_loss: 0.7825 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 79.0450930595398\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 38s 269ms/step - loss: 0.6478 - accuracy: 0.6116 - val_loss: 0.5206 - val_accuracy: 0.7719\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 25s 234ms/step - loss: 0.5090 - accuracy: 0.7543 - val_loss: 0.4906 - val_accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 27s 249ms/step - loss: 0.4140 - accuracy: 0.8177 - val_loss: 0.5366 - val_accuracy: 0.7745\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 27s 251ms/step - loss: 0.3626 - accuracy: 0.8521 - val_loss: 0.6596 - val_accuracy: 0.7374\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 27s 248ms/step - loss: 0.3177 - accuracy: 0.8661 - val_loss: 0.5691 - val_accuracy: 0.7507\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 27s 254ms/step - loss: 0.2915 - accuracy: 0.8712 - val_loss: 0.6197 - val_accuracy: 0.7427\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 27s 254ms/step - loss: 0.2591 - accuracy: 0.8953 - val_loss: 0.6306 - val_accuracy: 0.7480\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 78.51458787918091\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 50s 341ms/step - loss: 0.6321 - accuracy: 0.6450 - val_loss: 0.7038 - val_accuracy: 0.6260\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 39s 368ms/step - loss: 0.5110 - accuracy: 0.7448 - val_loss: 0.5334 - val_accuracy: 0.6950\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 36s 336ms/step - loss: 0.4407 - accuracy: 0.7932 - val_loss: 0.4671 - val_accuracy: 0.7586\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 36s 338ms/step - loss: 0.3728 - accuracy: 0.8399 - val_loss: 0.4879 - val_accuracy: 0.7454\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 331ms/step - loss: 0.3296 - accuracy: 0.8557 - val_loss: 0.4715 - val_accuracy: 0.7692\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 35s 329ms/step - loss: 0.2720 - accuracy: 0.8926 - val_loss: 0.5189 - val_accuracy: 0.7613\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 36s 333ms/step - loss: 0.2518 - accuracy: 0.9057 - val_loss: 0.5182 - val_accuracy: 0.7772\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 34s 322ms/step - loss: 0.2187 - accuracy: 0.9080 - val_loss: 0.5978 - val_accuracy: 0.7692\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.1755 - accuracy: 0.9304 - val_loss: 0.5200 - val_accuracy: 0.7772\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 35s 324ms/step - loss: 0.1698 - accuracy: 0.9341 - val_loss: 0.5697 - val_accuracy: 0.7719\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 34s 314ms/step - loss: 0.1258 - accuracy: 0.9557 - val_loss: 0.7240 - val_accuracy: 0.7533\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.1027 - accuracy: 0.9645 - val_loss: 0.7633 - val_accuracy: 0.7507\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 77.71883010864258\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 45s 331ms/step - loss: 0.6349 - accuracy: 0.6316 - val_loss: 0.5589 - val_accuracy: 0.6870\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 29s 274ms/step - loss: 0.4877 - accuracy: 0.7469 - val_loss: 0.5003 - val_accuracy: 0.7507\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 27s 257ms/step - loss: 0.4180 - accuracy: 0.7914 - val_loss: 0.5230 - val_accuracy: 0.7374\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 27s 249ms/step - loss: 0.3466 - accuracy: 0.8400 - val_loss: 0.5395 - val_accuracy: 0.7215\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 27s 257ms/step - loss: 0.3254 - accuracy: 0.8643 - val_loss: 0.5850 - val_accuracy: 0.7427\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 27s 251ms/step - loss: 0.2995 - accuracy: 0.8641 - val_loss: 0.6216 - val_accuracy: 0.7294\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 26s 246ms/step - loss: 0.2720 - accuracy: 0.8711 - val_loss: 0.6562 - val_accuracy: 0.7427\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 75.06631016731262\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 38s 265ms/step - loss: 0.6181 - accuracy: 0.6535 - val_loss: 0.5772 - val_accuracy: 0.6737\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 20s 191ms/step - loss: 0.4778 - accuracy: 0.7545 - val_loss: 0.5665 - val_accuracy: 0.7188\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 18s 164ms/step - loss: 0.4004 - accuracy: 0.8064 - val_loss: 0.5991 - val_accuracy: 0.7241\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.3758 - accuracy: 0.8321 - val_loss: 0.6294 - val_accuracy: 0.7215\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 18s 167ms/step - loss: 0.3208 - accuracy: 0.8600 - val_loss: 0.6281 - val_accuracy: 0.7241\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 18s 167ms/step - loss: 0.3011 - accuracy: 0.8740 - val_loss: 0.7434 - val_accuracy: 0.7241\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 18s 166ms/step - loss: 0.2612 - accuracy: 0.8799 - val_loss: 0.6430 - val_accuracy: 0.7454\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.2368 - accuracy: 0.8952 - val_loss: 0.9597 - val_accuracy: 0.7109\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.1979 - accuracy: 0.9145 - val_loss: 0.8001 - val_accuracy: 0.7294\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 18s 168ms/step - loss: 0.1602 - accuracy: 0.9346 - val_loss: 1.1002 - val_accuracy: 0.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "107/107 [==============================] - 18s 169ms/step - loss: 0.1557 - accuracy: 0.9389 - val_loss: 1.0373 - val_accuracy: 0.7533\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 18s 166ms/step - loss: 0.1140 - accuracy: 0.9555 - val_loss: 1.0876 - val_accuracy: 0.7268\n",
      "Epoch 13/15\n",
      "107/107 [==============================] - 18s 166ms/step - loss: 0.0822 - accuracy: 0.9663 - val_loss: 0.9620 - val_accuracy: 0.7347\n",
      "Epoch 14/15\n",
      "107/107 [==============================] - 18s 167ms/step - loss: 0.0740 - accuracy: 0.9700 - val_loss: 1.0657 - val_accuracy: 0.7347\n",
      "Epoch 15/15\n",
      "107/107 [==============================] - 18s 166ms/step - loss: 0.0764 - accuracy: 0.9717 - val_loss: 1.3149 - val_accuracy: 0.7321\n",
      "Test Accuracy: 73.209547996521\n",
      "\n",
      "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  81.216931  77.51323  78.042328  81.481481  79.100531  79.045093  78.514588   \n",
      "\n",
      "       acc8      acc9      acc10        AVG  \n",
      "0  77.71883  75.06631  73.209548  78.090887  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.216931</td>\n",
       "      <td>77.51323</td>\n",
       "      <td>78.042328</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>79.100531</td>\n",
       "      <td>79.045093</td>\n",
       "      <td>78.514588</td>\n",
       "      <td>77.71883</td>\n",
       "      <td>75.06631</td>\n",
       "      <td>73.209548</td>\n",
       "      <td>78.090887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  81.216931  77.51323  78.042328  81.481481  79.100531  79.045093  78.514588   \n",
       "\n",
       "       acc8      acc9      acc10        AVG  \n",
       "0  77.71883  75.06631  73.209548  78.090887  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_CR_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 44s 314ms/step - loss: 0.6117 - accuracy: 0.6679 - val_loss: 0.6095 - val_accuracy: 0.6984\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 30s 280ms/step - loss: 0.3526 - accuracy: 0.8506 - val_loss: 0.5555 - val_accuracy: 0.7143\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 32s 301ms/step - loss: 0.2187 - accuracy: 0.9148 - val_loss: 0.7210 - val_accuracy: 0.7090\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 32s 302ms/step - loss: 0.1072 - accuracy: 0.9628 - val_loss: 0.9662 - val_accuracy: 0.6799\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 32s 301ms/step - loss: 0.0727 - accuracy: 0.9758 - val_loss: 1.1529 - val_accuracy: 0.7011\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 33s 305ms/step - loss: 0.0379 - accuracy: 0.9923 - val_loss: 1.3727 - val_accuracy: 0.6852\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 36s 335ms/step - loss: 0.0293 - accuracy: 0.9933 - val_loss: 1.2232 - val_accuracy: 0.7011\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 71.42857313156128\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 67s 536ms/step - loss: 0.6198 - accuracy: 0.6515 - val_loss: 0.4395 - val_accuracy: 0.7857\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 52s 485ms/step - loss: 0.3631 - accuracy: 0.8440 - val_loss: 0.3756 - val_accuracy: 0.8069\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 45s 423ms/step - loss: 0.2118 - accuracy: 0.9174 - val_loss: 0.4464 - val_accuracy: 0.8254\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 44s 415ms/step - loss: 0.1129 - accuracy: 0.9639 - val_loss: 0.4462 - val_accuracy: 0.8254\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 44s 408ms/step - loss: 0.0646 - accuracy: 0.9802 - val_loss: 0.5002 - val_accuracy: 0.8228\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 43s 405ms/step - loss: 0.0414 - accuracy: 0.9874 - val_loss: 0.7059 - val_accuracy: 0.8386\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 43s 404ms/step - loss: 0.0319 - accuracy: 0.9874 - val_loss: 0.6543 - val_accuracy: 0.8280\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 42s 396ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.6046 - val_accuracy: 0.8254\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.7404 - val_accuracy: 0.8254\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 42s 397ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.8432 - val_accuracy: 0.8386\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.9061 - val_accuracy: 0.8466\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9409 - val_accuracy: 0.8413\n",
      "Epoch 13/15\n",
      "107/107 [==============================] - 43s 398ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.9913 - val_accuracy: 0.8386\n",
      "Epoch 14/15\n",
      "107/107 [==============================] - 43s 400ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 1.0366 - val_accuracy: 0.8386\n",
      "Epoch 15/15\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.0735 - val_accuracy: 0.8307\n",
      "Test Accuracy: 83.06878209114075\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 53s 402ms/step - loss: 0.6140 - accuracy: 0.6518 - val_loss: 0.4540 - val_accuracy: 0.7831\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 37s 347ms/step - loss: 0.3529 - accuracy: 0.8559 - val_loss: 0.4368 - val_accuracy: 0.7884\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 32s 302ms/step - loss: 0.2006 - accuracy: 0.9256 - val_loss: 0.4789 - val_accuracy: 0.7884\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 32s 302ms/step - loss: 0.1080 - accuracy: 0.9622 - val_loss: 0.5529 - val_accuracy: 0.7831\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 33s 305ms/step - loss: 0.0651 - accuracy: 0.9829 - val_loss: 0.8309 - val_accuracy: 0.7672\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 33s 306ms/step - loss: 0.0456 - accuracy: 0.9825 - val_loss: 0.6887 - val_accuracy: 0.7698\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 32s 302ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.9167 - val_accuracy: 0.7751\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 78.83597612380981\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 47s 346ms/step - loss: 0.6017 - accuracy: 0.6827 - val_loss: 0.5666 - val_accuracy: 0.7011\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 34s 319ms/step - loss: 0.3333 - accuracy: 0.8603 - val_loss: 0.4590 - val_accuracy: 0.7963\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 36s 332ms/step - loss: 0.1817 - accuracy: 0.9348 - val_loss: 0.5152 - val_accuracy: 0.7857\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 35s 324ms/step - loss: 0.0945 - accuracy: 0.9661 - val_loss: 0.5849 - val_accuracy: 0.7937\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 326ms/step - loss: 0.0567 - accuracy: 0.9823 - val_loss: 0.8270 - val_accuracy: 0.7513\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 35s 327ms/step - loss: 0.0456 - accuracy: 0.9856 - val_loss: 0.8690 - val_accuracy: 0.7804\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 35s 328ms/step - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.9892 - val_accuracy: 0.7566\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.62962985038757\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 46s 342ms/step - loss: 0.6179 - accuracy: 0.6472 - val_loss: 0.4495 - val_accuracy: 0.7725\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 33s 309ms/step - loss: 0.3326 - accuracy: 0.8605 - val_loss: 0.4142 - val_accuracy: 0.7989\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 33s 309ms/step - loss: 0.1880 - accuracy: 0.9309 - val_loss: 0.5473 - val_accuracy: 0.7857\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 33s 311ms/step - loss: 0.1259 - accuracy: 0.9558 - val_loss: 0.6735 - val_accuracy: 0.7672\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 33s 309ms/step - loss: 0.0566 - accuracy: 0.9823 - val_loss: 0.7755 - val_accuracy: 0.7566\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 33s 312ms/step - loss: 0.0349 - accuracy: 0.9912 - val_loss: 1.0709 - val_accuracy: 0.7487\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 34s 314ms/step - loss: 0.0219 - accuracy: 0.9936 - val_loss: 1.3003 - val_accuracy: 0.7354\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 79.89417910575867\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 76s 584ms/step - loss: 0.6138 - accuracy: 0.6516 - val_loss: 0.4442 - val_accuracy: 0.7958\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 59s 549ms/step - loss: 0.3534 - accuracy: 0.8467 - val_loss: 0.4324 - val_accuracy: 0.8037\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 50s 465ms/step - loss: 0.2125 - accuracy: 0.9129 - val_loss: 0.4861 - val_accuracy: 0.8090\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 47s 438ms/step - loss: 0.1067 - accuracy: 0.9644 - val_loss: 0.6538 - val_accuracy: 0.7692\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 47s 440ms/step - loss: 0.0512 - accuracy: 0.9801 - val_loss: 0.6935 - val_accuracy: 0.7374\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 47s 443ms/step - loss: 0.0513 - accuracy: 0.9875 - val_loss: 0.8502 - val_accuracy: 0.7719\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 47s 439ms/step - loss: 0.0244 - accuracy: 0.9945 - val_loss: 1.0037 - val_accuracy: 0.7613\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 47s 442ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 1.1110 - val_accuracy: 0.7692\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 80.90185523033142\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 59s 459ms/step - loss: 0.6060 - accuracy: 0.6567 - val_loss: 0.4565 - val_accuracy: 0.7745\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 43s 398ms/step - loss: 0.3660 - accuracy: 0.8411 - val_loss: 0.3868 - val_accuracy: 0.8170\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 35s 326ms/step - loss: 0.2032 - accuracy: 0.9241 - val_loss: 0.4261 - val_accuracy: 0.8223\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 35s 329ms/step - loss: 0.1106 - accuracy: 0.9606 - val_loss: 0.4662 - val_accuracy: 0.8249\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 35s 325ms/step - loss: 0.0696 - accuracy: 0.9759 - val_loss: 0.5033 - val_accuracy: 0.8355\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 34s 320ms/step - loss: 0.0347 - accuracy: 0.9910 - val_loss: 0.6084 - val_accuracy: 0.8276\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 34s 317ms/step - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.7493 - val_accuracy: 0.8196\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 34s 319ms/step - loss: 0.0213 - accuracy: 0.9926 - val_loss: 0.9333 - val_accuracy: 0.8170\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 34s 315ms/step - loss: 0.0179 - accuracy: 0.9955 - val_loss: 0.8230 - val_accuracy: 0.8196\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 33s 312ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.9746 - val_accuracy: 0.8117\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 83.55437517166138\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 36s 253ms/step - loss: 0.6129 - accuracy: 0.6641 - val_loss: 0.4548 - val_accuracy: 0.7798\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 20s 191ms/step - loss: 0.3542 - accuracy: 0.8537 - val_loss: 0.4647 - val_accuracy: 0.7984\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 19s 176ms/step - loss: 0.1968 - accuracy: 0.9196 - val_loss: 0.4586 - val_accuracy: 0.8170\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 19s 174ms/step - loss: 0.1052 - accuracy: 0.9659 - val_loss: 0.6036 - val_accuracy: 0.8064\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 19s 174ms/step - loss: 0.0867 - accuracy: 0.9733 - val_loss: 0.7515 - val_accuracy: 0.7905\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 19s 176ms/step - loss: 0.0401 - accuracy: 0.9867 - val_loss: 0.7868 - val_accuracy: 0.7984\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 19s 176ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.9925 - val_accuracy: 0.7984\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 19s 175ms/step - loss: 0.0120 - accuracy: 0.9967 - val_loss: 1.0258 - val_accuracy: 0.8037\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 81.69761300086975\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 39s 276ms/step - loss: 0.6230 - accuracy: 0.6398 - val_loss: 0.5082 - val_accuracy: 0.7480\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 27s 250ms/step - loss: 0.3617 - accuracy: 0.8389 - val_loss: 0.3899 - val_accuracy: 0.8037\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 30s 278ms/step - loss: 0.2003 - accuracy: 0.9226 - val_loss: 0.4211 - val_accuracy: 0.7984\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 0.1236 - accuracy: 0.9540 - val_loss: 0.4791 - val_accuracy: 0.8037\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 0.0816 - accuracy: 0.9765 - val_loss: 0.5196 - val_accuracy: 0.7878\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 30s 283ms/step - loss: 0.0403 - accuracy: 0.9906 - val_loss: 0.6871 - val_accuracy: 0.8037\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 31s 288ms/step - loss: 0.0288 - accuracy: 0.9909 - val_loss: 0.6263 - val_accuracy: 0.8196\n",
      "Epoch 8/15\n",
      "107/107 [==============================] - 31s 286ms/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 0.7307 - val_accuracy: 0.8037\n",
      "Epoch 9/15\n",
      "107/107 [==============================] - 30s 282ms/step - loss: 0.0123 - accuracy: 0.9978 - val_loss: 0.8017 - val_accuracy: 0.7878\n",
      "Epoch 10/15\n",
      "107/107 [==============================] - 31s 286ms/step - loss: 0.0166 - accuracy: 0.9958 - val_loss: 0.8338 - val_accuracy: 0.8117\n",
      "Epoch 11/15\n",
      "107/107 [==============================] - 30s 283ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.8243 - val_accuracy: 0.8117\n",
      "Epoch 12/15\n",
      "107/107 [==============================] - 30s 283ms/step - loss: 0.0065 - accuracy: 0.9973 - val_loss: 0.9343 - val_accuracy: 0.8037\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 81.9628655910492\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "107/107 [==============================] - 78s 614ms/step - loss: 0.6149 - accuracy: 0.6519 - val_loss: 0.4597 - val_accuracy: 0.7798\n",
      "Epoch 2/15\n",
      "107/107 [==============================] - 54s 507ms/step - loss: 0.3434 - accuracy: 0.8504 - val_loss: 0.4705 - val_accuracy: 0.8037\n",
      "Epoch 3/15\n",
      "107/107 [==============================] - 44s 415ms/step - loss: 0.1920 - accuracy: 0.9273 - val_loss: 0.5010 - val_accuracy: 0.8011\n",
      "Epoch 4/15\n",
      "107/107 [==============================] - 44s 408ms/step - loss: 0.1246 - accuracy: 0.9563 - val_loss: 0.5931 - val_accuracy: 0.8037\n",
      "Epoch 5/15\n",
      "107/107 [==============================] - 43s 401ms/step - loss: 0.0716 - accuracy: 0.9758 - val_loss: 0.8234 - val_accuracy: 0.7931\n",
      "Epoch 6/15\n",
      "107/107 [==============================] - 44s 411ms/step - loss: 0.0464 - accuracy: 0.9841 - val_loss: 0.8130 - val_accuracy: 0.7878\n",
      "Epoch 7/15\n",
      "107/107 [==============================] - 44s 407ms/step - loss: 0.0261 - accuracy: 0.9922 - val_loss: 1.0872 - val_accuracy: 0.7931\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 80.37135004997253\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  83.068782  80.158728  79.365081  82.539684  81.216931  76.657826   \n",
      "\n",
      "        acc7       acc8       acc9     acc10        AVG  \n",
      "0  80.636603  80.636603  81.167108  80.37135  80.581869  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.428573</td>\n",
       "      <td>83.068782</td>\n",
       "      <td>78.835976</td>\n",
       "      <td>79.62963</td>\n",
       "      <td>79.894179</td>\n",
       "      <td>80.901855</td>\n",
       "      <td>83.554375</td>\n",
       "      <td>81.697613</td>\n",
       "      <td>81.962866</td>\n",
       "      <td>80.37135</td>\n",
       "      <td>80.13452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3      acc4       acc5       acc6       acc7  \\\n",
       "0  71.428573  83.068782  78.835976  79.62963  79.894179  80.901855  83.554375   \n",
       "\n",
       "        acc8       acc9     acc10       AVG  \n",
       "0  81.697613  81.962866  80.37135  80.13452  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_CR_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
