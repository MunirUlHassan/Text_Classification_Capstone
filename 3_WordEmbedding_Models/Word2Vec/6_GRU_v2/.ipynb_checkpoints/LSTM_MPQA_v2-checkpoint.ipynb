{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with MPQA Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10606, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaining</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failing to support</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desperately needs</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many years of decay</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no quick fix</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>urged</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>hope</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10606 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sentence  label  split\n",
       "0              complaining      0  train\n",
       "1       failing to support      0  train\n",
       "2        desperately needs      0  train\n",
       "3      many years of decay      0  train\n",
       "4             no quick fix      0  train\n",
       "...                    ...    ...    ...\n",
       "10601                urged      1  train\n",
       "10602       strictly abide      1  train\n",
       "10603                 hope      1  train\n",
       "10604       strictly abide      1  train\n",
       "10605                           1  train\n",
       "\n",
       "[10606 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MPQA/MPQA.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10606 non-null  object\n",
      " 1   label     10606 non-null  int32 \n",
      " 2   split     10606 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 207.3+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3312</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          7294   7294\n",
       "1          3312   3312"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weaknesses are minor the feel and layout of the remote control are only so so . it does n 't show the complete file names of mp3s with really long names . you must cycle through every zoom setting ( 2x , 3x , 4x , 1 2x , etc . ) before getting back to normal size sorry if i 'm just ignorant of a way to get back to 1x quickly .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  no quick fix\n",
      "Into a sequence of int: [25, 945, 1476]\n",
      "Into a padded sequence: [  25  945 1476    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "of 3\n",
      "to 4\n",
      "a 5\n",
      "and 6\n",
      "not 7\n",
      "is 8\n",
      "in 9\n",
      "be 10\n",
      "6236\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 29s 67ms/step - loss: 0.5662 - accuracy: 0.7385 - val_loss: 0.3534 - val_accuracy: 0.8633\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.2214 - accuracy: 0.9263 - val_loss: 0.3910 - val_accuracy: 0.8596\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.1346 - accuracy: 0.9540 - val_loss: 0.4198 - val_accuracy: 0.8520\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.1008 - accuracy: 0.9631 - val_loss: 0.4683 - val_accuracy: 0.8549\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.0902 - accuracy: 0.9663 - val_loss: 0.5558 - val_accuracy: 0.8473\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.0783 - accuracy: 0.9686 - val_loss: 0.5774 - val_accuracy: 0.8483\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 86.33365035057068\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 33s 77ms/step - loss: 0.5636 - accuracy: 0.7337 - val_loss: 0.3820 - val_accuracy: 0.8332\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1997 - accuracy: 0.9319 - val_loss: 0.4187 - val_accuracy: 0.8247\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1309 - accuracy: 0.9562 - val_loss: 0.4804 - val_accuracy: 0.8134\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.1114 - accuracy: 0.9610 - val_loss: 0.5370 - val_accuracy: 0.8002\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0849 - accuracy: 0.9675 - val_loss: 0.5820 - val_accuracy: 0.8087\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.0645 - accuracy: 0.9749 - val_loss: 0.6296 - val_accuracy: 0.8332\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 83.31762552261353\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 30s 70ms/step - loss: 0.5687 - accuracy: 0.7336 - val_loss: 0.3791 - val_accuracy: 0.8549\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 16s 55ms/step - loss: 0.2039 - accuracy: 0.9321 - val_loss: 0.3946 - val_accuracy: 0.8483\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 17s 55ms/step - loss: 0.1357 - accuracy: 0.9533 - val_loss: 0.4495 - val_accuracy: 0.8407\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 16s 55ms/step - loss: 0.1079 - accuracy: 0.9630 - val_loss: 0.5069 - val_accuracy: 0.8285\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.0900 - accuracy: 0.9656 - val_loss: 0.5742 - val_accuracy: 0.8294\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 16s 55ms/step - loss: 0.0779 - accuracy: 0.9654 - val_loss: 0.5979 - val_accuracy: 0.8360\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 85.48539280891418\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 28s 65ms/step - loss: 0.5633 - accuracy: 0.7424 - val_loss: 0.3647 - val_accuracy: 0.8445\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 22s 72ms/step - loss: 0.2004 - accuracy: 0.9324 - val_loss: 0.3882 - val_accuracy: 0.8388\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.1339 - accuracy: 0.9525 - val_loss: 0.4464 - val_accuracy: 0.8417\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.0999 - accuracy: 0.9637 - val_loss: 0.4691 - val_accuracy: 0.8464\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0863 - accuracy: 0.9689 - val_loss: 0.5376 - val_accuracy: 0.8435\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.0667 - accuracy: 0.9745 - val_loss: 0.6070 - val_accuracy: 0.8407\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0520 - accuracy: 0.9766 - val_loss: 0.6397 - val_accuracy: 0.8369\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0471 - accuracy: 0.9821 - val_loss: 0.7024 - val_accuracy: 0.8464\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 0.7646 - val_accuracy: 0.8322\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 84.63713526725769\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 40s 101ms/step - loss: 0.5675 - accuracy: 0.7309 - val_loss: 0.3309 - val_accuracy: 0.8784\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.2093 - accuracy: 0.9305 - val_loss: 0.3200 - val_accuracy: 0.8812\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 19s 65ms/step - loss: 0.1310 - accuracy: 0.9538 - val_loss: 0.3552 - val_accuracy: 0.8775\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0993 - accuracy: 0.9670 - val_loss: 0.3789 - val_accuracy: 0.8662\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0945 - accuracy: 0.9627 - val_loss: 0.4276 - val_accuracy: 0.8690\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0784 - accuracy: 0.9680 - val_loss: 0.4862 - val_accuracy: 0.8728\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.0578 - accuracy: 0.9755 - val_loss: 0.5100 - val_accuracy: 0.8624\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.12441229820251\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 39s 100ms/step - loss: 0.5663 - accuracy: 0.7301 - val_loss: 0.3460 - val_accuracy: 0.8680\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.2064 - accuracy: 0.9309 - val_loss: 0.3592 - val_accuracy: 0.8341\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1426 - accuracy: 0.9522 - val_loss: 0.4011 - val_accuracy: 0.8228\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1072 - accuracy: 0.9616 - val_loss: 0.4300 - val_accuracy: 0.8238\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0896 - accuracy: 0.9642 - val_loss: 0.4745 - val_accuracy: 0.8275\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0730 - accuracy: 0.9701 - val_loss: 0.4872 - val_accuracy: 0.8615\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 86.80490255355835\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 39s 100ms/step - loss: 0.5670 - accuracy: 0.7353 - val_loss: 0.3629 - val_accuracy: 0.8604\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.2022 - accuracy: 0.9323 - val_loss: 0.3693 - val_accuracy: 0.8642\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.1323 - accuracy: 0.9549 - val_loss: 0.3973 - val_accuracy: 0.8575\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 27s 92ms/step - loss: 0.1018 - accuracy: 0.9638 - val_loss: 0.4245 - val_accuracy: 0.8434\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0870 - accuracy: 0.9657 - val_loss: 0.4585 - val_accuracy: 0.8462\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.0678 - accuracy: 0.9739 - val_loss: 0.5233 - val_accuracy: 0.8509\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 27s 92ms/step - loss: 0.0595 - accuracy: 0.9754 - val_loss: 0.5847 - val_accuracy: 0.8481\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.41509413719177\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 41s 109ms/step - loss: 0.5669 - accuracy: 0.7376 - val_loss: 0.3532 - val_accuracy: 0.8538\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.2184 - accuracy: 0.9255 - val_loss: 0.3655 - val_accuracy: 0.8698\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.1416 - accuracy: 0.9536 - val_loss: 0.3875 - val_accuracy: 0.8651\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.1104 - accuracy: 0.9611 - val_loss: 0.4061 - val_accuracy: 0.8736\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0928 - accuracy: 0.9650 - val_loss: 0.4286 - val_accuracy: 0.8660\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 28s 94ms/step - loss: 0.0691 - accuracy: 0.9714 - val_loss: 0.4813 - val_accuracy: 0.8651\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.0616 - accuracy: 0.9743 - val_loss: 0.5056 - val_accuracy: 0.8717\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0439 - accuracy: 0.9822 - val_loss: 0.5576 - val_accuracy: 0.8670\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0433 - accuracy: 0.9822 - val_loss: 0.6265 - val_accuracy: 0.8642\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 87.35849261283875\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 41s 105ms/step - loss: 0.5653 - accuracy: 0.7237 - val_loss: 0.3523 - val_accuracy: 0.8642\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 31s 104ms/step - loss: 0.2200 - accuracy: 0.9223 - val_loss: 0.3501 - val_accuracy: 0.8679\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 30s 100ms/step - loss: 0.1417 - accuracy: 0.9469 - val_loss: 0.3947 - val_accuracy: 0.8613\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.1085 - accuracy: 0.9596 - val_loss: 0.4206 - val_accuracy: 0.8623\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.0906 - accuracy: 0.9643 - val_loss: 0.4518 - val_accuracy: 0.8575\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0687 - accuracy: 0.9693 - val_loss: 0.5109 - val_accuracy: 0.8566\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.0570 - accuracy: 0.9765 - val_loss: 0.5209 - val_accuracy: 0.8613\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.79245114326477\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 37s 92ms/step - loss: 0.5674 - accuracy: 0.7404 - val_loss: 0.3565 - val_accuracy: 0.8642\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.2052 - accuracy: 0.9297 - val_loss: 0.3577 - val_accuracy: 0.8745\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.4013 - val_accuracy: 0.8670\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.1105 - accuracy: 0.9585 - val_loss: 0.4464 - val_accuracy: 0.8519\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.0931 - accuracy: 0.9641 - val_loss: 0.4821 - val_accuracy: 0.8453\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.0712 - accuracy: 0.9715 - val_loss: 0.5261 - val_accuracy: 0.8557\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.0548 - accuracy: 0.9775 - val_loss: 0.5424 - val_accuracy: 0.8491\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.45282888412476\n",
      "\n",
      "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  86.33365  83.317626  85.485393  84.637135  88.124412  86.804903  86.415094   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  87.358493  86.792451  87.452829  86.272199  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.33365</td>\n",
       "      <td>83.317626</td>\n",
       "      <td>85.485393</td>\n",
       "      <td>84.637135</td>\n",
       "      <td>88.124412</td>\n",
       "      <td>86.804903</td>\n",
       "      <td>86.415094</td>\n",
       "      <td>87.358493</td>\n",
       "      <td>86.792451</td>\n",
       "      <td>87.452829</td>\n",
       "      <td>86.272199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  86.33365  83.317626  85.485393  84.637135  88.124412  86.804903  86.415094   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  87.358493  86.792451  87.452829  86.272199  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('LSTM_MPQA_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6083 words present from 6236 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.word_vec(word)\n",
    "            \n",
    "        # initialize the unknown word with standard normal distribution values\n",
    "        else:\n",
    "            embed_matrix[idx] = np.random.randn(emb_dim)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.28005372, -2.31110019, -1.14368075, ...,  0.97108741,\n",
       "         0.24024221,  0.96457136],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 31s 70ms/step - loss: 0.4953 - accuracy: 0.7642 - val_loss: 0.3639 - val_accuracy: 0.8426\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.3048 - accuracy: 0.8796 - val_loss: 0.3223 - val_accuracy: 0.8756\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 18s 60ms/step - loss: 0.2755 - accuracy: 0.8986 - val_loss: 0.3347 - val_accuracy: 0.8680\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 18s 61ms/step - loss: 0.2404 - accuracy: 0.9117 - val_loss: 0.3288 - val_accuracy: 0.8709\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.2378 - accuracy: 0.9100 - val_loss: 0.3630 - val_accuracy: 0.8643\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.2163 - accuracy: 0.9139 - val_loss: 0.3623 - val_accuracy: 0.8756\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 18s 61ms/step - loss: 0.1856 - accuracy: 0.9296 - val_loss: 0.3909 - val_accuracy: 0.8690\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.55890727043152\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 29s 68ms/step - loss: 0.5047 - accuracy: 0.7449 - val_loss: 0.3288 - val_accuracy: 0.8671\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 17s 55ms/step - loss: 0.3055 - accuracy: 0.8776 - val_loss: 0.3096 - val_accuracy: 0.8746\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.2680 - accuracy: 0.8985 - val_loss: 0.3046 - val_accuracy: 0.8775\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.2492 - accuracy: 0.9016 - val_loss: 0.3099 - val_accuracy: 0.8709\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 17s 58ms/step - loss: 0.2328 - accuracy: 0.9163 - val_loss: 0.3066 - val_accuracy: 0.8746\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.2088 - accuracy: 0.9232 - val_loss: 0.3369 - val_accuracy: 0.8652\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.2001 - accuracy: 0.9214 - val_loss: 0.3521 - val_accuracy: 0.8699\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 18s 59ms/step - loss: 0.1797 - accuracy: 0.9320 - val_loss: 0.4022 - val_accuracy: 0.8558\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 87.74740695953369\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 22s 44ms/step - loss: 0.4958 - accuracy: 0.7624 - val_loss: 0.4837 - val_accuracy: 0.7776\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 10s 33ms/step - loss: 0.3007 - accuracy: 0.8826 - val_loss: 0.4458 - val_accuracy: 0.8030\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 0.2798 - accuracy: 0.8930 - val_loss: 0.4658 - val_accuracy: 0.7964\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2513 - accuracy: 0.9051 - val_loss: 0.4148 - val_accuracy: 0.8209\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 0.2365 - accuracy: 0.9100 - val_loss: 0.3794 - val_accuracy: 0.8303\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2279 - accuracy: 0.9105 - val_loss: 0.4274 - val_accuracy: 0.8096\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 0.2082 - accuracy: 0.9204 - val_loss: 0.3907 - val_accuracy: 0.8190\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 0.1865 - accuracy: 0.9284 - val_loss: 0.3992 - val_accuracy: 0.8247\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1727 - accuracy: 0.9389 - val_loss: 0.3658 - val_accuracy: 0.8351\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 0.1563 - accuracy: 0.9412 - val_loss: 0.3787 - val_accuracy: 0.8247\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 0.1530 - accuracy: 0.9416 - val_loss: 0.3831 - val_accuracy: 0.8388\n",
      "Epoch 12/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1317 - accuracy: 0.9499 - val_loss: 0.4537 - val_accuracy: 0.8303\n",
      "Epoch 13/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1161 - accuracy: 0.9553 - val_loss: 0.4230 - val_accuracy: 0.8624\n",
      "Epoch 14/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1076 - accuracy: 0.9583 - val_loss: 0.4594 - val_accuracy: 0.8615\n",
      "Epoch 15/15\n",
      "299/299 [==============================] - 9s 30ms/step - loss: 0.1051 - accuracy: 0.9625 - val_loss: 0.4845 - val_accuracy: 0.8539\n",
      "Test Accuracy: 85.39113998413086\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 21s 43ms/step - loss: 0.4911 - accuracy: 0.7650 - val_loss: 0.4060 - val_accuracy: 0.8549\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2959 - accuracy: 0.8897 - val_loss: 0.4368 - val_accuracy: 0.8530\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2741 - accuracy: 0.9005 - val_loss: 0.4441 - val_accuracy: 0.8558\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2463 - accuracy: 0.9035 - val_loss: 0.4335 - val_accuracy: 0.8652\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2311 - accuracy: 0.9141 - val_loss: 0.4395 - val_accuracy: 0.8652\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.2135 - accuracy: 0.9214 - val_loss: 0.4713 - val_accuracy: 0.8662\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1986 - accuracy: 0.9269 - val_loss: 0.4854 - val_accuracy: 0.8728\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1866 - accuracy: 0.9302 - val_loss: 0.5319 - val_accuracy: 0.8737\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1688 - accuracy: 0.9361 - val_loss: 0.5243 - val_accuracy: 0.8765\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1494 - accuracy: 0.9423 - val_loss: 0.5524 - val_accuracy: 0.8756\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1413 - accuracy: 0.9462 - val_loss: 0.5912 - val_accuracy: 0.8728\n",
      "Epoch 12/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1328 - accuracy: 0.9504 - val_loss: 0.6100 - val_accuracy: 0.8718\n",
      "Epoch 13/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1155 - accuracy: 0.9567 - val_loss: 0.6385 - val_accuracy: 0.8765\n",
      "Epoch 14/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.1099 - accuracy: 0.9598 - val_loss: 0.6976 - val_accuracy: 0.8794\n",
      "Epoch 15/15\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 0.0991 - accuracy: 0.9636 - val_loss: 0.7137 - val_accuracy: 0.8671\n",
      "Test Accuracy: 86.71064972877502\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 36s 90ms/step - loss: 0.5033 - accuracy: 0.7620 - val_loss: 0.3498 - val_accuracy: 0.8605\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 23s 78ms/step - loss: 0.3002 - accuracy: 0.8836 - val_loss: 0.3331 - val_accuracy: 0.8643\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.2693 - accuracy: 0.8941 - val_loss: 0.3615 - val_accuracy: 0.8200\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.2408 - accuracy: 0.9085 - val_loss: 0.3849 - val_accuracy: 0.8162\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 23s 78ms/step - loss: 0.2329 - accuracy: 0.9112 - val_loss: 0.4272 - val_accuracy: 0.8134\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 23s 78ms/step - loss: 0.2235 - accuracy: 0.9129 - val_loss: 0.4890 - val_accuracy: 0.7908\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.2013 - accuracy: 0.9235 - val_loss: 0.4557 - val_accuracy: 0.8087\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.42789721488953\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 31s 75ms/step - loss: 0.5002 - accuracy: 0.7583 - val_loss: 0.6672 - val_accuracy: 0.7436\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 20s 65ms/step - loss: 0.3069 - accuracy: 0.8829 - val_loss: 0.7642 - val_accuracy: 0.7455\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 19s 64ms/step - loss: 0.2797 - accuracy: 0.8934 - val_loss: 0.7934 - val_accuracy: 0.7738\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.2487 - accuracy: 0.9082 - val_loss: 0.8322 - val_accuracy: 0.7738\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.2434 - accuracy: 0.9038 - val_loss: 0.9618 - val_accuracy: 0.7719\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.2203 - accuracy: 0.9173 - val_loss: 0.9738 - val_accuracy: 0.7861\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 19s 63ms/step - loss: 0.1943 - accuracy: 0.9258 - val_loss: 1.0071 - val_accuracy: 0.7917\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.1852 - accuracy: 0.9283 - val_loss: 1.0779 - val_accuracy: 0.7898\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 18s 62ms/step - loss: 0.1689 - accuracy: 0.9339 - val_loss: 1.2106 - val_accuracy: 0.7908\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 18s 62ms/step - loss: 0.1556 - accuracy: 0.9386 - val_loss: 1.3412 - val_accuracy: 0.7823\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.1393 - accuracy: 0.9444 - val_loss: 1.4744 - val_accuracy: 0.7842\n",
      "Epoch 12/15\n",
      "299/299 [==============================] - 19s 62ms/step - loss: 0.1381 - accuracy: 0.9449 - val_loss: 1.5203 - val_accuracy: 0.7870\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 79.17059659957886\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 27s 63ms/step - loss: 0.4934 - accuracy: 0.7648 - val_loss: 0.3582 - val_accuracy: 0.8613\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 15s 52ms/step - loss: 0.2978 - accuracy: 0.8809 - val_loss: 0.3423 - val_accuracy: 0.8651\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 15s 51ms/step - loss: 0.2688 - accuracy: 0.8935 - val_loss: 0.3320 - val_accuracy: 0.8736\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 15s 51ms/step - loss: 0.2375 - accuracy: 0.9102 - val_loss: 0.3304 - val_accuracy: 0.8708\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 15s 51ms/step - loss: 0.2224 - accuracy: 0.9145 - val_loss: 0.3348 - val_accuracy: 0.8726\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 15s 51ms/step - loss: 0.2082 - accuracy: 0.9225 - val_loss: 0.3373 - val_accuracy: 0.8764\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 0.1943 - accuracy: 0.9311 - val_loss: 0.3709 - val_accuracy: 0.8349\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 0.1737 - accuracy: 0.9366 - val_loss: 0.3917 - val_accuracy: 0.8434\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 0.1609 - accuracy: 0.9387 - val_loss: 0.4135 - val_accuracy: 0.8368\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 0.1517 - accuracy: 0.9414 - val_loss: 0.4412 - val_accuracy: 0.8264\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 0.1341 - accuracy: 0.9501 - val_loss: 0.4846 - val_accuracy: 0.8302\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 87.64150738716125\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 27s 63ms/step - loss: 0.4902 - accuracy: 0.7689 - val_loss: 0.3598 - val_accuracy: 0.8632\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.2951 - accuracy: 0.8844 - val_loss: 0.3597 - val_accuracy: 0.8651\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.2744 - accuracy: 0.8909 - val_loss: 0.3912 - val_accuracy: 0.8755\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.2390 - accuracy: 0.9068 - val_loss: 0.3797 - val_accuracy: 0.8736\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.2287 - accuracy: 0.9125 - val_loss: 0.3715 - val_accuracy: 0.8736\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.2126 - accuracy: 0.9225 - val_loss: 0.3707 - val_accuracy: 0.8745\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.2025 - accuracy: 0.9245 - val_loss: 0.3826 - val_accuracy: 0.8745\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 14s 48ms/step - loss: 0.1917 - accuracy: 0.9257 - val_loss: 0.3955 - val_accuracy: 0.8660\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 87.54717111587524\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 45s 109ms/step - loss: 0.4930 - accuracy: 0.7697 - val_loss: 0.4503 - val_accuracy: 0.7849\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 25s 85ms/step - loss: 0.2941 - accuracy: 0.8867 - val_loss: 0.5060 - val_accuracy: 0.7783\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.2676 - accuracy: 0.8985 - val_loss: 0.4688 - val_accuracy: 0.7943\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.2436 - accuracy: 0.9053 - val_loss: 0.4461 - val_accuracy: 0.8123\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 25s 84ms/step - loss: 0.2287 - accuracy: 0.9108 - val_loss: 0.4404 - val_accuracy: 0.8132\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 25s 82ms/step - loss: 0.2179 - accuracy: 0.9160 - val_loss: 0.4536 - val_accuracy: 0.8198\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 25s 82ms/step - loss: 0.2035 - accuracy: 0.9242 - val_loss: 0.4841 - val_accuracy: 0.8189\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1932 - accuracy: 0.9275 - val_loss: 0.4980 - val_accuracy: 0.8217\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1636 - accuracy: 0.9402 - val_loss: 0.5004 - val_accuracy: 0.8151\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1488 - accuracy: 0.9434 - val_loss: 0.4927 - val_accuracy: 0.8189\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1346 - accuracy: 0.9471 - val_loss: 0.5489 - val_accuracy: 0.8255\n",
      "Epoch 12/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.1252 - accuracy: 0.9508 - val_loss: 0.5840 - val_accuracy: 0.8236\n",
      "Epoch 13/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1175 - accuracy: 0.9534 - val_loss: 0.6089 - val_accuracy: 0.8264\n",
      "Epoch 14/15\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.1042 - accuracy: 0.9594 - val_loss: 0.6692 - val_accuracy: 0.8208\n",
      "Epoch 15/15\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.0998 - accuracy: 0.9628 - val_loss: 0.7061 - val_accuracy: 0.8264\n",
      "Test Accuracy: 82.64151215553284\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 41s 105ms/step - loss: 0.4986 - accuracy: 0.7622 - val_loss: 0.4257 - val_accuracy: 0.7962\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 0.2996 - accuracy: 0.8800 - val_loss: 0.3996 - val_accuracy: 0.8132\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 0.2681 - accuracy: 0.8987 - val_loss: 0.4053 - val_accuracy: 0.8075\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 0.2519 - accuracy: 0.9102 - val_loss: 0.4174 - val_accuracy: 0.8142\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 0.2392 - accuracy: 0.9082 - val_loss: 0.4025 - val_accuracy: 0.8066\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 0.2228 - accuracy: 0.9158 - val_loss: 0.4113 - val_accuracy: 0.8113\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.1910 - accuracy: 0.9298 - val_loss: 0.4106 - val_accuracy: 0.8208\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 25s 85ms/step - loss: 0.1902 - accuracy: 0.9281 - val_loss: 0.4206 - val_accuracy: 0.8198\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.1632 - accuracy: 0.9388 - val_loss: 0.4356 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.1437 - accuracy: 0.9436 - val_loss: 0.4632 - val_accuracy: 0.8160\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 25s 84ms/step - loss: 0.1375 - accuracy: 0.9429 - val_loss: 0.4942 - val_accuracy: 0.8189\n",
      "Epoch 12/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1259 - accuracy: 0.9530 - val_loss: 0.5066 - val_accuracy: 0.8094\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 82.07547068595886\n",
      "\n",
      "        acc1       acc2      acc3      acc4       acc5       acc6       acc7  \\\n",
      "0  87.558907  87.747407  85.39114  86.71065  86.427897  79.170597  87.641507   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  87.547171  82.641512  82.075471  85.291226  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.558907</td>\n",
       "      <td>87.747407</td>\n",
       "      <td>85.39114</td>\n",
       "      <td>86.71065</td>\n",
       "      <td>86.427897</td>\n",
       "      <td>79.170597</td>\n",
       "      <td>87.641507</td>\n",
       "      <td>87.547171</td>\n",
       "      <td>82.641512</td>\n",
       "      <td>82.075471</td>\n",
       "      <td>85.291226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2      acc3      acc4       acc5       acc6       acc7  \\\n",
       "0  87.558907  87.747407  85.39114  86.71065  86.427897  79.170597  87.641507   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  87.547171  82.641512  82.075471  85.291226  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('LSTM_MPQA_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 39s 100ms/step - loss: 0.4709 - accuracy: 0.7880 - val_loss: 0.3083 - val_accuracy: 0.8888\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.1984 - accuracy: 0.9306 - val_loss: 0.3211 - val_accuracy: 0.8897\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1378 - accuracy: 0.9550 - val_loss: 0.3600 - val_accuracy: 0.8822\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1000 - accuracy: 0.9658 - val_loss: 0.4034 - val_accuracy: 0.8775\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 25s 82ms/step - loss: 0.0878 - accuracy: 0.9642 - val_loss: 0.4362 - val_accuracy: 0.8775\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.0692 - accuracy: 0.9723 - val_loss: 0.4697 - val_accuracy: 0.8709\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.0611 - accuracy: 0.9751 - val_loss: 0.5281 - val_accuracy: 0.8643\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.97266983985901\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 35s 89ms/step - loss: 0.4761 - accuracy: 0.7730 - val_loss: 0.3502 - val_accuracy: 0.8671\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.2009 - accuracy: 0.9262 - val_loss: 0.3699 - val_accuracy: 0.8652\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 22s 72ms/step - loss: 0.1279 - accuracy: 0.9570 - val_loss: 0.4080 - val_accuracy: 0.8219\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 22s 74ms/step - loss: 0.1109 - accuracy: 0.9618 - val_loss: 0.4597 - val_accuracy: 0.8228\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 23s 76ms/step - loss: 0.0887 - accuracy: 0.9681 - val_loss: 0.4908 - val_accuracy: 0.8219\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 22s 75ms/step - loss: 0.0712 - accuracy: 0.9704 - val_loss: 0.5843 - val_accuracy: 0.8153\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 86.71064972877502\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 61s 164ms/step - loss: 0.4744 - accuracy: 0.7834 - val_loss: 0.3420 - val_accuracy: 0.8228\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.2001 - accuracy: 0.9289 - val_loss: 0.4071 - val_accuracy: 0.8124\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.1335 - accuracy: 0.9524 - val_loss: 0.4950 - val_accuracy: 0.8040\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 48s 161ms/step - loss: 0.0967 - accuracy: 0.9677 - val_loss: 0.5225 - val_accuracy: 0.8002\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 49s 165ms/step - loss: 0.0860 - accuracy: 0.9671 - val_loss: 0.5883 - val_accuracy: 0.7974\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 50s 166ms/step - loss: 0.0680 - accuracy: 0.9729 - val_loss: 0.6617 - val_accuracy: 0.8068\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 82.28086829185486\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 63s 181ms/step - loss: 0.4798 - accuracy: 0.7730 - val_loss: 0.4740 - val_accuracy: 0.8049\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 53s 177ms/step - loss: 0.2028 - accuracy: 0.9260 - val_loss: 0.5374 - val_accuracy: 0.8030\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.1292 - accuracy: 0.9555 - val_loss: 0.6477 - val_accuracy: 0.7992\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 52s 175ms/step - loss: 0.0941 - accuracy: 0.9677 - val_loss: 0.6482 - val_accuracy: 0.8021\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0882 - accuracy: 0.9657 - val_loss: 0.7484 - val_accuracy: 0.7917\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.0671 - accuracy: 0.9720 - val_loss: 0.7758 - val_accuracy: 0.8087\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0586 - accuracy: 0.9757 - val_loss: 0.8795 - val_accuracy: 0.7983\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.0516 - accuracy: 0.9782 - val_loss: 0.9834 - val_accuracy: 0.7964\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 52s 174ms/step - loss: 0.0412 - accuracy: 0.9831 - val_loss: 0.9158 - val_accuracy: 0.8068\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 51s 169ms/step - loss: 0.0384 - accuracy: 0.9826 - val_loss: 0.9622 - val_accuracy: 0.7974\n",
      "Epoch 11/15\n",
      "299/299 [==============================] - 52s 172ms/step - loss: 0.0357 - accuracy: 0.9839 - val_loss: 1.0228 - val_accuracy: 0.7889\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 80.86710572242737\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 48s 130ms/step - loss: 0.4677 - accuracy: 0.7832 - val_loss: 0.3272 - val_accuracy: 0.8728\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 37s 125ms/step - loss: 0.1894 - accuracy: 0.9348 - val_loss: 0.3229 - val_accuracy: 0.8765\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 36s 121ms/step - loss: 0.1410 - accuracy: 0.9500 - val_loss: 0.3693 - val_accuracy: 0.8775\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.1059 - accuracy: 0.9647 - val_loss: 0.3797 - val_accuracy: 0.8709\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0801 - accuracy: 0.9707 - val_loss: 0.4142 - val_accuracy: 0.8690\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.0660 - accuracy: 0.9738 - val_loss: 0.4403 - val_accuracy: 0.8690\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 36s 120ms/step - loss: 0.0599 - accuracy: 0.9768 - val_loss: 0.4820 - val_accuracy: 0.8567\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 36s 120ms/step - loss: 0.0511 - accuracy: 0.9794 - val_loss: 0.5299 - val_accuracy: 0.8615\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 87.74740695953369\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 47s 127ms/step - loss: 0.4878 - accuracy: 0.7670 - val_loss: 0.3409 - val_accuracy: 0.8417\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 36s 121ms/step - loss: 0.1947 - accuracy: 0.9312 - val_loss: 0.3698 - val_accuracy: 0.8445\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 36s 119ms/step - loss: 0.1384 - accuracy: 0.9532 - val_loss: 0.4091 - val_accuracy: 0.8417\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 36s 120ms/step - loss: 0.0968 - accuracy: 0.9639 - val_loss: 0.4447 - val_accuracy: 0.8369\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 36s 121ms/step - loss: 0.0851 - accuracy: 0.9675 - val_loss: 0.5384 - val_accuracy: 0.8247\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 36s 120ms/step - loss: 0.0708 - accuracy: 0.9686 - val_loss: 0.6100 - val_accuracy: 0.8285\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 35s 118ms/step - loss: 0.0585 - accuracy: 0.9757 - val_loss: 0.5973 - val_accuracy: 0.8303\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 84.44863557815552\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 56s 148ms/step - loss: 0.4652 - accuracy: 0.7887 - val_loss: 0.5359 - val_accuracy: 0.7821\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 47s 156ms/step - loss: 0.2005 - accuracy: 0.9291 - val_loss: 0.5143 - val_accuracy: 0.7991\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 46s 155ms/step - loss: 0.1332 - accuracy: 0.9561 - val_loss: 0.5607 - val_accuracy: 0.8009\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 46s 155ms/step - loss: 0.1001 - accuracy: 0.9645 - val_loss: 0.5930 - val_accuracy: 0.8075\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 46s 153ms/step - loss: 0.0887 - accuracy: 0.9664 - val_loss: 0.6191 - val_accuracy: 0.8113\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 45s 152ms/step - loss: 0.0754 - accuracy: 0.9714 - val_loss: 0.5994 - val_accuracy: 0.8085\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 47s 156ms/step - loss: 0.0609 - accuracy: 0.9762 - val_loss: 0.7535 - val_accuracy: 0.8000\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 46s 153ms/step - loss: 0.0507 - accuracy: 0.9769 - val_loss: 0.7916 - val_accuracy: 0.8009\n",
      "Epoch 9/15\n",
      "299/299 [==============================] - 46s 155ms/step - loss: 0.0446 - accuracy: 0.9809 - val_loss: 0.8547 - val_accuracy: 0.8009\n",
      "Epoch 10/15\n",
      "299/299 [==============================] - 46s 153ms/step - loss: 0.0456 - accuracy: 0.9792 - val_loss: 0.8565 - val_accuracy: 0.7934\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 81.13207817077637\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 47s 128ms/step - loss: 0.4700 - accuracy: 0.7722 - val_loss: 0.4690 - val_accuracy: 0.7896\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 30s 100ms/step - loss: 0.2060 - accuracy: 0.9321 - val_loss: 0.4615 - val_accuracy: 0.8028\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.1398 - accuracy: 0.9543 - val_loss: 0.4990 - val_accuracy: 0.8094\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0994 - accuracy: 0.9669 - val_loss: 0.5430 - val_accuracy: 0.8047\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0809 - accuracy: 0.9728 - val_loss: 0.6668 - val_accuracy: 0.7934\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0659 - accuracy: 0.9724 - val_loss: 0.6730 - val_accuracy: 0.8038\n",
      "Epoch 7/15\n",
      "299/299 [==============================] - 29s 98ms/step - loss: 0.0612 - accuracy: 0.9750 - val_loss: 0.7226 - val_accuracy: 0.7953\n",
      "Epoch 8/15\n",
      "299/299 [==============================] - 29s 97ms/step - loss: 0.0535 - accuracy: 0.9756 - val_loss: 0.7732 - val_accuracy: 0.7972\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 80.94339370727539\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 40s 104ms/step - loss: 0.4700 - accuracy: 0.7811 - val_loss: 0.3173 - val_accuracy: 0.8840\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 28s 94ms/step - loss: 0.2059 - accuracy: 0.9296 - val_loss: 0.3513 - val_accuracy: 0.8811\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 28s 94ms/step - loss: 0.1282 - accuracy: 0.9558 - val_loss: 0.3643 - val_accuracy: 0.8792\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.1005 - accuracy: 0.9641 - val_loss: 0.3911 - val_accuracy: 0.8755\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0824 - accuracy: 0.9686 - val_loss: 0.4109 - val_accuracy: 0.8802\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.0712 - accuracy: 0.9720 - val_loss: 0.4345 - val_accuracy: 0.8764\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 88.39622735977173\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "299/299 [==============================] - 40s 104ms/step - loss: 0.4834 - accuracy: 0.7706 - val_loss: 0.4078 - val_accuracy: 0.8217\n",
      "Epoch 2/15\n",
      "299/299 [==============================] - 28s 93ms/step - loss: 0.1968 - accuracy: 0.9287 - val_loss: 0.4586 - val_accuracy: 0.8151\n",
      "Epoch 3/15\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.1371 - accuracy: 0.9540 - val_loss: 0.6273 - val_accuracy: 0.8066\n",
      "Epoch 4/15\n",
      "299/299 [==============================] - 29s 95ms/step - loss: 0.1065 - accuracy: 0.9633 - val_loss: 0.6187 - val_accuracy: 0.8189\n",
      "Epoch 5/15\n",
      "299/299 [==============================] - 29s 96ms/step - loss: 0.0919 - accuracy: 0.9642 - val_loss: 0.7015 - val_accuracy: 0.8160\n",
      "Epoch 6/15\n",
      "299/299 [==============================] - 28s 95ms/step - loss: 0.0776 - accuracy: 0.9712 - val_loss: 0.7408 - val_accuracy: 0.8132\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 82.16981291770935\n",
      "\n",
      "       acc1       acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
      "0  86.33365  83.317626  85.485393  84.637135  88.124412  86.804903  86.415094   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  87.358493  86.792451  87.452829  86.272199  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.97267</td>\n",
       "      <td>86.71065</td>\n",
       "      <td>82.280868</td>\n",
       "      <td>80.867106</td>\n",
       "      <td>87.747407</td>\n",
       "      <td>84.448636</td>\n",
       "      <td>81.132078</td>\n",
       "      <td>80.943394</td>\n",
       "      <td>88.396227</td>\n",
       "      <td>82.169813</td>\n",
       "      <td>84.366885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc1      acc2       acc3       acc4       acc5       acc6       acc7  \\\n",
       "0  88.97267  86.71065  82.280868  80.867106  87.747407  84.448636  81.132078   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  80.943394  88.396227  82.169813  84.366885  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('LSTM_MPQA_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
